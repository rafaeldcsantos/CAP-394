[
  {
    "objectID": "00-Lectures-Intro.html",
    "href": "00-Lectures-Intro.html",
    "title": "About the Course",
    "section": "",
    "text": "This is the material (slides and notes) of the introduction to the course.\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!",
    "crumbs": [
      "Lectures",
      "About the Course"
    ]
  },
  {
    "objectID": "00-Lectures-Intro.html#slides",
    "href": "00-Lectures-Intro.html#slides",
    "title": "About the Course",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!",
    "crumbs": [
      "Lectures",
      "About the Course"
    ]
  },
  {
    "objectID": "01-Lectures-WhatIs.html",
    "href": "01-Lectures-WhatIs.html",
    "title": "What is Data Science?",
    "section": "",
    "text": "This is the material (slides and notes) of the first lecture on the course.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.",
    "crumbs": [
      "Lectures",
      "What is Data Science?"
    ]
  },
  {
    "objectID": "01-Lectures-WhatIs.html#slides",
    "href": "01-Lectures-WhatIs.html#slides",
    "title": "What is Data Science?",
    "section": "",
    "text": "This is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.",
    "crumbs": [
      "Lectures",
      "What is Data Science?"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About this Course",
    "section": "",
    "text": "These pages contains the material used for the course Introduction to Data Science, part of the curriculum of the Graduate Program in Applied Computing, offeredy by the Brazilian National Institute for Space Research.\nThe course is usually offered in the second term of each year, exclusively asynchronously. Although the material is in English, classes are usually taught in Portuguese.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "About this Course",
    "section": "Syllabus",
    "text": "Syllabus\n    What is Data Science? Why Data Science exists as a discipline?     The role of a Data Scientist. Other roles and tasks in Data Science.     Data Collection and Discovery. Data Provenance and Annotation.      Data Representation: Flat Files, Databases, Tidy Data.     Introduction to Analytics, Exploratory Data Analysis and Machine Learning.     Reproducible Research. Data-based products.      Examples of applications, case studies and project development.\n\nBibliography\nFor a good all-around reference in Data Science see The Art of Data Science: A Guide for Anyone who works with Data (Peng and Matsui 2015) or Doing Data Science: Straight Talk from the Frontline (O’Neil and Schutt 2013).\nFor information on data scientists profiles see Analyzing the Analyzers: An Introspective Survey of Data Scientists and Their Work (Harris, Murphy, and Vaisman 2013) or Data scientists at work (Gutierrez 2014).\n\n\nUnder Construction\n\nThis material is continually being revised and improved.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#Schedule",
    "href": "index.html#Schedule",
    "title": "About this Course",
    "section": "Schedule for 2025",
    "text": "Schedule for 2025\nFor the second term of 2025 our schedule is Fridays, from 1:30PM to 5:30PM.\n\n\n\n\n\n\n\nDay\nTopics\n\n\n\n\nJune 20\nLectures suspended today\n\n\nJune 27\nIntroduction to the course. Reading of About the Course and What is Data Science? lectures, rules for evaluation.\n\n\nJuly 4\nReading of Skills for Data Science lecture. Q&A about the first homework (if needed).\n\n\nJuly 11\n\n\n\nJuly 18\n\n\n\nJuly 25\n\n\n\nAugust 1\n\n\n\nAugust 8\n\n\n\nAugust 15\n\n\n\nAugust 22\n\n\n\nAugust 27\n\n\n\nSeptember 5\nPresentation and comments on the Capstone Project.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#homework",
    "href": "index.html#homework",
    "title": "About this Course",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\n\nAssignment\nDue date (before that day’s lecture!)\n\n\n\n\nHomework #1\nJuly 4\n\n\nTBD\nAugust 9\n\n\nCapstone Project\nSeptember 5",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "Resources/Slides/CAP394-2025-2-Skills.html",
    "href": "Resources/Slides/CAP394-2025-2-Skills.html",
    "title": "CAP-394",
    "section": "",
    "text": "This is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\n\n\n\n\n\n\nBasically, we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\n\n\n\n\n\n\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\n\n\n\n\n\n\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\n\n\n\n\n\n\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \n\n\n\n\n\n\nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\n\n\n\n\n\n\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\n\n\n\n\n\n\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\n\n\n\n\n\n\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\n\n\n\n\n\n\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\n\n\n\n\n\n\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\n\n\n\n\n\n\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\n\n\n\n\n\n\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\n\n\n\n\n\n\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\n\n\n\n\n\n\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\n\n\n\n\n\n\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\n\n\n\n\n\n\nOpinions on the use of LLMs (e.g., ChatGPT) for coding are divided. Some argue that relying on an LLM to generate code constitutes a form of cheating, as the author may not fully understand the solution. Others view LLMs as tools that can support and enhance the coding process. In the following slides, we will explore a historical perspective on writing code and offer some guidelines on the ethical use of LLMs in coding and data science.\n\n\n\n\n\n\nBefore the Internet, programmers learned primarily from coding books filled with examples. A common practice was to type out these examples, observe how they worked, and then adapt them to suit specific needs. Over time, programmers became capable of writing code from scratch, but they still frequently referred to books for details on commands and syntax.\n\n\n\n\n\n\nProgramming languages became more complex, and books covering their basics—along with catalogs of APIs and their functions—grew larger and more detailed. Eventually, much of this content became available online or downloadable, making it easier for programmers to consult as needed.\n\n\n\n\n\n\nOver time, a more structured approach emerged for finding coding references and examples—starting with general search engines and later shifting toward specialized websites.“Copying and Pasting from Stack Overflow” isn’t a real book, of course, but looking up code snippets there (or using ChatGPT) isn’t inherently wrong—as long as you verify the answers and make an effort to understand how and why they work.\n\n\n\n\n\n\nThere’s nothing inherently wrong with using LLMs to assist with coding. It can be seen as a natural evolution of earlier practices—such as copying code from books or the Internet and adapting it. However, in all cases, we should avoid blindly accepting what these sources provide, whether it’s a book, a website, or an AI model. Always test the code, run it, and adjust the details. This approach is far more effective when working with small code snippets than with large, complex applications.\n\n\n\n\n\n\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\n\n\n\n\n\n\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\n\n\n\n\n\n\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please write code in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\n\n\n\n\n\n\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically, we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\n\n\n\n\n\n\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\n\n\n\n\n\n\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\n\n\n\n\n\n\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\n\n\n\n\n\n\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\n\n\n\n\n\n\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\n\n\n\n\n\n\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\n\n\n\n\n\n\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\n\n\n\n\n\n\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\n\n\n\n\n\n\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\n\n\n\n\n\n\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\n\n\n\n\n\n\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\n\n\n\n\n\n\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\n\n\n\n\n\n\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\n\n\n\n\n\n\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\n\n\n\n\n\n\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\n\n\n\n\n\n\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\n\n\n\n\n\n\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\n\n\n\n\n\n\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\n\n\n\n\n\n\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\n\n\n\n\n\n\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\n\n\n\n\n\n\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.\n\n\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\nBasically, we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\nOpinions on the use of LLMs (e.g., ChatGPT) for coding are divided. Some argue that relying on an LLM to generate code constitutes a form of cheating, as the author may not fully understand the solution. Others view LLMs as tools that can support and enhance the coding process. In the following slides, we will explore a historical perspective on writing code and offer some guidelines on the ethical use of LLMs in coding and data science.\nBefore the Internet, programmers learned primarily from coding books filled with examples. A common practice was to type out these examples, observe how they worked, and then adapt them to suit specific needs. Over time, programmers became capable of writing code from scratch, but they still frequently referred to books for details on commands and syntax.\nProgramming languages became more complex, and books covering their basics—along with catalogs of APIs and their functions—grew larger and more detailed. Eventually, much of this content became available online or downloadable, making it easier for programmers to consult as needed.\nOver time, a more structured approach emerged for finding coding references and examples—starting with general search engines and later shifting toward specialized websites.“Copying and Pasting from Stack Overflow” isn’t a real book, of course, but looking up code snippets there (or using ChatGPT) isn’t inherently wrong—as long as you verify the answers and make an effort to understand how and why they work.\nThere’s nothing inherently wrong with using LLMs to assist with coding. It can be seen as a natural evolution of earlier practices—such as copying code from books or the Internet and adapting it. However, in all cases, we should avoid blindly accepting what these sources provide, whether it’s a book, a website, or an AI model. Always test the code, run it, and adjust the details. This approach is far more effective when working with small code snippets than with large, complex applications.\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please write code in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically, we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow."
  },
  {
    "objectID": "Resources/Slides/CAP394-2025-2-Skills.html#slides",
    "href": "Resources/Slides/CAP394-2025-2-Skills.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "This is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\n\n\n\n\n\n\nBasically, we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\n\n\n\n\n\n\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\n\n\n\n\n\n\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\n\n\n\n\n\n\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \n\n\n\n\n\n\nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\n\n\n\n\n\n\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\n\n\n\n\n\n\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\n\n\n\n\n\n\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\n\n\n\n\n\n\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\n\n\n\n\n\n\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\n\n\n\n\n\n\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\n\n\n\n\n\n\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\n\n\n\n\n\n\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\n\n\n\n\n\n\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\n\n\n\n\n\n\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\n\n\n\n\n\n\nOpinions on the use of LLMs (e.g., ChatGPT) for coding are divided. Some argue that relying on an LLM to generate code constitutes a form of cheating, as the author may not fully understand the solution. Others view LLMs as tools that can support and enhance the coding process. In the following slides, we will explore a historical perspective on writing code and offer some guidelines on the ethical use of LLMs in coding and data science.\n\n\n\n\n\n\nBefore the Internet, programmers learned primarily from coding books filled with examples. A common practice was to type out these examples, observe how they worked, and then adapt them to suit specific needs. Over time, programmers became capable of writing code from scratch, but they still frequently referred to books for details on commands and syntax.\n\n\n\n\n\n\nProgramming languages became more complex, and books covering their basics—along with catalogs of APIs and their functions—grew larger and more detailed. Eventually, much of this content became available online or downloadable, making it easier for programmers to consult as needed.\n\n\n\n\n\n\nOver time, a more structured approach emerged for finding coding references and examples—starting with general search engines and later shifting toward specialized websites.“Copying and Pasting from Stack Overflow” isn’t a real book, of course, but looking up code snippets there (or using ChatGPT) isn’t inherently wrong—as long as you verify the answers and make an effort to understand how and why they work.\n\n\n\n\n\n\nThere’s nothing inherently wrong with using LLMs to assist with coding. It can be seen as a natural evolution of earlier practices—such as copying code from books or the Internet and adapting it. However, in all cases, we should avoid blindly accepting what these sources provide, whether it’s a book, a website, or an AI model. Always test the code, run it, and adjust the details. This approach is far more effective when working with small code snippets than with large, complex applications.\n\n\n\n\n\n\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\n\n\n\n\n\n\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\n\n\n\n\n\n\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please write code in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\n\n\n\n\n\n\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically, we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\n\n\n\n\n\n\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\n\n\n\n\n\n\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\n\n\n\n\n\n\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\n\n\n\n\n\n\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\n\n\n\n\n\n\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\n\n\n\n\n\n\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\n\n\n\n\n\n\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\n\n\n\n\n\n\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\n\n\n\n\n\n\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\n\n\n\n\n\n\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\n\n\n\n\n\n\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\n\n\n\n\n\n\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\n\n\n\n\n\n\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\n\n\n\n\n\n\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\n\n\n\n\n\n\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\n\n\n\n\n\n\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\n\n\n\n\n\n\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\n\n\n\n\n\n\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\n\n\n\n\n\n\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\n\n\n\n\n\n\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\n\n\n\n\n\n\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.\n\n\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\nBasically, we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\nOpinions on the use of LLMs (e.g., ChatGPT) for coding are divided. Some argue that relying on an LLM to generate code constitutes a form of cheating, as the author may not fully understand the solution. Others view LLMs as tools that can support and enhance the coding process. In the following slides, we will explore a historical perspective on writing code and offer some guidelines on the ethical use of LLMs in coding and data science.\nBefore the Internet, programmers learned primarily from coding books filled with examples. A common practice was to type out these examples, observe how they worked, and then adapt them to suit specific needs. Over time, programmers became capable of writing code from scratch, but they still frequently referred to books for details on commands and syntax.\nProgramming languages became more complex, and books covering their basics—along with catalogs of APIs and their functions—grew larger and more detailed. Eventually, much of this content became available online or downloadable, making it easier for programmers to consult as needed.\nOver time, a more structured approach emerged for finding coding references and examples—starting with general search engines and later shifting toward specialized websites.“Copying and Pasting from Stack Overflow” isn’t a real book, of course, but looking up code snippets there (or using ChatGPT) isn’t inherently wrong—as long as you verify the answers and make an effort to understand how and why they work.\nThere’s nothing inherently wrong with using LLMs to assist with coding. It can be seen as a natural evolution of earlier practices—such as copying code from books or the Internet and adapting it. However, in all cases, we should avoid blindly accepting what these sources provide, whether it’s a book, a website, or an AI model. Always test the code, run it, and adjust the details. This approach is far more effective when working with small code snippets than with large, complex applications.\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please write code in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically, we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow."
  },
  {
    "objectID": "31-Visualizing-Shapefiles.html",
    "href": "31-Visualizing-Shapefiles.html",
    "title": "Visualizing Geographic Information",
    "section": "",
    "text": "In this section we will see some recipes to display geographic information (i.e. data with have some spatial context) and that can be visualized using maps and choropleths.\nHere are some useful definitions:",
    "crumbs": [
      "Visualization",
      "Visualizing Geographic Information"
    ]
  },
  {
    "objectID": "31-Visualizing-Shapefiles.html#before-we-start",
    "href": "31-Visualizing-Shapefiles.html#before-we-start",
    "title": "Visualizing Geographic Information",
    "section": "Before we start…",
    "text": "Before we start…\nMost of the data used in this section can be downloaded from the Instituto Brasileiro de Geografia e Estatística - Malha Municipal’s site. The data is stored in shapefiles, files with the same name but different file extensions that contains the coordinates for the geographic objects, projection, associated data, etc.\nFiles downloaded from the IBGE site are zip (compressed) files containing all files associated with that shapefile. When reading shapefiles we only need to open the .shp file – all associated files will be open and read automatically. In these examples we assume that the zip files were downloaded and stored in local folder.\nLet’s see how to read a shapefile and get basic information. First let’s import all the libraries we will use in this section.\n\nimport pandas as pd\nimport geopandas as gpd\nimport json\nimport plotly.express as px\nimport plotly.colors as pc\nimport plotly.graph_objects as go\n\n\n\nAbout Plotly\n\nI prefer to use Plotly for visualization – there are other alternatives but I think it is more flexible and the plots and charts are interactive and visually more attractive.\n\nLet’s read the Brazil’s states shapefile:\n\n# Path to the .shp file.\nshapefile_path = \"Resources/Data/Shapefiles/BR_UF_2024.shp\"\n# Read shapefile\ngdf = gpd.read_file(shapefile_path)\n\nWhat’s in the shapefile? Let’s display its first few rows.\n\nprint(gdf.head())\n\n  CD_UF           NM_UF SIGLA_UF CD_REGIA NM_REGIA SIGLA_RG     AREA_KM2  \\\n0    35       São Paulo       SP        3  Sudeste       SE   248219.485   \n1    15            Pará       PA        1    Norte        N  1245828.829   \n2    32  Espírito Santo       ES        3  Sudeste       SE    46074.448   \n3    12            Acre       AC        1    Norte        N   164082.960   \n4    13        Amazonas       AM        1    Norte        N  1558706.127   \n\n                                            geometry  \n0  MULTIPOLYGON (((-48.03541 -25.35682, -48.0355 ...  \n1  MULTIPOLYGON (((-50.84599 -9.80064, -51.05801 ...  \n2  MULTIPOLYGON (((-40.88336 -21.16372, -40.88345...  \n3  POLYGON ((-68.39021 -11.04496, -68.39073 -11.0...  \n4  POLYGON ((-67.51732 -9.56071, -67.51776 -9.560...  \n\n\nFor the states’ shapefile we have a dataframe with one state per record, with information on names, abbreviations of the state and region, its area and geometry – this is a set of geometric structures and coordinates that will be used to draw the data boundaries.\nWe can use the data in the shapefiles to do some queries (e.g. which tis the largest state in the NE region), but we don’t need to use the geometry directly – there are functions that use it.\nUsually shapefiles have some type of index (in this example, CD_UF) that can be associated to an external data source to create, for example, rich choropleths.\n\nGeometry simplification\nAs mentioned earlier, shapefiles store coordinates that define the shapes of geographic features. Official maps often include highly detailed polygons with a large number of coordinate points.\nIn this section, we’ll display maps on a computer screen. Even when zooming in, we rarely need that level of detail. Using the original, high-resolution coordinates increases memory usage and computational load — even simple tasks like rendering a map in a web page can become noticeably slow. For this reason, it’s often necessary to reduce the geometric complexity of shapefiles before displaying them.\nThere is a simple method that can be used to reduce the complexity of geometries in shapefiles: simplify. Here is an example of its usage:\n\nshapefile_path = \"Resources/Data/Shapefiles/SP_UF_2024.shp\"\nshapeSP = gpd.read_file(shapefile_path)\n# Create a simplified copy.\nshapeSP_simplified = shapeSP.copy()\nshapeSP_simplified[\"geometry\"] = \\\n  shapeSP_simplified[\"geometry\"].simplify(tolerance=0.001, preserve_topology=False)\n\nPlease refer to shapely‘s documentation for information and example of the methods’ parameters.\nUsing simplify may trigger a warning in some cases, particularly when a geometry is smaller than the specified tolerance. To avoid this, you can try using a smaller tolerance value. In practice, it may take some adjustment to find an appropriate value, but for simple visualization purposes, the warning can often be safely ignored.\n\n\nWarning\n\n\nNote on Using Shapefiles and GeoJSON with Plotly\n\nWhile shapefiles and GeoJSONs are great for working with geographic data, be aware that Plotly can struggle with large or highly detailed geometries — especially when rendering complex polygons or large datasets directly in the browser. If your maps are slow to render or fail to load, consider simplifying your geometries or using lower-resolution data.\n\n\nFor more advanced visualization of large-scale or high-resolution spatial data, consider using tools like Kepler.gl, Leaflet, or deck.gl, which are designed to handle large geographic datasets more efficiently. These tools are beyond the scope of this section, but worth exploring for heavy-duty mapping needs.",
    "crumbs": [
      "Visualization",
      "Visualizing Geographic Information"
    ]
  },
  {
    "objectID": "31-Visualizing-Shapefiles.html#one-layer-brazils-boundaries",
    "href": "31-Visualizing-Shapefiles.html#one-layer-brazils-boundaries",
    "title": "Visualizing Geographic Information",
    "section": "One layer: Brazil’s boundaries",
    "text": "One layer: Brazil’s boundaries\nLet’s start with a very simple map, containing the coordinates for Brazil’s boundaries. Let’s read the shapefile:\n\n# Path to the .shp file.\nshapefile_path = \"Resources/Data/Shapefiles/BR_Pais_2024.shp\"\n# Read shapefile\nshapeBR_Raw = gpd.read_file(shapefile_path)\n# Simplify the geometries for faster processing and rendering!\nshapeBR = shapeBR_Raw.copy()\nshapeBR[\"geometry\"] = shapeBR[\"geometry\"].simplify(tolerance=0.001, preserve_topology=False)\n# How many records do I have?\nprint(len(shapeBR))\n\n1\n\n\nAs expected, the shapefile for Brazil contains only one record.\nTo display the shapefile we loaded, first we create a choropleth using the shapefile as the source of the data and a field of the shapefile as the source of the (GeoJSON) geographic coordinates. We will also use the field PAIS to set the color of the map and set the fields’ values that will appear when we hover on the map.\n\nfig = px.choropleth(\n    shapeBR,\n    geojson=shapeBR.__geo_interface__,\n    locations=shapeBR.index,\n    color=\"PAIS\",  \n    hover_name=\"PAIS\",\n    color_discrete_sequence=[\"#009440\"],\n    custom_data=[\"PAIS\", \"AREA_KM2\"],\n    projection=\"mercator\"\n)\n\nThe code below sets the format of the hover message, adjusts the bounds of the figure so it will fill the whole plot area and set some values for the maps’ appearance:\n\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;\" +  # Name\n                  \"Area: %{customdata[1]:,.0f} km²&lt;br&gt;\" +  # Area\n                  \"&lt;extra&gt;&lt;/extra&gt;\"  # Hide trace name\n)\nfig.update_layout(\n        title=\"Brazil\",\n        showlegend=False,\n        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}\n    )\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nYou can zoom and hover the map for more information!\nJust for fun, let’s see what happens when we oversimplify this shapefiles’ geometries. First we create another version of the simplified shapefile, with a much larger tolerance (much simpler shapes):\n\n# Simplify the geometries for faster processing and rendering!\nshapeBR2 = shapeBR_Raw.copy()\nshapeBR2[\"geometry\"] = \\\n    shapeBR2[\"geometry\"].simplify(tolerance=0.75, preserve_topology=False)\n\nAnd display it:\n\nfig = px.choropleth(\n    shapeBR2,\n    geojson=shapeBR2.__geo_interface__,\n    locations=shapeBR2.index,\n    color=\"PAIS\",  \n    color_discrete_sequence=[\"#009440\"],\n    projection=\"mercator\"\n)\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_traces(hovertemplate=\"&lt;extra&gt;&lt;/extra&gt;\")  # disables hover\nfig.update_layout(\n        title=\"Brazil\",\n        showlegend=False,\n        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}\n    )\n\n                            \n                                            \n\n\nThe features are still recognizable even with a much larger tolerance!",
    "crumbs": [
      "Visualization",
      "Visualizing Geographic Information"
    ]
  },
  {
    "objectID": "31-Visualizing-Shapefiles.html#one-layer-boundaries-of-brazils-states",
    "href": "31-Visualizing-Shapefiles.html#one-layer-boundaries-of-brazils-states",
    "title": "Visualizing Geographic Information",
    "section": "One layer: Boundaries of Brazil’s States",
    "text": "One layer: Boundaries of Brazil’s States\nWe can reuse the code to display a map of Brazil’s states. We need only to load a different shapefile:\n\n# Path to the .shp file.\nshapefile_path = \"Resources/Data/Shapefiles/BR_UF_2024.shp\"\n# Read shapefile\nshapeUF_Raw = gpd.read_file(shapefile_path)\n# Simplify the geometries for faster processing and rendering!\nshapeUF = shapeUF_Raw.copy()\nshapeUF[\"geometry\"] = shapeUF[\"geometry\"].simplify(tolerance=0.05, preserve_topology=False)\n# How many records do I have?\nprint(len(shapeUF))\n\n27\n\n\nLet’s display it, reusing the code for the whole country. We will change the thickness of the lines.\n\nfig = px.choropleth(\n    shapeUF,\n    geojson=shapeUF.__geo_interface__,\n    locations=shapeUF.index,\n    color=\"CD_UF\",  \n    hover_name=\"NM_UF\",\n    custom_data=[\"NM_UF\",\"NM_REGIA\",\"AREA_KM2\"],\n    color_discrete_sequence=[\"#009440\"],\n    projection=\"mercator\"\n)\nfig.update_traces(marker_line_width=2, marker_line_color=\"yellow\")\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;\" +  # Name\n                  \"Region %{customdata[1]}&lt;br&gt;\" +  # Region\n                  \"Area: %{customdata[2]:,.0f} km²&lt;br&gt;\" +  # Area\n                  \"&lt;extra&gt;&lt;/extra&gt;\"  # Hide trace name\n)\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_layout(\n        title=\"States of Brazil\",\n        showlegend=False,\n        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}\n    )",
    "crumbs": [
      "Visualization",
      "Visualizing Geographic Information"
    ]
  },
  {
    "objectID": "31-Visualizing-Shapefiles.html#one-layer-boundaries-of-brazils-states-different-colors-per-state",
    "href": "31-Visualizing-Shapefiles.html#one-layer-boundaries-of-brazils-states-different-colors-per-state",
    "title": "Visualizing Geographic Information",
    "section": "One layer: Boundaries of Brazil’s States (different colors per state)",
    "text": "One layer: Boundaries of Brazil’s States (different colors per state)\nIn the previous example all the states were displayed with the same color. Let’s see how we can show each one in a different color:\n\nfig = px.choropleth(\n    shapeUF,\n    geojson=shapeUF.__geo_interface__,\n    locations=shapeUF.index,\n    color=\"CD_UF\",  \n    hover_name=\"NM_UF\",\n    custom_data=[\"NM_UF\",\"NM_REGIA\",\"AREA_KM2\"],    \n    color_discrete_sequence=px.colors.qualitative.Alphabet,\n    projection=\"mercator\"\n)\nfig.update_traces(marker_line_width=2, marker_line_color=\"#808080\")\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;\" +  # Name\n                  \"Region %{customdata[1]}&lt;br&gt;\" +  # Region\n                  \"Area: %{customdata[2]:,.0f} km²&lt;br&gt;\" +  # Area\n                  \"&lt;extra&gt;&lt;/extra&gt;\"  # Hide trace name\n)\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_layout(\n        title=\"States of Brazil\",\n        showlegend=False,\n        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}\n    )\n\n                            \n                                            \n\n\nWe can create discrete color palettes from continuous ones with this:\n\ndef my_colorscale(n, scale='Rainbow'):\n    return pc.sample_colorscale(pc.get_colorscale(scale), [i / (n - 1) for i in range(n)])\n\nAnd use it to plot the maps:\n\nfig = px.choropleth(\n    shapeUF,\n    geojson=shapeUF.__geo_interface__,\n    locations=shapeUF.index,\n    color=\"CD_UF\",  \n    hover_name=\"NM_UF\",\n    custom_data=[\"NM_UF\",\"NM_REGIA\",\"AREA_KM2\"],    \n    color_discrete_sequence=my_colorscale(27,'Viridis'), \n    projection=\"mercator\"\n)\nfig.update_traces(marker_line_width=2, marker_line_color=\"#808080\")\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;\" +  # Name\n                  \"Region %{customdata[1]}&lt;br&gt;\" +  # Region\n                  \"Area: %{customdata[2]:,.0f} km²&lt;br&gt;\" +  # Area\n                  \"&lt;extra&gt;&lt;/extra&gt;\"  # Hide trace name\n)\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_layout(\n        title=\"States of Brazil\",\n        showlegend=False,\n        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}\n    )",
    "crumbs": [
      "Visualization",
      "Visualizing Geographic Information"
    ]
  },
  {
    "objectID": "31-Visualizing-Shapefiles.html#one-layer-boundaries-of-brazils-states-colors-based-on-area",
    "href": "31-Visualizing-Shapefiles.html#one-layer-boundaries-of-brazils-states-colors-based-on-area",
    "title": "Visualizing Geographic Information",
    "section": "One layer: Boundaries of Brazil’s States (colors based on area)",
    "text": "One layer: Boundaries of Brazil’s States (colors based on area)\nLet’s create a simple choropleth by using the information about each state area to assign a color to it. If the information is already associated to the dataframe of the shapefile it is just a case of selecting the field as the color and using a proper continuous scale:\n\nfig = px.choropleth(\n    shapeUF,\n    geojson=shapeUF.__geo_interface__,\n    locations=shapeUF.index,\n    color=\"AREA_KM2\",  \n    hover_name=\"NM_UF\",\n    custom_data=[\"NM_UF\",\"NM_REGIA\",\"AREA_KM2\"],    \n    color_continuous_scale='YlGnBu',\n    projection=\"mercator\"\n)\nfig.update_traces(marker_line_width=2, marker_line_color=\"#808080\")\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;\" +  # Name\n                  \"Region %{customdata[1]}&lt;br&gt;\" +  # Region\n                  \"Area: %{customdata[2]:,.0f} km²&lt;br&gt;\" +  # Area\n                  \"&lt;extra&gt;&lt;/extra&gt;\"  # Hide trace name\n)\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_layout(\n        title=\"States of Brazil\",\n        showlegend=False,\n        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n        coloraxis_colorbar=dict(\n           title=\"Área (km²)\",  \n           tickformat=\".0f\",    \n           lenmode=\"pixels\", len=450,  \n           thickness=20          \n        )\n    )",
    "crumbs": [
      "Visualization",
      "Visualizing Geographic Information"
    ]
  },
  {
    "objectID": "31-Visualizing-Shapefiles.html#one-layer-boundaries-of-brazils-states-conditional-coloring",
    "href": "31-Visualizing-Shapefiles.html#one-layer-boundaries-of-brazils-states-conditional-coloring",
    "title": "Visualizing Geographic Information",
    "section": "One layer: Boundaries of Brazil’s States (conditional coloring)",
    "text": "One layer: Boundaries of Brazil’s States (conditional coloring)\nEventually we would like to highlight some polygons on the choropleth based on conditional information. This is easy to do if we create another column on the dataframe that will be used as a filter and that will create a category associated to each row.\nLet’s see a simple example: I want to annotate all states in the shapefile with a field that will indicate if the state is on the North region. Here’s the code to do this:\n\nshapeUF[\"isNorth\"] = (shapeUF[\"NM_REGIA\"] == \"Norte\").map({True: \"Yes\", False: \"No\"})\n\nThis seems redundant since we already have a field that indicates that the state is on the North region, but creating an additional field will give us more flexibility later.\nThe second step is to create a colormap that will be used when creating the cloropleth. The colormap must associate a color to each possible value of our filter column:\n\nmap_colors = {\"Yes\": \"#009440\",\"No\": \"#dddddd\"}\n\nNow we can plot the map, using basically the same approach as in other examples on this section:\n\nfig = px.choropleth(\n    shapeUF,\n    geojson=shapeUF.__geo_interface__,\n    locations=shapeUF.index,\n    color=\"isNorth\", \n    hover_name=\"NM_UF\",\n    custom_data=[\"NM_UF\", \"NM_REGIA\", \"AREA_KM2\"],\n    color_discrete_map=map_colors,\n    projection=\"mercator\"\n)\n\nfig.update_traces(marker_line_width=2, marker_line_color=\"#808080\")\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;\" +  \n                  \"Region %{customdata[1]}&lt;br&gt;\" +  \n                  \"Area: %{customdata[2]:,.0f} km²&lt;br&gt;\" +\n                  \"&lt;extra&gt;&lt;/extra&gt;\"\n)\n\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_layout(\n    title=\"States of the North Region of Brazil\",\n    showlegend=False,\n    margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}\n)\n\n                            \n                                            \n\n\nIt is easy to create more complex color rules using the data that is already part of the shapefile’s dataframe. Here is an example of a function that will return NE for states in the northeast, SE for regions in the southeast but only if the states’ area is larger than 100.000km². The function will return Other for states that does not match these criteria.\n\ndef mark_larger(row):\n    area = row[\"AREA_KM2\"]\n    region = row[\"NM_REGIA\"]\n    if (area &gt; 100000):\n        if region == \"Nordeste\":\n            return \"NE\"\n        elif region == \"Sudeste\":\n            return \"SE\"\n        else:\n            return \"Other\"\n    else:   \n        return \"Other\"        \n\nNow we can create a new column on the dataframe to represent the category of the state, accordingly to the function we created:\n\nshapeUF[\"Category\"] = shapeUF.apply(mark_larger, axis=1)\n\nWe need a map for the colors for each category:\n\nmap_colors = {\"NE\": \"#0D7DBD\",\"SE\": \"#F79322\",\"Other\": \"#dddddd\"}\n\nNow we can plot the map:\n\nfig = px.choropleth(\n    shapeUF,\n    geojson=shapeUF.__geo_interface__,\n    locations=shapeUF.index,\n    color=\"Category\", \n    hover_name=\"NM_UF\",\n    custom_data=[\"NM_UF\", \"NM_REGIA\", \"AREA_KM2\"],\n    color_discrete_map=map_colors,\n    projection=\"mercator\"\n)\n\nfig.update_traces(marker_line_width=2, marker_line_color=\"#808080\")\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;\" +  \n                  \"Region %{customdata[1]}&lt;br&gt;\" +  \n                  \"Area: %{customdata[2]:,.0f} km²&lt;br&gt;\" +\n                  \"&lt;extra&gt;&lt;/extra&gt;\"\n)\n\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_layout(\n    title=\"Larger States in the Northeast and Southeast Regions of Brazil\",\n    showlegend=False,\n    margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}\n)",
    "crumbs": [
      "Visualization",
      "Visualizing Geographic Information"
    ]
  },
  {
    "objectID": "31-Visualizing-Shapefiles.html#two-layers-boundaries-of-brazil-and-states",
    "href": "31-Visualizing-Shapefiles.html#two-layers-boundaries-of-brazil-and-states",
    "title": "Visualizing Geographic Information",
    "section": "Two layers: Boundaries of Brazil and States",
    "text": "Two layers: Boundaries of Brazil and States\nEventually we will need to plot more than one layer in a single map. When using plotly the trick is to start by the base or most important layer (usually the one with the data, and the one for which we want to have the hover function) and add other layers as traces.\nHere’s how to create the base layer:\n\nfig = px.choropleth(\n    shapeUF,\n    geojson=shapeUF.__geo_interface__,\n    locations=shapeUF.index,\n    color=\"CD_UF\",  \n    hover_name=\"NM_UF\",\n    custom_data=[\"NM_UF\", \"NM_REGIA\", \"AREA_KM2\"],\n    color_discrete_sequence=[\"#009440\"],\n    projection=\"mercator\",\n)\n\nWe can adjust some rendering options but we need to assign the update to the dummy vatiable _ to avoid the display of a temporary map:\n\n_ = fig.update_traces(marker_line_width=1, marker_line_color=\"#a0a0ff\")\n\nNow we create the other layer, that will be rendered in transparent color:\n\n# Outline layer (entire Brazil — no color fill, just black outline)\noutline = go.Choropleth(\n    geojson=shapeBR.__geo_interface__,\n    locations=shapeBR[\"PAIS\"],\n    featureidkey=\"properties.PAIS\",  \n    z=[0]*len(shapeBR),  # one z value per feature, any value\n    showscale=False,\n    marker_line_color='#808080',\n    marker_line_width=3,\n    colorscale=[[0, 'rgba(0,0,0,0)'], [1, 'rgba(0,0,0,0)']],\n    hoverinfo='skip',      # &lt;-- disables hover\n    hovertemplate=None,    # &lt;-- disables custom hover\n    name=\"Brasil\"\n)\n\nLet’s add the trace to the base map (again using a dummy variable):\n\n_ = fig.add_trace(outline)\n\nNow we can set the hover information, but only for the base layer:\n\nfor trace in fig.data:\n    if trace.type == 'choropleth' and trace.name != 'Brasil':\n        trace.hovertemplate = (\n            \"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;\" +  # Name\n            \"Region: %{customdata[1]}&lt;br&gt;\" +\n            \"Area: %{customdata[2]:,.0f} km²&lt;br&gt;\" +\n            \"&lt;extra&gt;&lt;/extra&gt;\"\n        )\n\nAnd set up boundaries and the title for the map:\n\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_layout(\n    title=\"States of Brazil\",\n    showlegend=False,\n    margin={\"r\": 0, \"t\": 40, \"l\": 0, \"b\": 0}\n)",
    "crumbs": [
      "Visualization",
      "Visualizing Geographic Information"
    ]
  },
  {
    "objectID": "20-Cookbook-Intro.html",
    "href": "20-Cookbook-Intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis section will contain some examples on how to solve common data science tasks using Python. Usually there is relation with some examples we’ll see elsewhere in this site, but will be shown in more detail in this section.",
    "crumbs": [
      "Cookbook",
      "Introduction"
    ]
  },
  {
    "objectID": "50-Projects-Intro.html",
    "href": "50-Projects-Intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis section will contain some Data Science Projects that are used in our lectures.\nProjects are somehow complex tasks around a specific dataset that requires data wrangling, organization, EDA, visualization, and in some cases application of machine learning techniques.\nMost projects will include ideas for class discussions and several exercises.\nProjects that are ready to be explored are on the sidebar on the left. Many of those projects are incomplete: identifying further development possibilities is left as an exercise to the students.",
    "crumbs": [
      "Projects",
      "Introduction"
    ]
  },
  {
    "objectID": "51-Projects-Amendments.html",
    "href": "51-Projects-Amendments.html",
    "title": "Parliamentary Amendments",
    "section": "",
    "text": "A parliamentary amendment is a proposal made by a member of parliament or a legislator to specify provisions in a bill or legislation that is being considered by the parliament. This term is commonly used in the legislative processes of various countries, including Brazil, often to earmark funds for specific projects.\nThis chapter presents a basic (and incomplete!) example of a data science approach to analysis of a very simple parliamentary amendment database. The data was obtained from a government site.\nThe examples in this chapter show how even a simple dataset can be explored and used to give ideas for more interesting questions that may require more data.",
    "crumbs": [
      "Projects",
      "Parliamentary Amendments"
    ]
  },
  {
    "objectID": "51-Projects-Amendments.html#functions-and-subfunctions",
    "href": "51-Projects-Amendments.html#functions-and-subfunctions",
    "title": "Parliamentary Amendments",
    "section": "Functions and Subfunctions",
    "text": "Functions and Subfunctions\nFunctions (Nome Função) and subfunctions (Nome Subfunção) are the categories of application of the amendments’ values. Let’s take a look at their combinations and frequency.\n\ncfunctions = df.groupby([\"Nome Função\",\"Nome Subfunção\"]).size().reset_index(name='Count')\ncfunctions\n\n\n\n\n\n\n\n\n\nNome Função\nNome Subfunção\nCount\n\n\n\n\n0\nAdministração\nAdministração geral\n26\n\n\n1\nAdministração\nComunicação social\n4\n\n\n2\nAdministração\nControle interno\n4\n\n\n3\nAdministração\nDesenvolvimento científico\n1\n\n\n4\nAdministração\nDireitos individuais, coletivos e difusos\n2\n\n\n...\n...\n...\n...\n\n\n204\nUrbanismo\nOutras transferências\n2\n\n\n205\nUrbanismo\nPlanejamento e orçamento\n2\n\n\n206\nUrbanismo\nServiços urbanos\n7\n\n\n207\nUrbanismo\nTransportes coletivos urbanos\n46\n\n\n208\nUrbanismo\ninfra-estrutura urbana\n4090\n\n\n\n\n209 rows × 3 columns\n\n\n\n\nWe can create sunburst charts to show the distribution of amendments by functions and subfunctions. For consistency we will create a colormap for the sunburst charts that ensure the use of the same color for the same function:\n\nfunctionsU = cfunctions[\"Nome Função\"].unique()\ncolors = px.colors.qualitative.Plotly\n# Create a dictionary to map each \"Nome Função\" to a specific color\ncolormap = {function: colors[i % len(colors)] for i, function in enumerate(functionsU)}\n\nWith the grouped data and colormap we can create the sunburst chart with the code below:\n\n# Create a Sunburst chart\nsunburstC = px.sunburst(cfunctions, \n                        path=['Nome Função', 'Nome Subfunção'], \n                        values='Count', \n                        title='Number of Amendments by Function and Subfunction',\n                        labels={'Nome Função': 'Function', 'Nome Subfunção': \\\n                            'Subfunction', 'Count': 'Number of Amendments'},\n                        color='Nome Função',\n                        color_discrete_map=colormap)\n# Display the chart\nsunburstC.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nLet’s do the same chart but considering the total amount of the amendments. All we need to do is create a different grouping of the data:\n\nvfunctions = df.groupby([\"Nome Função\", \"Nome Subfunção\"])[\"Valor\"].sum().reset_index(name='Amount')\nvfunctions\n\n\n\n\n\n\n\n\n\nNome Função\nNome Subfunção\nAmount\n\n\n\n\n0\nAdministração\nAdministração geral\n2.741300e+07\n\n\n1\nAdministração\nComunicação social\n7.571335e+05\n\n\n2\nAdministração\nControle interno\n9.034404e+06\n\n\n3\nAdministração\nDesenvolvimento científico\n7.836601e+06\n\n\n4\nAdministração\nDireitos individuais, coletivos e difusos\n1.955050e+06\n\n\n...\n...\n...\n...\n\n\n204\nUrbanismo\nOutras transferências\n6.500000e+05\n\n\n205\nUrbanismo\nPlanejamento e orçamento\n2.629205e+06\n\n\n206\nUrbanismo\nServiços urbanos\n6.343775e+06\n\n\n207\nUrbanismo\nTransportes coletivos urbanos\n5.076996e+08\n\n\n208\nUrbanismo\ninfra-estrutura urbana\n1.762114e+10\n\n\n\n\n209 rows × 3 columns\n\n\n\n\nThen plot the sunburst chart (using the same colormap as the previous chart to make comparisons easier):\n\n# Create a Sunburst chart\nsunburstV = px.sunburst(vfunctions, \n                        path=['Nome Função', 'Nome Subfunção'], \n                        values='Amount', \n                        title='Total Amount of Amendments by Function and Subfunction',\n                        labels={'Nome Função': 'Function', 'Nome Subfunção': \\\n                            'Subfunction', 'Count': 'Number of Amendments'},\n                        color='Nome Função',\n                        color_discrete_map=colormap)\n# Display the chart\nsunburstV.show()\n\n                            \n                                            \n\n\n\n\nClass Discussion\n\nDo a quick comparison between the two plots and find differences in the visibility of their slices.",
    "crumbs": [
      "Projects",
      "Parliamentary Amendments"
    ]
  },
  {
    "objectID": "51-Projects-Amendments.html#countrywide-amendments",
    "href": "51-Projects-Amendments.html#countrywide-amendments",
    "title": "Parliamentary Amendments",
    "section": "Countrywide Amendments",
    "text": "Countrywide Amendments\nHow many types of Nome Função (main function) do we have for countrywide amendments?\n\nfunctionCounts = dfCountry[\"Nome Função\"].value_counts()\nfunctionCounts\n\nNome Função\nSaúde                    1431\nDefesa nacional          1035\nDireitos da cidadania     318\nSegurança pública         266\nAgricultura               229\nCultura                   193\nEducação                  157\nEncargos especiais        146\nAssistência social        121\nCiência e Tecnologia      107\nOrganização agrária       104\nDesporto e lazer           98\nGestão ambiental           66\nTrabalho                   64\nUrbanismo                  58\nTransporte                 47\nMúltiplo                   41\nAdministração              28\nComércio e serviços        24\nComunicações               16\nPrevidência social         14\nRelações exteriores        13\nIndústria                  11\nSaneamento                  7\nEnergia                     6\nEssencial à justiça         2\nHabitação                   2\nName: count, dtype: int64\n\n\nAre all those functions spread evenly through the years? Let’s compare them with a plot. First we group the data by Nome Função and Ano da Emenda counting how many amendments we had in each group:\n\ncpy = dfCountry.groupby([\"Nome Função\",\"Ano da Emenda\"]).size().\\\n        reset_index(name='Count')\ncpy\n\n\n\n\n\n\n\n\n\nNome Função\nAno da Emenda\nCount\n\n\n\n\n0\nAdministração\n2016\n3\n\n\n1\nAdministração\n2017\n6\n\n\n2\nAdministração\n2019\n2\n\n\n3\nAdministração\n2020\n9\n\n\n4\nAdministração\n2022\n3\n\n\n...\n...\n...\n...\n\n\n208\nUrbanismo\n2020\n11\n\n\n209\nUrbanismo\n2021\n9\n\n\n210\nUrbanismo\n2022\n16\n\n\n211\nUrbanismo\n2023\n1\n\n\n212\nUrbanismo\n2024\n3\n\n\n\n\n213 rows × 3 columns\n\n\n\n\nThen we create a faceted line plot, with a facet for each Nome Função.\n\n# Create a faceted line plot\nfig = px.line(cpy, \n              x='Ano da Emenda', \n              y='Count', \n              color='Nome Função',\n              facet_col='Nome Função', \n              facet_col_wrap=3,\n              title='Amendments per Year per Function',\n              labels={'Ano da Emenda': 'Year', 'Count': 'Amendments', \n                      'Nome Função': ''},\n              markers=True)\n# Update layout for better spacing and readability\nfig.update_layout(\n    height=1020,\n    showlegend=False,\n    margin=dict(t=80) \n)\n# Remove the automatic subplot titles\nfor annotation in fig['layout']['annotations']:\n    annotation['text'] = annotation['text'].split(\"=\")[-1]\n# Update x-axes to show ticks and labels for all subplots\nyears = cpy[\"Ano da Emenda\"].unique()\nyears.sort()\nfig.update_xaxes(showticklabels=True,tickvals=years,tickangle=90,\n                 tickfont=dict(size=10))  \nfig.update_yaxes(title_font=dict(size=10))\n# Display the plot\nfig.show()\n\n                            \n                                            \n\n\nNot all functions had amendments for each year in our data – some have gaps, while some happened only during a short period.\nThe plot above counted the number of amendments. Let’s modify it so it shows the total value of the amendments, under the same conditions. First we group the data:\n\n# Group by \"Nome Função\" and \"Ano da Emenda\" and sum the values\nspy = dfCountry.groupby([\"Nome Função\", \"Ano da Emenda\"])[\"Valor\"].sum().\\\n        reset_index(name='Total Value')\nspy\n\n\n\n\n\n\n\n\n\nNome Função\nAno da Emenda\nTotal Value\n\n\n\n\n0\nAdministração\n2016\n2.144076e+07\n\n\n1\nAdministração\n2017\n4.561856e+08\n\n\n2\nAdministração\n2019\n2.229047e+07\n\n\n3\nAdministração\n2020\n4.494842e+07\n\n\n4\nAdministração\n2022\n2.306679e+06\n\n\n...\n...\n...\n...\n\n\n208\nUrbanismo\n2020\n1.779921e+09\n\n\n209\nUrbanismo\n2021\n2.782494e+09\n\n\n210\nUrbanismo\n2022\n4.040173e+08\n\n\n211\nUrbanismo\n2023\n3.000242e+09\n\n\n212\nUrbanismo\n2024\n5.061627e+08\n\n\n\n\n213 rows × 3 columns\n\n\n\n\nThen we do the same faceted plot:\n\n# Create a faceted line plot\nfig = px.line(spy, \n              x='Ano da Emenda', \n              y='Total Value', \n              color='Nome Função',\n              facet_col='Nome Função', \n              facet_col_wrap=3,\n              title='Amendments Total Value per Year per Function',\n              labels={'Ano da Emenda': 'Year', 'Count': 'Amendmts', 'Nome Função': ''},\n              markers=True)\n# Update layout for better spacing and readability\nfig.update_layout(\n    height=990,\n    showlegend=False,\n    margin=dict(t=80)     \n)\n# Remove the automatic subplot titles\nfor annotation in fig['layout']['annotations']:\n    annotation['text'] = annotation['text'].split(\"=\")[-1]\n# Update x-axes to show ticks and labels for all subplots\nyears = spy[\"Ano da Emenda\"].unique()\nyears.sort()\nfig.update_xaxes(showticklabels=True,tickvals=years,tickangle=90,tickfont=dict(size=10))  \nfig.update_yaxes(title_font=dict(size=10))\n# Display the plot\nfig.show()\n\n                            \n                                            \n\n\n\n\nClass Discussion\n\nDo a quick comparison between the two plots to find functions and years with a large value but small number of amendments.\n\n\n\n \n\nDo the same faceted plot for your region, state and county. Point differences between the number of amendments and values.",
    "crumbs": [
      "Projects",
      "Parliamentary Amendments"
    ]
  },
  {
    "objectID": "51-Projects-Amendments.html#amendments-per-state",
    "href": "51-Projects-Amendments.html#amendments-per-state",
    "title": "Parliamentary Amendments",
    "section": "Amendments per State",
    "text": "Amendments per State\nLet’s take a look at the amendments per state dataframe and some subsets of it. Just for fun let’s use choropleths – maps that represent variables.\nWe can use a simple 3-step process to create a choropleth: first get one GeoJSON file ready for our purposes, then prepare our data so it matches what is on the GeoJSON then create the plot itself.\nA quick search on the web will show several sites with GeoJSON data that can be used for our project. Two are Moisés Lima’s brazil-states-geojson at Kaggle, and Giuliano Macedo geodata-br-states at Github. We’ll use this second one, which was downloaded and stored locally here.\nThen we need to prepare the subset of our data that contains states’ amendments only, and also rename the Localidade do gasto field so it contains only the states’ name in uppercase:\n\ndfStatesPrep = dfStates.groupby([\"Localidade do gasto\"]).size().reset_index(name='Count')\ndfStatesPrep['Localidade do gasto'] = \\\n  dfStatesPrep['Localidade do gasto'].str.replace(r'\\ \\(UF\\)', '', regex=True)\ndfStatesPrep\n\n\n\n\n\n\n\n\n\nLocalidade do gasto\nCount\n\n\n\n\n0\nACRE\n813\n\n\n1\nALAGOAS\n784\n\n\n2\nAMAPÁ\n690\n\n\n3\nAMAZONAS\n602\n\n\n4\nBAHIA\n2924\n\n\n5\nCEARÁ\n1452\n\n\n6\nDISTRITO FEDERAL\n961\n\n\n7\nESPÍRITO SANTO\n1228\n\n\n8\nGOIÁS\n1690\n\n\n9\nMARANHÃO\n1006\n\n\n10\nMATO GROSSO\n610\n\n\n11\nMATO GROSSO DO SUL\n830\n\n\n12\nMINAS GERAIS\n4413\n\n\n13\nPARANÁ\n2487\n\n\n14\nPARAÍBA\n1153\n\n\n15\nPARÁ\n1246\n\n\n16\nPERNAMBUCO\n2063\n\n\n17\nPIAUÍ\n780\n\n\n18\nRIO DE JANEIRO\n2932\n\n\n19\nRIO GRANDE DO NORTE\n1176\n\n\n20\nRIO GRANDE DO SUL\n2801\n\n\n21\nRONDÔNIA\n595\n\n\n22\nRORAIMA\n531\n\n\n23\nSANTA CATARINA\n1437\n\n\n24\nSERGIPE\n827\n\n\n25\nSÃO PAULO\n5216\n\n\n26\nTOCANTINS\n729\n\n\n\n\n\n\n\n\nThe GeoJSON file contains coordinates for the shapes of the object it contains and also some properties for each object. In our case we have a property named Estado that contains the state’s names. We will load the GeoJSON file and change the values of the Estado property so these will also be in uppercase:\n\nwith open(\"Resources/Data/Emendas/br_states.json\") as f:\n    geojson_data = json.load(f)\nfor feature in geojson_data['features']:\n    feature['properties']['Estado'] = feature['properties']['Estado'].upper()\n\nNow that we prepared the data and the GeoJSON file we can create the choropleth with Plotly with this code:\n\nfig = px.choropleth(\n    dfStatesPrep, # The dataframe\n    geojson=geojson_data, # The GeoJSON data\n    # These two parameters identify the field on the dataframe and \n    # property on the GeoJSON data that must match.\n    locations='Localidade do gasto', \n    featureidkey=\"properties.Estado\", \n    color='Count', \n    hover_name='Localidade do gasto',\n    hover_data=['Count'],\n    title='Number of Amendments per State',\n    color_continuous_scale=\"Viridis\"\n)\n# Update layout for better spacing and readability\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_layout(margin={\"r\":0,\"t\":35,\"l\":0,\"b\":0})\n# Display the map\nfig.show()\n\n                            \n                                            \n\n\n\n\n \n\nDo a similar choropleth for the total amount of amendments per state.\n\n\n\n \n\nDo a similar choropleth for a subset of the States’ data for a specific function (get a popular one!), showing either the number of amendments or total amount.\n\n\n\n \n\nFind a GeoJSON file for your state, and create a choropleth for all counties in your state with the total amount of amendments for a specific function.\n\n\n\n \n\nGet the population of states (and regions) from Wikipedia: Lista de unidades federativas do Brasil por população or Federative units of Brazil. Calculate and plot the number of amendments and total value per state considering the population. Which charts can help visualize and compare this data?",
    "crumbs": [
      "Projects",
      "Parliamentary Amendments"
    ]
  },
  {
    "objectID": "51-Projects-Amendments.html#amendments-per-author",
    "href": "51-Projects-Amendments.html#amendments-per-author",
    "title": "Parliamentary Amendments",
    "section": "Amendments per Author",
    "text": "Amendments per Author\nHow’s the distribution of amendments per Código do Autor da Emenda?\n\ncountAdE = df[\"Código do Autor da Emenda\"].value_counts()\n# Print the result\nprint(countAdE)\n\nCódigo do Autor da Emenda\nS/I     2481\n8100     331\n2776     190\n1775     186\n2616     182\n        ... \n2748       1\n1640       1\n2854       1\n2503       1\n4371       1\nName: count, Length: 1551, dtype: int64\n\n\nAmendments with author code S/I (Sem Informação , without information) are anonymous, therefore intriguing. Let’s explore it a bit, first creating two subsets of the original dataframe (and checking how many amendments’ authors are S/I)\n\ndfSI = df[df[\"Código do Autor da Emenda\"] == \"S/I\"]\ndfWI = df[df[\"Código do Autor da Emenda\"] != \"S/I\"]\nrecordSI = len(dfSI)\nprint(f\"Total records in dfSI DataFrame: {recordSI}\")\n\nTotal records in dfSI DataFrame: 2481\n\n\nIs the distribution of the unidentified authors’ amendments different from the ones from identified authors? Let’s check with sunburts plots, comparing the amounts by function and subfunction!\nFirst we create the groupings:\n\nSIdist = dfSI.groupby([\"Nome Função\",\"Nome Subfunção\"])[\"Valor\"].sum().reset_index(name='Amount')\nWIdist = dfWI.groupby([\"Nome Função\",\"Nome Subfunção\"])[\"Valor\"].sum().reset_index(name='Amount')\n\nNow the sunburst charts:\n\nsunburstSI = px.sunburst(SIdist, \n                         path=['Nome Função', 'Nome Subfunção'], \n                         values='Amount', \n                         title='Amount of SI Amendments by Function and Subfunction',\n                         labels={'Nome Função': 'Function', 'Nome Subfunção': \\\n                             'Subfunction', 'Count': 'Number of Amendments'},\n                         color='Nome Função',\n                         color_discrete_map=colormap)\n# Display the chart\nsunburstV.show()\n\n                            \n                                            \n\n\n\n# Create a Sunburst chart\nsunburstV = px.sunburst(WIdist, \n                        path=['Nome Função', 'Nome Subfunção'], \n                        values='Amount', \n                        title='Amount of WI Amendments by Function and Subfunction',\n                        labels={'Nome Função': 'Function', 'Nome Subfunção': \\\n                            'Subfunction', 'Count': 'Number of Amendments'},\n                        color='Nome Função',\n                        color_discrete_map=colormap)\n# Display the chart\nsunburstV.show()\n\n                            \n                                            \n\n\n\n\n \n\nDistributions of amendments with and without the identification of the author are somehow similar, but some functions have a different order (total amount). Identify those and dig on the data for the values.\n\n\n\n \n\nAmendments authors’ can be identified by their names or by the – but we filtered this information on the Reading and Preprocessing section. Redo the filtered dataframes so we can have one for individual amendments, one for unidentified authors’ amendments and one for amendments where the author’s names start with “BANCADA DA” (a Party Parliamentary Group or Wing). These three subsets must complement each other in the sense that no record should appear in more than one subset and no records is out of any subset.\n\n\n\n \n\nBased on the results of the previous exercise, consider that most “Bancadas” are associated to a state. Are all amendments from a particular state group destined for that state? Analyze the cases where this happens and where it doesn’t.\n\nLet’s take a last look at the identified/non-identified amendments. How’s their count per year?\nFirst we group the separate dataframes and label them so we can use them in the same chart:\n\n# For the ones without identification\ngrDfSI = dfSI.groupby(\"Ano da Emenda\").size().reset_index(name='Count')\ngrDfSI['Type'] = 'Without Id.'\n# For the ones with identification\ngrDfWI = dfWI.groupby(\"Ano da Emenda\").size().reset_index(name='Count')\ngrDfWI['Type'] = 'With Id.'\n# Combine them\nbothDfs = pd.concat([grDfSI,grDfWI])\n\nNow we plot the total of amendments per year:\n\nfig = px.line(bothDfs, x=\"Ano da Emenda\", y=\"Count\", color='Type',\n              title=\"Total Amount of Amendments per Year\",\n              labels={\"Ano da Emenda\": \"Year\", \"Count\": \"Number of Amendments\", \"Type\": \"Type\"})\nfig.show()\n\n                            \n                                            \n\n\nNo amendments without the authors’ identification were submitted after 2020!\nGoing back to the distribution of the amendments per author, we can see that there are some authors (or groups) that are more prolific. How many amendments they create per year? Let’s consider the top 12 authors:\n\ntop12 = dfWI[\"Código do Autor da Emenda\"].value_counts().nlargest(12).index\ndftop12 = dfWI[dfWI[\"Código do Autor da Emenda\"].isin(top12)]\ndfG = dftop12.groupby([\"Código do Autor da Emenda\", \"Ano da Emenda\"]).size().\\\n               reset_index(name='Count')\n\nNow let’s plot how many amendments each authored per year.\n\nfig = px.line(dfG, x=\"Ano da Emenda\", y=\"Count\", color=\"Código do Autor da Emenda\",\n              markers=True,\n              title=\"Number of Amendments per Year by Top 12 Authors\",\n              labels={\"Ano da Emenda\": \"Year\", \"Count\": \"Number of Amendments\",\n                      \"Código do Autor da Emenda\": \"Author\"})\nfig.update_yaxes(type='log')\n# Display the plot\nfig.show()\n\n                            \n                                            \n\n\n\n\n \n\nWho is the author whose code is 8100? Should we consider this data? If not, filter the amendments from this author and redo the chart.",
    "crumbs": [
      "Projects",
      "Parliamentary Amendments"
    ]
  },
  {
    "objectID": "51-Projects-Amendments.html#amendments-per-functions-and-years",
    "href": "51-Projects-Amendments.html#amendments-per-functions-and-years",
    "title": "Parliamentary Amendments",
    "section": "Amendments per Functions and Years",
    "text": "Amendments per Functions and Years\nLet’s pick up some interesting functions (Defesa nacional, Educação and Saúde) and compare the total value of the amendments they received per year. We’ll consider amendments that were destined to the whole country.\n\ndfDefense = dfCountry[dfCountry[\"Nome Função\"] == \"Defesa nacional\"]\ndfDefenseY = dfDefense.groupby(\"Ano da Emenda\")[\"Valor\"].sum().reset_index()\ndfDefenseY['Type'] = 'Defesa Nacional'\ndfEducation = dfCountry[dfCountry[\"Nome Função\"] == \"Educação\"]\ndfEducationY = dfEducation.groupby(\"Ano da Emenda\")[\"Valor\"].sum().reset_index()\ndfEducationY['Type'] = 'Educação'\ndfHealth = dfCountry[dfCountry[\"Nome Função\"] == \"Saúde\"]\ndfHealthY = dfHealth.groupby(\"Ano da Emenda\")[\"Valor\"].sum().reset_index()\ndfHealthY['Type'] = 'Saúde'\n# Combine them\nbothDfs = pd.concat([dfDefenseY,dfEducationY,dfHealthY])\n\nThen create the chart:\n\nfig = px.line(bothDfs, x=\"Ano da Emenda\", y=\"Valor\", color='Type',\n              title=\"Total Amount of Amendments per Year\",\n              labels={\"Ano da Emenda\": \"Year\", \"Count\": \"Number of Amendments\",\\\n                      \"Type\": \"Type\"})\nfig.show()\n\n                            \n                                            \n\n\n\n\n \n\nHow would the information on this chart compare to amendments destined for states or regions?",
    "crumbs": [
      "Projects",
      "Parliamentary Amendments"
    ]
  },
  {
    "objectID": "Resources/Slides/CAP394-2025-0-AboutCourse.html",
    "href": "Resources/Slides/CAP394-2025-0-AboutCourse.html",
    "title": "CAP-394",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!"
  },
  {
    "objectID": "Resources/Slides/CAP394-2025-0-AboutCourse.html#slides",
    "href": "Resources/Slides/CAP394-2025-0-AboutCourse.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!"
  },
  {
    "objectID": "Resources/Slides/CAP394-2025-1-WhatIs.html",
    "href": "Resources/Slides/CAP394-2025-1-WhatIs.html",
    "title": "CAP-394",
    "section": "",
    "text": "This is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data."
  },
  {
    "objectID": "Resources/Slides/CAP394-2025-1-WhatIs.html#slides",
    "href": "Resources/Slides/CAP394-2025-1-WhatIs.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "This is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data."
  },
  {
    "objectID": "02-Lectures-Skills.html",
    "href": "02-Lectures-Skills.html",
    "title": "Skills for Data Science",
    "section": "",
    "text": "This is the material (slides and notes) of the second lecture on the course.",
    "crumbs": [
      "Lectures",
      "Skills for Data Science"
    ]
  },
  {
    "objectID": "02-Lectures-Skills.html#slides",
    "href": "02-Lectures-Skills.html#slides",
    "title": "Skills for Data Science",
    "section": "Slides",
    "text": "Slides\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\n\n\n\n\n\n\nBasically, we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\n\n\n\n\n\n\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\n\n\n\n\n\n\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\n\n\n\n\n\n\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \n\n\n\n\n\n\nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\n\n\n\n\n\n\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\n\n\n\n\n\n\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\n\n\n\n\n\n\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\n\n\n\n\n\n\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\n\n\n\n\n\n\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\n\n\n\n\n\n\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\n\n\n\n\n\n\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\n\n\n\n\n\n\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\n\n\n\n\n\n\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\n\n\n\n\n\n\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\n\n\n\n\n\n\nOpinions on the use of LLMs (e.g., ChatGPT) for coding are divided. Some argue that relying on an LLM to generate code constitutes a form of cheating, as the author may not fully understand the solution. Others view LLMs as tools that can support and enhance the coding process. In the following slides, we will explore a historical perspective on writing code and offer some guidelines on the ethical use of LLMs in coding and data science.\n\n\n\n\n\n\nBefore the Internet, programmers learned primarily from coding books filled with examples. A common practice was to type out these examples, observe how they worked, and then adapt them to suit specific needs. Over time, programmers became capable of writing code from scratch, but they still frequently referred to books for details on commands and syntax.\n\n\n\n\n\n\nProgramming languages became more complex, and books covering their basics—along with catalogs of APIs and their functions—grew larger and more detailed. Eventually, much of this content became available online or downloadable, making it easier for programmers to consult as needed.\n\n\n\n\n\n\nOver time, a more structured approach emerged for finding coding references and examples—starting with general search engines and later shifting toward specialized websites.“Copying and Pasting from Stack Overflow” isn’t a real book, of course, but looking up code snippets there (or using ChatGPT) isn’t inherently wrong—as long as you verify the answers and make an effort to understand how and why they work.\n\n\n\n\n\n\nThere’s nothing inherently wrong with using LLMs to assist with coding. It can be seen as a natural evolution of earlier practices—such as copying code from books or the Internet and adapting it. However, in all cases, we should avoid blindly accepting what these sources provide, whether it’s a book, a website, or an AI model. Always test the code, run it, and adjust the details. This approach is far more effective when working with small code snippets than with large, complex applications.\n\n\n\n\n\n\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\n\n\n\n\n\n\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\n\n\n\n\n\n\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please write code in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\n\n\n\n\n\n\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically, we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\n\n\n\n\n\n\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\n\n\n\n\n\n\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\n\n\n\n\n\n\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\n\n\n\n\n\n\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\n\n\n\n\n\n\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\n\n\n\n\n\n\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\n\n\n\n\n\n\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\n\n\n\n\n\n\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\n\n\n\n\n\n\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\n\n\n\n\n\n\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\n\n\n\n\n\n\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\n\n\n\n\n\n\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\n\n\n\n\n\n\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\n\n\n\n\n\n\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\n\n\n\n\n\n\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\n\n\n\n\n\n\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\n\n\n\n\n\n\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\n\n\n\n\n\n\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\n\n\n\n\n\n\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\n\n\n\n\n\n\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\n\n\n\n\n\n\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.",
    "crumbs": [
      "Lectures",
      "Skills for Data Science"
    ]
  },
  {
    "objectID": "02-Lectures-Skills.html#code-for-examples-used-in-this-lecture",
    "href": "02-Lectures-Skills.html#code-for-examples-used-in-this-lecture",
    "title": "Skills for Data Science",
    "section": "Code for examples used in this lecture",
    "text": "Code for examples used in this lecture\n\nDiagram for Time Spent/Enjoyable Activities\nThe pie charts in the slides used in this lecture were copied from a dead link (will be redirected to another site), but since we have the percentages it would be fairly simple to reproduce it.\nFirst we import the (Johnson et al. 2018) library:\n\nimport plotly.graph_objects as go\n\nLet’s create the data structures in Python:\n\ntasks = ['Collecting Data', 'Cleaning and Organizing', 'Building Training Sets',\n         'Mining Data', 'Refining Algorithms', 'Other']\nperTime = [19, 60, 3, 9, 4, 5]\nperUnenjoy = [21, 57, 10, 3, 4, 5]\n\nI’d like to use a pastel color scheme:\n\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0', '#ffb3e6']\n\nLet’s create the chart for “most spent part” and set some layour options:\n\nfig_time = go.Figure(data=[go.Pie(\n    labels=tasks,\n    values=perTime,\n    hole=0.4,\n    marker=dict(colors=colors),\n    textinfo='percent',\n    textposition='outside',\n    hoverinfo='label+percent',\n    sort=False, # Disable sorting to keep the order consistent\n    textfont=dict(family=\"Arial Black\",size=18,color=\"#808080\") \n)])\n\nfig_time.update_layout(\n    title='What data scientists spend the most time doing',\n    showlegend=True,\n    legend=dict(title=\"Tasks\", x=1.15, y=0.5, \n                traceorder=\"normal\", font=dict(size=12))\n)\nfig_time.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nDo more or less the same for the “lest enjoyable part” chart:\n\nfig_unenjoy = go.Figure(data=[go.Pie(\n    labels=tasks,\n    values=perUnenjoy,\n    hole=0.4,\n    marker=dict(colors=colors),\n    textinfo='percent',\n    textposition='outside',\n    hoverinfo='label+percent',\n    sort=False,  # Disable sorting to keep the order consistent\n    textfont=dict(family=\"Arial Black\",size=18,color=\"#808080\") \n)])\n\nfig_unenjoy.update_layout(\n    title=\"What's the least enjoyable part of data science?\",\n    showlegend=True,\n    legend=dict(title=\"Tasks\", x=1.15, y=0.5, \n                traceorder=\"normal\", font=dict(size=12))\n)\nfig_unenjoy.show()\n\n                            \n                                            \n\n\n\n\nAverage Temperature Basic Example\nIn this notebook we will do a very simple data science project: plot the Earth’s average land temperature and see if it is getting higher. The original data came from the Berkeley Earth site, an independent U.S. non-profit organization focused on environmental data science. The original data file can be downloaded here or here (a local copy). It is a text file with annual and five-year average temperatures and respective uncertainties.\nThe first 30 lines of that file are shown below:\n% This file contains a brief summary of the land-surface average results \n% produced by the Berkeley Averaging method.  Temperatures are in \n% Celsius and reported as anomalies relative to the Jan 1951-Dec 1980\n% average. Uncertainties represent the 95% confidence interval for \n% statistical and spatial undersampling effects.\n% \n% The current dataset presented here is described as: \n% \n%   Estimated Global Land-Surface TAVG based on the Complete Berkeley Dataset\n% \n% \n% This analysis was run on 07-Feb-2022 15:32:40\n% \n% Results are based on 50590 time series \n%   with 20945177 data points\n% \n% Estimated Jan 1951-Dec 1980 absolute temperature (C): 8.60 +/- 0.06\n% \n% \n% \n% Year, Annual Anomaly, Annual Unc., Five-year Anomaly, Five-year Unc.\n \n  1750      -1.220          NaN              NaN             NaN\n  1751      -1.311          NaN              NaN             NaN\n  1753      -0.955        1.005              NaN             NaN\n  1754      -0.379        0.934              NaN             NaN\n  1755      -0.698        0.980           -0.553           0.608\n  1756      -0.421        1.596           -0.831           0.586\n  1757      -0.310        0.896           -1.024           0.612\n  1758      -2.345        1.366           -1.347           0.882\nLet’s use Python (Rossum and Jr. 2001), Pandas (McKinney 2012) and Matplotlib (Hunter 2007) for the analysis scripts, which will read the data and plot the year versus the corrected annual average temperature, and after that let’s see if we can see a trend using a basic linear model.\nFirst we import the libraries we’re going to use.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nNow we can read the file into a data frame – more on this later in the course. If you want to reproduce this make sure you have the file in the right path. Note that we have to explicitely name the columns for this data set.\n\ninputfile = \"Resources/Data/AverageTemperature/Complete_TAVG_summary.txt\"\ndata = pd.read_csv(inputfile,sep=\"\\\\s+\",skiprows=22,header=None)\ndata.columns = [\"Year\",\"Annual.Anomaly\",\"Annual.Uncertainty\",\n                \"FiveYear.Anomaly\",\"FiveYear.Uncertainty\"]\n\nLet’s see how the data looks (as a data frame):\n\n data \n\n\n\n\n\n\n\n\n\nYear\nAnnual.Anomaly\nAnnual.Uncertainty\nFiveYear.Anomaly\nFiveYear.Uncertainty\n\n\n\n\n0\n1750\n-1.220\nNaN\nNaN\nNaN\n\n\n1\n1751\n-1.311\nNaN\nNaN\nNaN\n\n\n2\n1753\n-0.955\n1.005\nNaN\nNaN\n\n\n3\n1754\n-0.379\n0.934\nNaN\nNaN\n\n\n4\n1755\n-0.698\n0.980\n-0.553\n0.608\n\n\n...\n...\n...\n...\n...\n...\n\n\n266\n2017\n1.306\n0.035\n1.290\n0.037\n\n\n267\n2018\n1.145\n0.052\n1.345\n0.036\n\n\n268\n2019\n1.345\n0.048\n1.307\n0.035\n\n\n269\n2020\n1.499\n0.036\nNaN\nNaN\n\n\n270\n2021\n1.240\n0.038\nNaN\nNaN\n\n\n\n\n271 rows × 5 columns\n\n\n\n\nWe want to plot the temperature, but we have the anomaly. All we need to do is to add 8.65ºC to the anomaly (see a detailed explanation at this NASA’s Earth Observatory site or NOAA’s National Centers for Environmental Information).\n\ndata[\"Annual\"] = data[\"Annual.Anomaly\"]+8.65\n\nNow let’s plot the annual temperature against the year:\n\nax = data.plot(kind=\"line\",x=\"Year\",y=\"Annual\")\nplt.show()\n\n\n\n\n\n\n\n\nLet’s fit a linear regression model that will basically describe temperature a function of the year using the data we have. Don’t worry about the math details for the time being.\n\nmodel = LinearRegression()\nX = pd.DataFrame(data[\"Year\"])\nY = pd.DataFrame(data[\"Annual\"])\nmodel.fit(X,Y)\nY_pred = model.predict(X)\n\nY_pred contains the predicted temperature values from the years. We can now create another plot that shows the original data and the linear model as a straigth line in red.\n\nplt.plot(data[\"Year\"],data[\"Annual\"])\nplt.plot(data[\"Year\"],Y_pred,color=\"red\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\nBasically, we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\nOpinions on the use of LLMs (e.g., ChatGPT) for coding are divided. Some argue that relying on an LLM to generate code constitutes a form of cheating, as the author may not fully understand the solution. Others view LLMs as tools that can support and enhance the coding process. In the following slides, we will explore a historical perspective on writing code and offer some guidelines on the ethical use of LLMs in coding and data science.\nBefore the Internet, programmers learned primarily from coding books filled with examples. A common practice was to type out these examples, observe how they worked, and then adapt them to suit specific needs. Over time, programmers became capable of writing code from scratch, but they still frequently referred to books for details on commands and syntax.\nProgramming languages became more complex, and books covering their basics—along with catalogs of APIs and their functions—grew larger and more detailed. Eventually, much of this content became available online or downloadable, making it easier for programmers to consult as needed.\nOver time, a more structured approach emerged for finding coding references and examples—starting with general search engines and later shifting toward specialized websites.“Copying and Pasting from Stack Overflow” isn’t a real book, of course, but looking up code snippets there (or using ChatGPT) isn’t inherently wrong—as long as you verify the answers and make an effort to understand how and why they work.\nThere’s nothing inherently wrong with using LLMs to assist with coding. It can be seen as a natural evolution of earlier practices—such as copying code from books or the Internet and adapting it. However, in all cases, we should avoid blindly accepting what these sources provide, whether it’s a book, a website, or an AI model. Always test the code, run it, and adjust the details. This approach is far more effective when working with small code snippets than with large, complex applications.\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please write code in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically, we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.",
    "crumbs": [
      "Lectures",
      "Skills for Data Science"
    ]
  },
  {
    "objectID": "00-Homework-01.html",
    "href": "00-Homework-01.html",
    "title": "Homework #1",
    "section": "",
    "text": "How to deliver\n\nPlease deliver the answer to this homework in a single PDF file. Use figures, but try to keep it under three pages.\n\n\n\nFind a source of open data on the web (see some suggestions below). Try to get a source of data that you can relate to. Write one or two paragraphs about what it contains, who publishes it, what it can used for.\nGet at least one of the files from that source. How easy/hard was it? How well-documented is it? Does it looks tidy?\nAlternatively get a data source related to your own research and describe it.\n\n\n\nA badly categorized list.\n\n\n\nCatálogo IPEVSC – Bases de Dados Online.pdf\nPortal da Transparência - Dados Abertos\nPortal Brasileiro de Dados Abertos\nDados Abertos - Instituto Brasileiro de Geografia e Estatística\nPortal Brasileiro de Dados Abertos do Banco Central\nPortal da Transparência - Controladoria Geral da União\nGoverno Aberto SP\nPortal de Dados Abertos do Ministério da Educação\nipeadata\nAgência Nacional de Aviação Civil (Anac) - Dados Abertos\nDados Abertos da Agência Nacional de Águas e Saneamento Básico\nCatálogo de Dados Abertos do Senado Federal\nRemuneração dos Magistrados\nArquivo Nacional - Outras bases de dados\n\n\n\n\n\nU.S. Government’s Open Data\nWorld Bank Open Data\nUNdata\nEuropean data - The official portal for European data\nWorld Health Organization Data\nGlobal Biodiversity Information Facility\nCDC logoCenters for Disease Control and Prevention Data\n\n\n\n\n\nCopernicus Climate Data Store\nNASA Earth Observation Data\nNOAA Climate Data Online\n\n\n\n\n\nDownload PubMed Data\nTabela Brasileira de Composição de Alimentos\n\n\n\n\n\nHarvard Dataverse\nOpenML\nUC Irvine Machine Learning Repository\nKaggle Datasets\nOur World in Data\nSciELO Data\nAgência de Bibliotecas e Coleções Digitais da Universidade de São Paulo\nBase dos Dados (ONG)\nSete conjuntos de dados públicos que você pode analisar gratuitamente agora mesmo"
  },
  {
    "objectID": "00-Homework-01.html#find-data",
    "href": "00-Homework-01.html#find-data",
    "title": "Homework #1",
    "section": "",
    "text": "Find a source of open data on the web (see some suggestions below). Try to get a source of data that you can relate to. Write one or two paragraphs about what it contains, who publishes it, what it can used for.\nGet at least one of the files from that source. How easy/hard was it? How well-documented is it? Does it looks tidy?\nAlternatively get a data source related to your own research and describe it."
  },
  {
    "objectID": "00-Homework-01.html#data-sources",
    "href": "00-Homework-01.html#data-sources",
    "title": "Homework #1",
    "section": "",
    "text": "A badly categorized list.\n\n\n\nCatálogo IPEVSC – Bases de Dados Online.pdf\nPortal da Transparência - Dados Abertos\nPortal Brasileiro de Dados Abertos\nDados Abertos - Instituto Brasileiro de Geografia e Estatística\nPortal Brasileiro de Dados Abertos do Banco Central\nPortal da Transparência - Controladoria Geral da União\nGoverno Aberto SP\nPortal de Dados Abertos do Ministério da Educação\nipeadata\nAgência Nacional de Aviação Civil (Anac) - Dados Abertos\nDados Abertos da Agência Nacional de Águas e Saneamento Básico\nCatálogo de Dados Abertos do Senado Federal\nRemuneração dos Magistrados\nArquivo Nacional - Outras bases de dados\n\n\n\n\n\nU.S. Government’s Open Data\nWorld Bank Open Data\nUNdata\nEuropean data - The official portal for European data\nWorld Health Organization Data\nGlobal Biodiversity Information Facility\nCDC logoCenters for Disease Control and Prevention Data\n\n\n\n\n\nCopernicus Climate Data Store\nNASA Earth Observation Data\nNOAA Climate Data Online\n\n\n\n\n\nDownload PubMed Data\nTabela Brasileira de Composição de Alimentos\n\n\n\n\n\nHarvard Dataverse\nOpenML\nUC Irvine Machine Learning Repository\nKaggle Datasets\nOur World in Data\nSciELO Data\nAgência de Bibliotecas e Coleções Digitais da Universidade de São Paulo\nBase dos Dados (ONG)\nSete conjuntos de dados públicos que você pode analisar gratuitamente agora mesmo"
  }
]