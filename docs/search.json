[
  {
    "objectID": "00-Homework-02.html",
    "href": "00-Homework-02.html",
    "title": "Homework #2",
    "section": "",
    "text": "How to deliver\n\nThis homework requires reading of the Supermarket Basket Analysis case study. Please deliver the answer to this homework either as an online notebook link or as a single, correctly formatted PDF file created from the notebook. Code, results (text or figures) and comments are required.\n\n\n\nChoose three exercises on the Supermarket Basket Analysis case study that are about counting and solve them.\n\n\n\nWe found some strange patterns on the Supermarket Basket Analysis case study – e.g. very large transactions, large amounts of items bought in some days. Check if those patterns are specific to a payment method or store branch or status of the transaction."
  },
  {
    "objectID": "00-Homework-02.html#playing-data-detective-part-1.",
    "href": "00-Homework-02.html#playing-data-detective-part-1.",
    "title": "Homework #2",
    "section": "",
    "text": "Choose three exercises on the Supermarket Basket Analysis case study that are about counting and solve them."
  },
  {
    "objectID": "00-Homework-02.html#playing-data-detective-part-2.",
    "href": "00-Homework-02.html#playing-data-detective-part-2.",
    "title": "Homework #2",
    "section": "",
    "text": "We found some strange patterns on the Supermarket Basket Analysis case study – e.g. very large transactions, large amounts of items bought in some days. Check if those patterns are specific to a payment method or store branch or status of the transaction."
  },
  {
    "objectID": "53-Projects-Supermarket.html",
    "href": "53-Projects-Supermarket.html",
    "title": "Supermarket Basket Analysis",
    "section": "",
    "text": "This chapter presents an extensive (but incomplete!) example of a data science approach to analysis of a supermarket transaction database. The data was obtained from an anonymous Brazilian supermarket and preprocessed to make some examples easier to follow and modify.\nThis chapter also serves as an example on how an exploratory report could be organized.",
    "crumbs": [
      "Projects",
      "Supermarket Basket Analysis"
    ]
  },
  {
    "objectID": "53-Projects-Supermarket.html#checking-and-adjusting-the-dataframe",
    "href": "53-Projects-Supermarket.html#checking-and-adjusting-the-dataframe",
    "title": "Supermarket Basket Analysis",
    "section": "Checking and adjusting the dataframe",
    "text": "Checking and adjusting the dataframe\nHow many records and fields did we just read? The shape of the dataframe can be used to answer this, showing the number of rows and columns.\n\nsupermDF.shape\n\n(363966, 16)\n\n\nCSV is a text-based format, when reading it into a dataframe Pandas guesses which is the best data type for the columns. Let’s check the data types for our dataframe:\n\nprint(supermDF.dtypes)\n\nFileID            object\nStoreID            int64\nPOSID              int64\nTransactionID     object\nDate               int64\nitemOrder          int64\nDescription       object\nUnitPrice        float64\nQuantity         float64\nAmount           float64\nUnit              object\nPaymentMethod     object\nTotalPaid        float64\nTotalDue         float64\nChange           float64\nStatus            object\ndtype: object\n\n\nThe field Date was read as an int64 value, let’s convert it to a real date/time field.\n\n# Convert the field Date to a string, which is used by pd.to_datetime with a specific\n# format to be converted into an array of datetime object, which is stored in our\n# dataframe using a new column name.\nsupermDF['DateTime'] = pd.to_datetime(supermDF['Date'].astype(str),format='%Y%m%d%H%M%S')",
    "crumbs": [
      "Projects",
      "Supermarket Basket Analysis"
    ]
  },
  {
    "objectID": "53-Projects-Supermarket.html#how-many-different-products-does-the-supermarket-sells",
    "href": "53-Projects-Supermarket.html#how-many-different-products-does-the-supermarket-sells",
    "title": "Supermarket Basket Analysis",
    "section": "How many different products does the supermarket sells?",
    "text": "How many different products does the supermarket sells?\nIn order to answer this question we would need access to different data: a complete catalogue of the products the supermarket sells. But we can get the answer to how many different products the supermarket sold by looking at the Description field and counting unique values. Here’s how:\n\n# Get the column Description, create a series with the unique values on it, \n# return the length of this series. \nlen(pd.unique(supermDF['Description']))\n\n4791\n\n\nThis one line of code slices the dataframe, selecting only the column/field Description and passes it as an argument to a the function unique of the Pandas library that eliminates all repeated items and returns a list, which then is used as an argument to len that counts the elements on that list.\nAnother way to get the number of different products sold by the supermarket is to get a count of occurrences of each item, which can be easily done with a call to value_counts function of a slice of a dataframe:\n\n# Show a summary of the counts for all values of the column Description.\nsupermDF['Description'].value_counts()\n\nDescription\nPAO FRANCES               19648\nACUCAR REFINADO A. ALE     5067\nOLEO SOJA SOYA 900ML P     4800\nQUEIJO MUSSARELA           4694\nFLV CEBOLA                 4342\n                          ...  \nCERA POLIFLOR LIQ. INC        1\nALCACHOFRA C\\10               1\nESP. HIKARI PAPRICA DO        1\nDOCE KISABOR P.PE MOLE        1\nBRINQ. FESTLAR 10UN AN        1\nName: count, Length: 4791, dtype: int64\n\n\nWe can check which are the top ten sellers (by quantity) with:\n\n# Show the top ten items of the counts for all values of the column Description.\nsupermDF['Description'].value_counts().head(10)\n\nDescription\nPAO FRANCES               19648\nACUCAR REFINADO A. ALE     5067\nOLEO SOJA SOYA 900ML P     4800\nQUEIJO MUSSARELA           4694\nFLV CEBOLA                 4342\nFLV TOMATE CARMEM          4178\nBEB. REF.COCA COLA 2L      3303\nFLV BANANA PRATA           3252\nFLV ALHO                   3196\nFLV LARANJA POCAN          3162\nName: count, dtype: int64\n\n\nAnd even filter by quantity using a range:\n\n# Store the counts in a series.\ncounts = supermDF['Description'].value_counts()\n# Filter those between two values.\ncounts[counts.between(1000,1200)]\n\nDescription\nAGUA SANIT. FAZ.ESPERA    1190\nLEITE COND. MOCA 395G     1153\nLEITE PAST. SERRAMAR S    1149\nGELATINA DABARRA 85 GR    1139\nFLV BATATA BOLINHA        1138\nBACON REZENDE KG          1138\nBOV.PALETA                1137\nREFRESCO DA BARRA 35G     1127\nFLV BATATA LAVADA         1122\nMARGARINA QUALY 500G C    1113\nBOV. CONTRA FILE          1100\nBOV.MUSCULO               1075\nFLV PIMENTAO VERDE        1048\nSAL ITA 1KG               1042\nMORTADELA MARBA           1039\nFLV TAKAGAKI ALFACE LI    1014\nLEITE COND. ITAMBE 395    1010\nName: count, dtype: int64\n\n\n\n\n \n\nHow many different payment methods are there in our data? Which is the most popular?\n\n\n\n \n\nHow many transactions failed? How many succeeded?",
    "crumbs": [
      "Projects",
      "Supermarket Basket Analysis"
    ]
  },
  {
    "objectID": "53-Projects-Supermarket.html#whats-the-average-shopping-cart-cost",
    "href": "53-Projects-Supermarket.html#whats-the-average-shopping-cart-cost",
    "title": "Supermarket Basket Analysis",
    "section": "What’s the average shopping cart cost?",
    "text": "What’s the average shopping cart cost?\nIn order to get this information we would need to scan all itens, adding the values for each transaction and calculate the average of these values. But the way that we chose to store the itens makes the first step easier: since all item entries on the dataframe contains information on the transaction it belongs to, we can get all itens where the field itemOrder is one (the first item on that transaction) and use the field TotalDue to get the transaction’s value.\nFirst we create a subset of our dataframe containing only the first item on the transaction:\n\n# Create a new dataframe with only the first item in each transaction. \nonlyFirstItems = supermDF.loc[supermDF['itemOrder'] == 1].copy()\n\nNow we can get the average transaction value with:\n\n# Prints the mean of all transactions' TotalDue.\nonlyFirstItems['TotalDue'].mean()\n\n10.99892718855733\n\n\nWe can even have some fun with subsets of the dataframe:\n\n# Get the mean of all transactions' TotalDue.\nts1 = onlyFirstItems.loc[onlyFirstItems['StoreID'] == 1,'TotalDue'].mean()\nts2 = onlyFirstItems.loc[onlyFirstItems['StoreID'] == 2,'TotalDue'].mean()\nts3 = onlyFirstItems.loc[onlyFirstItems['StoreID'] == 3,'TotalDue'].mean()\nts4 = onlyFirstItems.loc[onlyFirstItems['StoreID'] == 4,'TotalDue'].mean()\n# Format and print these values.\nprint(f\"Mean Total Due by Store: 1:{ts1:.2f}, 2:{ts2:.2f}, 3:{ts3:.2f}, 4:{ts4:.2f}.\")\n\nMean Total Due by Store: 1:10.72, 2:9.74, 3:12.28, 4:nan.\n\n\nAs expected there isn’t an average value for sales in the store number 4, since it does not exist:\n\n# Print a series with all unique values for StoreID.\npd.unique(supermDF['StoreID'])\n\narray([3, 2, 1])\n\n\n\n\n \n\nThe code onlyFirstItems.loc[onlyFirstItems['StoreID'] == '0002'] returns an empty dataframe. Verify and explain the reason.\n\nWe can also plot a simple histogram showing the distribution of the total values for the baskets/transactions. The key command is plt.hist that creates a histogram using the values of the TotalDue field:\n\n# Create the plot.\nfig = px.histogram(onlyFirstItems, x='TotalDue', nbins=20, \n          title='Histogram of total amount due for each transaction')\n# Update layout for labels and grid.\nfig.update_layout(\n    xaxis_title='Total Due',\n    yaxis_title='Frequency',\n    showlegend=False\n)\nfig.update_xaxes(showgrid=True)\nfig.update_yaxes(showgrid=True)\n# Display it.\nfig.show()\n\n                                                \n\n\nWe can see most of the values are clumped around R$ 0.00, with some traces above R$ 800.00. Let’s try to change the scale of the total amount due to get a better sense of the distribution.\n\nepsilon = 0.0001 # Avoid zeros.\nonlyFirstItems['LogTotalDue'] = np.log10(onlyFirstItems['TotalDue']+epsilon)\n\nThen we create a customized histogram with a log scale:\n\n# We will use custom tick values and labels.\ntickvals = np.log10([0.01,0.05,0.10,0.25,0.5,1,2,5,10,20,50,100,200,500,1000])\nticktext = ['0.01','0.05','0.10','0.25','0.5','1','2','5','10','20','50','100','200','500','1000']\n# Create the histogram.\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=onlyFirstItems['LogTotalDue'],\n    nbinsx=50,\n    hoverinfo='skip' # Very hard to make a working hover info!\n))\n# Update layout.\nfig.update_layout(\n    title='Histogram of total amount due for each transaction',\n    xaxis_title='Total Due',\n    yaxis_title='Frequency',\n     xaxis=dict(\n        tickmode='array',\n        tickvals=tickvals,\n        ticktext=ticktext,\n        type='linear'  \n    ),\n    yaxis=dict(\n        type='linear',\n        tickformat=',d'\n    ),\n    showlegend=False\n)\n# Add grids.\nfig.update_xaxes(showgrid=True)\nfig.update_yaxes(showgrid=True)\n# Display it.\nfig.show()\n\n                                                \n\n\n\n\n \n\nSome transactions costed less than one Real. What itens are for sale that cost under one Real? How many transactions are under one Real, how many under 20 cents of a Real?\n\nWe can also see that there is one transaction with value above 800 reais. Let’s take a closer look at this transaction.\n\n# Select only some fields for displaying.\nfields = ['TransactionID','Description','UnitPrice','Quantity','Amount']\n# Filter the dataframe locating only rows where TotalPaid is &gt;= 800, then sort the\n# filtered dataframe by Amount (from larger to smaller) then show the fields we chose.\nsupermDF.loc[supermDF['TotalPaid'] &gt;= 800].sort_values('Amount',ascending=False)[fields]\n\n\n\n\n\n\n\n\n\nTransactionID\nDescription\nUnitPrice\nQuantity\nAmount\n\n\n\n\n107012\n100425_03010508\nOLEO SOJA SOYA 900ML P\n1.99\n40.00\n79.60\n\n\n118981\n132950_03062294\nLEITE LV MILENIO 1L\n1.39\n24.00\n33.36\n\n\n324284\n153909_03063631\nARROZ CIAGRO T.1 5KG\n6.59\n5.00\n32.95\n\n\n69756\n173627_02005291\nCAFE ROSA DE OURO 500G\n2.98\n11.00\n32.78\n\n\n118995\n132950_03062294\nBOTIJAO GAZ 13kg\n27.90\n1.00\n27.90\n\n\n...\n...\n...\n...\n...\n...\n\n\n73077\n195817_01000396\nFLV CHUCHU\n0.39\n0.38\n0.15\n\n\n140116\n154349_02006671\nFLV LIMAO TAITI\n0.45\n0.33\n0.15\n\n\n232304\n085525_04054454\nFLV LIMAO TAITI\n0.39\n0.38\n0.15\n\n\n308992\n154012_03063111\nFLV CEBOLA\n1.29\n0.11\n0.14\n\n\n90038\n095706_02005833\nFLV LIMAO TAITI\n0.29\n0.34\n0.10\n\n\n\n\n1448 rows × 5 columns\n\n\n\n\nIt seems a large purchase, 212 itens in a transaction, but can still be considered normal.\n\n\n \n\nWe chose supermDF['TotalPaid'] &gt;= 800 as a filter, knowing from the histogram that there is only one transaction that costed more than 800 reais. But what if there was more than one transaction? How do I identify the transaction IDs of purchases above a certain value, and display them separately?\n\nJust out of curiosity let’s do the same chart for the TotalPaid values: we expect to have a similar distribution since people pay the same values that are due or slightly more (for cash transactions).\nFirst let’s create the log-scaled data:\n\nepsilon = 0.0001 # Avoid zeros.\nonlyFirstItems['LogTotalPaid'] = np.log10(onlyFirstItems['TotalPaid']+epsilon)\n\nThen the histogram:\n\n# We will use custom tick values and labels.\ntickvals = np.log10([0.01,0.05,0.10,0.25,0.5,1,2,5,10,20,50,100,200,500,1000,100000])\nticktext = ['0.01','0.05','0.10','0.25','0.5','1','2','5','10','20',\\\n            '50','100','200','500','1000','100000']\n# Create the histogram.\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=onlyFirstItems['LogTotalPaid'],\n    nbinsx=50,\n    hoverinfo='skip' # Very hard to make a working hover info!\n))\n# Update layout.\nfig.update_layout(\n    title='Histogram of total amount paid for each transaction',\n    xaxis_title='Total Due',\n    yaxis_title='Frequency',\n     xaxis=dict(\n        tickmode='array',\n        tickvals=tickvals,\n        ticktext=ticktext,\n        type='linear'  \n    ),\n    yaxis=dict(\n        type='linear',\n        tickformat=',d'\n    ),\n    showlegend=False\n)\n# Add grids.\nfig.update_xaxes(showgrid=True)\nfig.update_yaxes(showgrid=True)\n# Display it.\nfig.show()\n\n                                                \n\n\nThis can’t be right – almost all transactions’ due values are clumped together below 10 reais reais and the X axis indicates the existence of a payment of around half a million reais (while the highest due amount was around 800 reais).\nLet’s print the top ten transactions’s payment by value:\n\n# Create a list of columns we want to show.\nfields = ['TransactionID','TotalDue','TotalPaid','Change','Status']\n# Print the top 10 largest TotalDue values with the fields we chose.\nonlyFirstItems.nlargest(10,'TotalPaid')[fields]\n\n\n\n\n\n\n\n\n\nTransactionID\nTotalDue\nTotalPaid\nChange\nStatus\n\n\n\n\n92896\n121443_04084497\n5.78\n505050.0\n505044.22\nT\n\n\n267595\n200441_04090425\n1.74\n101100.0\n101098.26\nT\n\n\n159154\n182523_04052926\n6.46\n101050.0\n101043.54\nT\n\n\n141321\n164130_04052857\n2.05\n101010.1\n101008.05\nT\n\n\n24458\n191520_03008081\n96.46\n100100.0\n100003.54\nT\n\n\n69744\n173627_02005291\n82.81\n100100.0\n100017.19\nT\n\n\n107012\n100425_03010508\n79.60\n100100.0\n100020.40\nT\n\n\n250981\n114813_02028185\n7.56\n31060.0\n31052.44\nT\n\n\n40004\n090409_04082972\n1.90\n21010.0\n21008.10\nT\n\n\n171630\n105814_04053199\n182.69\n19000.0\n18817.31\nT\n\n\n\n\n\n\n\n\nWe can see that there are several transactions with large total paid values and changes – we don’t have an explanation for that, but it is clear that we need to consider the TotalPaid field for our analysis. This is a lesson into not jumping on the analysis before better understanding the data, and on how basic EDA can show us some issues with the data.\n\n\n \n\nList basic information on all due payments above R$ 1.000,00.\n\n\n\n \n\nCreate a metric (boolean or numeric) that could be used to indicate a suspicious transaction (with large amounts for TotalDue and Change, for example.)\n\n\n\n \n\nIs there a temporal pattern on those transactions? Do they occur more in some days or hours of day?\n\n\n\n \n\nAre these transactions specific to one store or occur in all stores of the supermarket?\n\n\n\n \n\nAre all those transactions paid for with cash?",
    "crumbs": [
      "Projects",
      "Supermarket Basket Analysis"
    ]
  },
  {
    "objectID": "53-Projects-Supermarket.html#are-there-patterns-on-the-number-of-items-in-the-carts",
    "href": "53-Projects-Supermarket.html#are-there-patterns-on-the-number-of-items-in-the-carts",
    "title": "Supermarket Basket Analysis",
    "section": "Are there patterns on the number of items in the carts?",
    "text": "Are there patterns on the number of items in the carts?\nHow many items our costumers buy in each transaction? Are there patterns or distributions on this amount? Let’s explore!\nFirst let’s see a histogram of the distribution of the number of items per transaction. In the way our data is organized, we need to infer this number since it is not explicit. To count the number of items per transaction we can group the dataframe by TransactionID and get the number of records in each group:\n\n# Group records by TransactionID, select TransactionID to extract a metric (size).\nitemsIT = supermDF.groupby('TransactionID')['TransactionID'].transform('size')\n# Add it as a new column.\nsupermDF['itemsInTransaction'] = itemsIT\n# We want only one record per transaction!\nonlyFirstItems = supermDF.loc[supermDF['itemOrder'] == 1].copy()\n\nThen we can create a histogram to show the distribution of items per transaction:\n\n# Create histogram.\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=onlyFirstItems['itemsInTransaction'],\n    nbinsx=50\n))\n# Update layout.\nfig.update_layout(\n    title='Histogram of number of items in basket',\n    xaxis_title='Number of Items',\n    yaxis_title='Frequency',\n    yaxis=dict(\n        type='log',\n        tickformat='~s'\n    ),\n    showlegend=False\n)\nfig.update_xaxes(showgrid=True)\nfig.update_yaxes(showgrid=True)\n# Display it.\nfig.show()\n\n                                                \n\n\nAs expected, most of the transactions contain a few items.\n\n\n \n\nTo count the number of items per transaction we could also get the maximum value for each itemOrder with the dataframe grouped by TransactionID. Try it!\n\nWe expect a sort of correlation between the number of items in a transaction and its total cost (TotalPaid). Let’s do a quick XY (or scatter) plot to verify this:\n\n# Create scatter plot using Plotly\nfig = px.scatter(\n    onlyFirstItems,\n    x='itemsInTransaction',\n    y='TotalDue',\n    title='Scatter Plot of Number of Items in Cart versus Total Due',\n    labels={\n        'itemsInTransaction': 'Number of Items in Cart',\n        'TotalPaid': 'Total Due'\n    }\n)\n# Show the plot\nfig.show()\n\n                                                \n\n\nLet’s enhance the plot with a linear regression line. We need first to fit a model using scikit-learn (Pedregosa et al. (2011)):\n\n# Fit linear regression model.\nX = np.array(onlyFirstItems['itemsInTransaction']).reshape(-1, 1)\ny = np.array(onlyFirstItems['TotalDue'])\nmodel = LinearRegression()\nmodel.fit(X, y)\ny_pred = model.predict(X)\n# Create the figure.\nfig = go.Figure()\n# Add scatter plot using Plotly Graph Objects.\nfig.add_trace(go.Scatter(\n    x=onlyFirstItems['itemsInTransaction'],\n    y=onlyFirstItems['TotalDue'],\n    mode='markers',\n    name='Data Points',\n    showlegend=False,\n    marker=dict(opacity=0.6)  # Adjust marker opacity\n))\n# Add linear regression line using Plotly Graph Objects.\nfig.add_trace(go.Scatter(\n    x=onlyFirstItems['itemsInTransaction'],\n    y=y_pred,\n    mode='lines',\n    name='Linear Regression',\n    line=dict(color='red'),\n    showlegend=False,\n))\n# Update the figure.\nfig.update_layout(\n    title='Number of Items in Basket x Total Due',\n    xaxis_title='Number of Items',\n    yaxis_title='Total Due',\n    showlegend=False\n)\n# Show the plot.\nfig.show()\n\n                                                \n\n\nThere are some outliers but the total paid for each transaction seems correlated with the number of items in each cart.\n\n\n \n\nThe regression line may be heavily influenced by the transactions with only a few items or with a low paymend due. Redo the plot but using only transactions with total above a cutoff, or with transactions with more than a specific number of items, or both.\n\nAre those transactions paid by cash? Can we see any pattern or outlier on the payment method? Let’s use different colors to help discriminate the payment methods:\n\n# Create the scatter plot with Plotly Express.\nfig = px.scatter(\n    onlyFirstItems,\n    x=\"itemsInTransaction\",\n    y=\"TotalDue\",\n    color=\"PaymentMethod\",\n    title='Scatter Plot of Number of Items in Cart versus Total Due',\n    labels={\n        'itemsInTransaction': 'Number of Items in Cart',\n        'TotalDue': 'Total Due'\n    }\n)\n# Show the plot.\nfig.show()\n\n                                                \n\n\n\n\n \n\nThere is a payment category without a title. See how many transactions are in this situation and propose a way to deal with this data.\n\nThat wasn’t very useful, since there are many options of payments, some that are somehow similar, so we could aggregate them. Let’s create a new column on our dataframe to represent a simplified payment method.\n\n# A simple function that simply the categories of payment method.\ndef simplifyPayment(paymethod):\n    if paymethod.startswith(\"Dinheiro\"):\n        return \"Dinheiro\"\n    elif paymethod.startswith(\"Cartao\"):\n        return \"Cartao\"\n    elif paymethod.startswith(\"Cheque\"):\n        return \"Cheque\"\n    else:\n        return \"Outros\"\n# Now we can add a new column based on the value of PaymentMethod.\nonlyFirstItems['SimplifiedPaymentMethod'] = \\\n    onlyFirstItems['PaymentMethod'].apply(simplifyPayment)      \n\nWith this new column we can plot a more informative scatter plot:\n\n# Define a custom color palette.\ncustom_color_palette = {\n    'Dinheiro': 'yellowgreen',\n    'Cartao'  : 'slateblue',\n    'Cheque'  : 'magenta',\n    'Outros'  : 'lightgrey'  \n}\n# Create the scatter plot with Plotly Express.\nfig = px.scatter(\n    onlyFirstItems,\n    x=\"itemsInTransaction\",\n    y=\"TotalDue\",\n    color=\"SimplifiedPaymentMethod\",\n    title='Scatter Plot of Number of Items in Cart versus Total Due',\n    labels={\n        'itemsInTransaction': 'Number of Items in Cart',\n        'TotalDue': 'Total Due'\n    },\n    color_discrete_map=custom_color_palette\n)\n# Show the plot.\nfig.show()\n\n                                                \n\n\n\n\nClass Discussion\n\nWhat are the other payment method mappings that may reveal hidden information on this data?\n\n\n\n \n\nSome transactions’ values were about 500 reais and paid by cash – take a closer look to see what is being sold.\n\n\n\n \n\nCreate plots to explore the relationship between itemsInTransaction and TotalPaid but using other fields to set the color – e.g. are the payment patterns the same for each of the three stores?\n\n\n\n \n\nAre the payment patterns the same for different hours of the day? For different days on the week?",
    "crumbs": [
      "Projects",
      "Supermarket Basket Analysis"
    ]
  },
  {
    "objectID": "53-Projects-Supermarket.html#are-there-temporal-patterns-on-the-sales-data",
    "href": "53-Projects-Supermarket.html#are-there-temporal-patterns-on-the-sales-data",
    "title": "Supermarket Basket Analysis",
    "section": "Are there temporal patterns on the sales data?",
    "text": "Are there temporal patterns on the sales data?\nWe can use EDA to investigate several different aspects of the data. Let’s consider the sale of a particular item, the most sold, \"PAO FRANCES           \". Is there a temporal pattern on its sale? We can answer that with a two-dimensional histogram that shows the amount of units sold by day and time.\nFirst let’s create a subset of the data containing only the item in question. From this subset let’s also keep only the relevant fields. Let’s also create columns for day of the month and time of the day:\n\n# Get only items that are equal to \"PAO FRANCES           \"\nonlyBreadRolls = supermDF.loc[supermDF['Description'] == \"PAO FRANCES           \"]\n# Select only some of the columns.\nonlyBreadRolls = onlyBreadRolls[['StoreID','DateTime','Quantity']]\n# Extract the day of month from the DateTime column, store in new column.\nonlyBreadRolls['DayOfMonth'] = onlyBreadRolls['DateTime'].dt.day\n# Extract the hour of day from the DateTime column, store in new column.\nonlyBreadRolls['HourOfDay'] = onlyBreadRolls['DateTime'].dt.hour\n\nWith this dataframe we can create the two-dimensional histogram, which is done by counting all values for each combination of the fields DayOfMonth and HourOfDay (see Sum column based on another column in Pandas DataFrame).\n\n# Sum the amounts for each combination of DayOfMonth and HourOfDay.\nhistoFB = onlyBreadRolls.groupby([\"DayOfMonth\",\"HourOfDay\"]).Quantity.sum().reset_index() \n\nThis two-dimensional histogram contains an entry for the sum of the field Quantity each combination of the fields DayOfMonth and HourOfDay, but if there weren’t specific combinations of DayOfMonth and HourOfDay the entry will not be created – the result of grouby does not return a matrix or dataframe with all possible combinations, only with the existing ones.\n\n\n \n\nPrint the contents of histoFB to understand better what was created.\n\nThe results can be used to create a heat map (a good visual representation for a two-dimensional histogram) but empty cells will not be displayed. To create a better visualization we can pad the two-dimensional so all possible combinations of DayOfMonth and HourOfDay with the non-occurring combinations filled with zeros (See Creating a rectangular Heatmap from two columns of a Pandas dataframe, Pandas: How to Replace NaN Values in Pivot Table with Zeros, pandas.DataFrame.reindex).\n\n# Reorders the two-dimensional histogram as a matrix-like dataframe.\nhmap = histoFB.pivot(index=\"HourOfDay\", columns=\"DayOfMonth\", values=\"Quantity\").fillna(0)\n# Use those indexes for the row and column indexes.\ndays = list(range(1, 32))\nhours = list(range(24))\n# Apply those indexes to the dataframe, filling nonexistent (na) values with zeros.\nhmap = hmap.reindex(index=hours).reindex(columns=days).fillna(0)\nhmap.to_csv(\"breadroll-heatmap.csv\")\n\nNow that we have our well-formed two-dimensional dataframe we can plot it as a heat map.\n\n# Create the heatmap.\nfig = go.Figure(data=go.Heatmap(\n    z=hmap.values,\n    x=hmap.columns,\n    y=hmap.index,\n    colorscale='Jet',\n    text=hmap.values,\n    texttemplate=\"%{text:.0f}\",  # Format text to display as integers\n    textfont={\"size\":10},\n    hoverinfo='z',  # Display value on hover\n    showscale=True\n))\n# Update layout to add thin lines around each cell.\n# This was suggested by Code Copilot!\nfig.update_traces(\n    zmin=0, zmax=hmap.values.max(),  \n    xgap=1,  \n    ygap=1,  \n    colorbar=dict(tickfont=dict(size=10))  \n)\n# Customize the layout.\nfig.update_layout(\n    title='Heatmap of Bread Rolls Sold',\n    xaxis_title='Day of Month',\n    yaxis_title='Hour of Day',\n    xaxis=dict(\n        tickmode='linear',\n        dtick=1,  \n        tickfont=dict(size=10),\n        showgrid=False,\n        zeroline=False # Remove grid lines\n    ),\n    yaxis=dict(\n        tickmode='linear',\n        dtick=1, \n        tickfont=dict(size=10),\n        showgrid=False,\n        zeroline=False # Remove grid lines\n    ),\n    plot_bgcolor='#808080', \n)\n# Invert the Y-axis.\nfig.update_yaxes(autorange='reversed')\n# Show the plot.\nfig.show()\n\n                                                \n\n\n\n\n \n\nVisualization is a very important component of EDA. The heat map summarized the sales of bread rolls over time, pointing to several interesting features. List some of those, with possible explanations. Consider that a feature could be the beginning of another exploration path to learn more about the data! Some of these possible paths will be used in the exercises below.\n\n\n\n \n\nWe used the color map ‘Jet’ which gives good-looking results but have some issues (colors for values under 350 are somehow similar). Try different color maps to see if others enhance some aspects of the data (see Built-in Continuous Color Scales in Python or Continuous Color Scales and Color Bars in Python).\n\n\n\n \n\nThe heat map shows the number of bread rolls sold by the whole supermarket (all stores). Recreate the heat maps for the different stores, one heat map per store. Do they appear to behave in a similar way?\n\n\n\n \n\nCreate new versions of the heat map separating bread rolls that were paid by cash, credit card and other payment methods. See first which payment methods are more common.\n\n\n\n \n\nWe noticed that there were some days and hours where a lot of bread rolls were sold. Do a quick analysis on how many buyers bought bread rolls at that time and day, and how many bread rolls each costumer bought.\n\n\n\n \n\nDo similar heat maps for three or four other items that are frequently sold. Choose any item that grabbed your attention! Analyze the heat maps to see if there are any patterns or oddities.",
    "crumbs": [
      "Projects",
      "Supermarket Basket Analysis"
    ]
  },
  {
    "objectID": "00-Homework-01.html",
    "href": "00-Homework-01.html",
    "title": "Homework #1",
    "section": "",
    "text": "How to deliver\n\nPlease deliver the answer to this homework in a single PDF file. Use figures, but try to keep it under three pages.\n\n\n\nThink about your expectatives about this course. Which one(s) of the descriptions in What is a data scientist? 14 definitions of a data scientist! applies to you? Which one(s) doesn’t?\nA local PDF of that site is stored here.\n\n\n\nFind a source of open data on the web. Try to get a source of data that you can relate to. Write one or two paragraphs about what it contains, who publishes it, what it can used for.\nGet at least one of the files from that source. How easy/hard was it? How well-documented is it? Does it looks tidy?\n\n\nHint\n\nIf you cannot find good open data sources and if you can read Portuguese you can always check up some government data sources, e.g. Portal Brasileiro de Dados Abertos, Portal da Transparência - Controladoria Geral da União, Governo Aberto SP, Portal de Dados Abertos do Ministério da Educação, Catálogo de Dados Abertos do Senado Federal, ipeadata, Dados Abertos da Agência Nacional de Águas e Saneamento Básico, Remuneração dos Magistrados, Harvard Dataverse .\n\n\n\n\nCheck out Google Colab, Kaggle, SciServer or other online notebook environments, play with them a little and select one (or more!) to write notebooks for this class. Alternatively, if you prefer, you can run your code locally. This tutorial on Sharing Notebooks may help."
  },
  {
    "objectID": "00-Homework-01.html#identify-yourself",
    "href": "00-Homework-01.html#identify-yourself",
    "title": "Homework #1",
    "section": "",
    "text": "Think about your expectatives about this course. Which one(s) of the descriptions in What is a data scientist? 14 definitions of a data scientist! applies to you? Which one(s) doesn’t?\nA local PDF of that site is stored here."
  },
  {
    "objectID": "00-Homework-01.html#find-open-data.",
    "href": "00-Homework-01.html#find-open-data.",
    "title": "Homework #1",
    "section": "",
    "text": "Find a source of open data on the web. Try to get a source of data that you can relate to. Write one or two paragraphs about what it contains, who publishes it, what it can used for.\nGet at least one of the files from that source. How easy/hard was it? How well-documented is it? Does it looks tidy?\n\n\nHint\n\nIf you cannot find good open data sources and if you can read Portuguese you can always check up some government data sources, e.g. Portal Brasileiro de Dados Abertos, Portal da Transparência - Controladoria Geral da União, Governo Aberto SP, Portal de Dados Abertos do Ministério da Educação, Catálogo de Dados Abertos do Senado Federal, ipeadata, Dados Abertos da Agência Nacional de Águas e Saneamento Básico, Remuneração dos Magistrados, Harvard Dataverse ."
  },
  {
    "objectID": "00-Homework-01.html#choose-an-environment.",
    "href": "00-Homework-01.html#choose-an-environment.",
    "title": "Homework #1",
    "section": "",
    "text": "Check out Google Colab, Kaggle, SciServer or other online notebook environments, play with them a little and select one (or more!) to write notebooks for this class. Alternatively, if you prefer, you can run your code locally. This tutorial on Sharing Notebooks may help."
  },
  {
    "objectID": "02-Lectures-Skills.html",
    "href": "02-Lectures-Skills.html",
    "title": "Skills for Data Science",
    "section": "",
    "text": "This is the material (slides and notes) of the second lecture on the course.",
    "crumbs": [
      "Lectures",
      "Skills for Data Science"
    ]
  },
  {
    "objectID": "02-Lectures-Skills.html#slides",
    "href": "02-Lectures-Skills.html#slides",
    "title": "Skills for Data Science",
    "section": "Slides",
    "text": "Slides\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\n\n\n\n\n\n\nBasically we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\n\n\n\n\n\n\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\n\n\n\n\n\n\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\n\n\n\n\n\n\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \n\n\n\n\n\n\nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\n\n\n\n\n\n\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\n\n\n\n\n\n\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\n\n\n\n\n\n\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\n\n\n\n\n\n\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\n\n\n\n\n\n\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\n\n\n\n\n\n\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\n\n\n\n\n\n\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\n\n\n\n\n\n\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\n\n\n\n\n\n\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\n\n\n\n\n\n\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\n\n\n\n\n\n\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\n\n\n\n\n\n\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\n\n\n\n\n\n\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please writecode in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\n\n\n\n\n\n\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\n\n\n\n\n\n\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\n\n\n\n\n\n\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\n\n\n\n\n\n\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\n\n\n\n\n\n\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\n\n\n\n\n\n\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\n\n\n\n\n\n\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\n\n\n\n\n\n\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\n\n\n\n\n\n\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\n\n\n\n\n\n\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\n\n\n\n\n\n\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\n\n\n\n\n\n\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\n\n\n\n\n\n\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\n\n\n\n\n\n\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\n\n\n\n\n\n\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\n\n\n\n\n\n\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\n\n\n\n\n\n\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\n\n\n\n\n\n\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\n\n\n\n\n\n\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\n\n\n\n\n\n\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\n\n\n\n\n\n\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\n\n\n\n\n\n\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!",
    "crumbs": [
      "Lectures",
      "Skills for Data Science"
    ]
  },
  {
    "objectID": "02-Lectures-Skills.html#code-for-examples-used-in-this-lecture",
    "href": "02-Lectures-Skills.html#code-for-examples-used-in-this-lecture",
    "title": "Skills for Data Science",
    "section": "Code for examples used in this lecture",
    "text": "Code for examples used in this lecture\n\nDiagram for Time Spent/Enjoyable Activities\nThe pie charts in the slides used in this lecture were copied from a dead link (will be redirected to another site), but since we have the percentages it would be fairly simple to reproduce it.\nFirst we import the (Johnson et al. 2018) library:\n\nimport plotly.graph_objects as go\n\nLet’s create the data structures in Python:\n\ntasks = ['Collecting Data', 'Cleaning and Organizing', 'Building Training Sets',\n         'Mining Data', 'Refining Algorithms', 'Other']\nperTime = [19, 60, 3, 9, 4, 5]\nperUnenjoy = [21, 57, 10, 3, 4, 5]\n\nI’d like to use a pastel color scheme:\n\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0', '#ffb3e6']\n\nLet’s create the chart for “most spent part” and set some layour options:\n\nfig_time = go.Figure(data=[go.Pie(\n    labels=tasks,\n    values=perTime,\n    hole=0.4,\n    marker=dict(colors=colors),\n    textinfo='percent',\n    textposition='outside',\n    hoverinfo='label+percent',\n    sort=False, # Disable sorting to keep the order consistent\n    textfont=dict(family=\"Arial Black\",size=18,color=\"#808080\") \n)])\n\nfig_time.update_layout(\n    title='What data scientists spend the most time doing',\n    showlegend=True,\n    legend=dict(title=\"Tasks\", x=1.15, y=0.5, \n                traceorder=\"normal\", font=dict(size=12))\n)\n\n                                                \n\n\nDo more or less the same for the “lest enjoyable part” chart:\n\nfig_unenjoy = go.Figure(data=[go.Pie(\n    labels=tasks,\n    values=perUnenjoy,\n    hole=0.4,\n    marker=dict(colors=colors),\n    textinfo='percent',\n    textposition='outside',\n    hoverinfo='label+percent',\n    sort=False,  # Disable sorting to keep the order consistent\n    textfont=dict(family=\"Arial Black\",size=18,color=\"#808080\") \n)])\n\nfig_unenjoy.update_layout(\n    title=\"What's the least enjoyable part of data science?\",\n    showlegend=True,\n    legend=dict(title=\"Tasks\", x=1.15, y=0.5, \n                traceorder=\"normal\", font=dict(size=12))\n)\n\n                                                \n\n\n\n\nAverage Temperature Basic Example\nIn this notebook we will do a very simple data science project: plot the Earth’s average land temperature and see if it is getting higher. The original data came from the Berkeley Earth site, an independent U.S. non-profit organization focused on environmental data science. The original data file can be downloaded here or here (a local copy). It is a text file with annual and five-year average temperatures and respective uncertainties.\nThe first 30 lines of that file are shown below:\n% This file contains a brief summary of the land-surface average results \n% produced by the Berkeley Averaging method.  Temperatures are in \n% Celsius and reported as anomalies relative to the Jan 1951-Dec 1980\n% average. Uncertainties represent the 95% confidence interval for \n% statistical and spatial undersampling effects.\n% \n% The current dataset presented here is described as: \n% \n%   Estimated Global Land-Surface TAVG based on the Complete Berkeley Dataset\n% \n% \n% This analysis was run on 07-Feb-2022 15:32:40\n% \n% Results are based on 50590 time series \n%   with 20945177 data points\n% \n% Estimated Jan 1951-Dec 1980 absolute temperature (C): 8.60 +/- 0.06\n% \n% \n% \n% Year, Annual Anomaly, Annual Unc., Five-year Anomaly, Five-year Unc.\n \n  1750      -1.220          NaN              NaN             NaN\n  1751      -1.311          NaN              NaN             NaN\n  1753      -0.955        1.005              NaN             NaN\n  1754      -0.379        0.934              NaN             NaN\n  1755      -0.698        0.980           -0.553           0.608\n  1756      -0.421        1.596           -0.831           0.586\n  1757      -0.310        0.896           -1.024           0.612\n  1758      -2.345        1.366           -1.347           0.882\nLet’s use Python (Rossum and Jr. 2001), Pandas (McKinney 2012) and Matplotlib (Hunter 2007) for the analysis scripts, which will read the data and plot the year versus the corrected annual average temperature, and after that let’s see if we can see a trend using a basic linear model.\nFirst we import the libraries we’re going to use.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nNow we can read the file into a data frame – more on this later in the course. If you want to reproduce this make sure you have the file in the right path. Note that we have to explicitely name the columns for this data set.\n\ninputfile = \"Resources/Data/AverageTemperature/Complete_TAVG_summary.txt\"\ndata = pd.read_csv(inputfile,sep=\"\\\\s+\",skiprows=22,header=None)\ndata.columns = [\"Year\",\"Annual.Anomaly\",\"Annual.Uncertainty\",\n                \"FiveYear.Anomaly\",\"FiveYear.Uncertainty\"]\n\nLet’s see how the data looks (as a data frame):\n\n data \n\n\n\n\n\n\n\n\n\nYear\nAnnual.Anomaly\nAnnual.Uncertainty\nFiveYear.Anomaly\nFiveYear.Uncertainty\n\n\n\n\n0\n1750\n-1.220\nNaN\nNaN\nNaN\n\n\n1\n1751\n-1.311\nNaN\nNaN\nNaN\n\n\n2\n1753\n-0.955\n1.005\nNaN\nNaN\n\n\n3\n1754\n-0.379\n0.934\nNaN\nNaN\n\n\n4\n1755\n-0.698\n0.980\n-0.553\n0.608\n\n\n...\n...\n...\n...\n...\n...\n\n\n266\n2017\n1.306\n0.035\n1.290\n0.037\n\n\n267\n2018\n1.145\n0.052\n1.345\n0.036\n\n\n268\n2019\n1.345\n0.048\n1.307\n0.035\n\n\n269\n2020\n1.499\n0.036\nNaN\nNaN\n\n\n270\n2021\n1.240\n0.038\nNaN\nNaN\n\n\n\n\n271 rows × 5 columns\n\n\n\n\nWe want to plot the temperature, but we have the anomaly. All we need to do is to add 8.65ºC to the anomaly (see a detailed explanation at this NASA’s Earth Observatory site or NOAA’s National Centers for Environmental Information).\n\ndata[\"Annual\"] = data[\"Annual.Anomaly\"]+8.65\n\nNow let’s plot the annual temperature against the year:\n\nax = data.plot(kind=\"line\",x=\"Year\",y=\"Annual\")\nplt.show()\n\n\n\n\n\n\n\n\nLet’s fit a linear regression model that will basically describe temperature a function of the year using the data we have. Don’t worry about the math details for the time being.\n\nmodel = LinearRegression()\nX = pd.DataFrame(data[\"Year\"])\nY = pd.DataFrame(data[\"Annual\"])\nmodel.fit(X,Y)\nY_pred = model.predict(X)\n\nY_pred contains the predicted temperature values from the years. We can now create another plot that shows the original data and the linear model as a straigth line in red.\n\nplt.plot(data[\"Year\"],data[\"Annual\"])\nplt.plot(data[\"Year\"],Y_pred,color=\"red\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\nBasically we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please writecode in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!",
    "crumbs": [
      "Lectures",
      "Skills for Data Science"
    ]
  },
  {
    "objectID": "30-Visualization-Intro.html",
    "href": "30-Visualization-Intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis section will contain some visualization examples.",
    "crumbs": [
      "Visualization",
      "Introduction"
    ]
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-1-WhatIs.html",
    "href": "Resources/Slides/CAP394-2024-1-WhatIs.html",
    "title": "CAP-394",
    "section": "",
    "text": "This is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data."
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-1-WhatIs.html#slides",
    "href": "Resources/Slides/CAP394-2024-1-WhatIs.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "This is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data."
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-3-Data.html",
    "href": "Resources/Slides/CAP394-2024-3-Data.html",
    "title": "CAP-394",
    "section": "",
    "text": "Today’s lecture will be about Data.\n\n\n\n\n\n\nHere’s are our main topics.\n\n\n\n\n\n\nJust as an exercise, try to explain data yourself. Avoid self-references!\n\n\n\n\n\n\nA very, very short definition of “data” for our purposes (“analyzing data”). It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\n\n\n\n\n\n\nRaw Data is the most common data type on the Web (and off it too) – data that was collected and put online or stored, without any preprocessing, annotations or even metadata.\n\n\n\n\n\n\nI got this from “An overview of a dataset digitized by citizen science volunteers – the 1900-1910 Daily Weather Reports“ (https://blogs.reading.ac.uk/weather-and-climate-at-reading/2020/an-overview-of-a-dataset-digitized-by-citizen-science-volunteers-the-1900-1910-daily-weather-reports/) – very interesting read! \u000bHumans collected the data by hand, then, many years later, digitized it using Citizen Science.\n\n\n\n\n\n\nThis are the daily measures of the level on Rio Ladário, part of the Paraguay River Basin. You can get its PDF from the “Centro deHidrografia e Navegação do Oeste - MARINHA DO BRASIL” website (https://www.marinha.mil.br/chn-6/?q=alturaAnterioresRios).There is probably a spreadsheet version of this PDF somewhere. But it is usable?The raw (useful!) data is somewhere at https://www.snirh.gov.br/hidroweb/serieshistoricas\n\n\n\n\n\n\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The data is shown as a HTML table. The chart is part of a report for the Engineering’s III Evaluation Group.Tables can be represented in text files, spreadsheets, XML and other formats.\u000bBut it is still not perfect data for us – how to we query those?\n\n\n\n\n\n\nFrom the Portal de Imigração Laboral of MINISTÉRIO DA JUSTIÇA E SEGURANÇA PÚBLICA (https://portaldeimigracao.mj.gov.br/pt/dados/dados-consolidados1) we can get data from immigrants in Brazil. The “plano tabular” is a spreadsheet with summaries but may not be useful for most queries – we need the full data set that was used to create those summaries.We can get it from the DataMigra portal (https://www.datamigraweb.unb.br/#/public), option “SOLICITANTES DE REFÚGIO”. \n\n\n\n\n\n\nThis section is a general discussion of what data can represent measures from the world, how it represents and how it is stored. It is a prelude to some notes on data transformation eventually leading to EDA and machine learning.In most of the cases we would like to deal with tables, but this is not always possible…\n\n\n\n\n\n\nHere’s an example of the different ways to consider data:Structure of Data: How the data is logically organized (e.g., rows representing municipalities and columns representing features of those municipalities such as area, population, and other attributes).Representation of Data: The format or schema used to express the data (e.g., a table with rows and columns, CSV format, JSON format).Storage of Data: The physical or digital medium and file format used to save the data (e.g., flat text file, spreadsheet, database).\n\n\n\n\n\n\nThis is our most basic data representation scheme: a collection of tuples, where each tuple element (the table column) contains measurements of some features of an object (the table lines).In this example we have a tuple on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may not be unique).We can easily imagine some operation on the tuples, e.g. find all values for a specific combination of “Localidade do gasto” and “Nome Função”.\u000bThis is the “Emendas Parlamentares” dataset, we’ll play with it later! Get it at https://portaldatransparencia.gov.br/download-de-dados/emendas-parlamentares\n\n\n\n\n\n\nThis is our most basic data representation scheme: a collection of tuples. In this example we also have tuples on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may be unique).It is easy to consider combination or aggregation operations on the tuples, for example, counting the immigrants from a country in a state, regardless of sex.This is the “Migration” dataset, we’ll play with it later!\n\n\n\n\n\n\nTime series are data: there is a temporal index that set an explicit order to it. Usually are represented as tuples of time -&gt; value1, value2, valueN. We expect the time to be unique.\n\n\n\n\n\n\nYet another collection of tuples but with a twist. Not only there are tuples for each participant in a project but the mapping of participants into projects is a graph, a different type of data that can be represented by tuples.This is the “CAPES-Projetos” dataset, we’ll play with it later!\n\n\n\n\n\n\nHere’s a region of the table that corresponds to a single project. We can consider each combination of ID_PESSOA within the same ID_PROJETO to be vertices in a network with the project being an edge.The additional material on https://rafaeldcsantos.github.io/CAP-394 shows how we can do it.\n\n\n\n\n\n\nThis is a single entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\n\n\n\n\n\n\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\n\n\n\n\n\n\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)Multimedia data cannot be used directly into machine learning algorithms: we must extract features of it beforehand, which often is not a trivial task.Movies and audio are other types of multimedia data which also requires feature extraction.\n\n\n\n\n\n\nWhen dealing with data obtained from the Web try to get also a data dictionary, a document that describes what the data contains (metadata!)\u000bOne common example of feature creation from data: if we have a timestamp, we can process it to get fields for days, hours, etc., possibly expanding to get information for days of the week and working hours, for example.Context is much more subtle: it is not part of data nor metadata, but may give important information about it.\n\n\n\n\n\n\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf\n\n\n\n\n\n\nBasically, we would like our data to look like a spreadsheet: each row represents an observation; each column represents one (and only one!) measure or variable on that observation. For this analogy to work we expect that the first row in the spreadsheet contains the names of the variables!Please refer to R for Data Science (2e), https://r4ds.hadley.nz/, for a more detailed description.\n\n\n\n\n\n\nDoes it look like a spreadsheet? It is probably tidy (but check for the single-value-in-a-cell rule!)Images can be considered tidy, but the semantics are very different – we don’t use pixels as values, usually there are some preprocessing steps first.\n\n\n\n\n\n\nIn wide format data, each variable is spread out across columns. It is better for reading by humans, some visualization and machine learning tasks. It can have missing data in its cells.\u000bIn long format data, each row is an observation and there are fewer columns. It is better for subsetting, it is tidier and more economic.\n\n\n\n\n\n\nOur supermarket data looks like a spreadsheet, looks tidy but could be made tidier – at what cost?Tidiness is also somehow relative. Consider what is a record: in this example it is an item being sold so the data is tidy-ish. If we consider transactions as the data represented than the data is somehow less tidy.\n\n\n\n\n\n\nWe have the data, possibly raw, hopefully tidy. We may need to change its shape to process it. Let’s talk about data shapes and transformations!\n\n\n\n\n\n\nLet’s think about the parliamentary amendments (”Emendas Parlamentares”) dataset. Each entry happened in one date (year), had an author, a code for the function, a place for the destination of the amendment and a value. On a 4-dimensional data cube we could have Year, Author, Code, Place for the dimensions and Value as the value stored in the cell. Using this way of representation, we could do some simple operations that allows exploration of the data, but on the other hand, conceptually, the cube could be made of mostly empty cells.\n\n\n\n\n\n\nWe can slice a N-dimensional data cube creating a (N-1)-dimensional data cube. Consider this as an operation that selects all records where a feature is equal to a single value. Considering the amendments example, we can slice the data cube getting all records for a particular year or for a particular place for the destination. The resulting dataset can be considered a new one (e.g. Amendments for 2014) and the feature used for the slicing can be discarded, since all data will have the same value for this feature.\n\n\n\n\n\n\nThis is basically the same – slicing can be done in any dimensions of the data cube. This would be the slicing of all amendments that will be used in a specific location. \u000bWe can, of course, slice again cubes that were already sliced further reducing its dimension (e.g. all amendment for a year and to be used in a place).\n\n\n\n\n\n\nDicing is slightly different from slicing: when we dice a data cube, we keep the same number of dimensions but get a reduced data set. Consider for example getting all amendments for a range of years and set of locations. The resulting data needs to keep the features used for dicing since we may have more than one value for those.\n\n\n\n\n\n\nPivoting data is a bit more complex: it uses the values of the features to create new columns and is usually done to summarize data.When we pivot data, we can use different operations to create the pivoted table cell’s values. In this case we could use “sum” to sum the values for all cells with the same Local, Categoria and Ano.\n\n\n\n\n\n\nHere is another example of pivoting: each project in the “CAPES-Projetos” dataset is spread out into several lines. I could pivot those to count how many members of each type each project contains. We’ll see in the examples!\n\n\n\n\n\n\nDrilling up is basically aggregation and counting. Drilling down is the opposite. We can only drill down if there is data to be used to show more details.For example, I cannot drill down the table in this slide because I don’t have how to add more information to get finer details. But if I had the full date instead of they year I could break it into day, month, year and do something conceptually similar to a drill down.\n\n\n\n\n\n\nToday’s lecture will be about Data.\nHere’s are our main topics.\nJust as an exercise, try to explain data yourself. Avoid self-references!\nA very, very short definition of “data” for our purposes (“analyzing data”). It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\nRaw Data is the most common data type on the Web (and off it too) – data that was collected and put online or stored, without any preprocessing, annotations or even metadata.\nI got this from “An overview of a dataset digitized by citizen science volunteers – the 1900-1910 Daily Weather Reports“ (https://blogs.reading.ac.uk/weather-and-climate-at-reading/2020/an-overview-of-a-dataset-digitized-by-citizen-science-volunteers-the-1900-1910-daily-weather-reports/) – very interesting read! \u000bHumans collected the data by hand, then, many years later, digitized it using Citizen Science.\nThis are the daily measures of the level on Rio Ladário, part of the Paraguay River Basin. You can get its PDF from the “Centro deHidrografia e Navegação do Oeste - MARINHA DO BRASIL” website (https://www.marinha.mil.br/chn-6/?q=alturaAnterioresRios).There is probably a spreadsheet version of this PDF somewhere. But it is usable?The raw (useful!) data is somewhere at https://www.snirh.gov.br/hidroweb/serieshistoricas\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The data is shown as a HTML table. The chart is part of a report for the Engineering’s III Evaluation Group.Tables can be represented in text files, spreadsheets, XML and other formats.\u000bBut it is still not perfect data for us – how to we query those?\nFrom the Portal de Imigração Laboral of MINISTÉRIO DA JUSTIÇA E SEGURANÇA PÚBLICA (https://portaldeimigracao.mj.gov.br/pt/dados/dados-consolidados1) we can get data from immigrants in Brazil. The “plano tabular” is a spreadsheet with summaries but may not be useful for most queries – we need the full data set that was used to create those summaries.We can get it from the DataMigra portal (https://www.datamigraweb.unb.br/#/public), option “SOLICITANTES DE REFÚGIO”. \nThis section is a general discussion of what data can represent measures from the world, how it represents and how it is stored. It is a prelude to some notes on data transformation eventually leading to EDA and machine learning.In most of the cases we would like to deal with tables, but this is not always possible…\nHere’s an example of the different ways to consider data:Structure of Data: How the data is logically organized (e.g., rows representing municipalities and columns representing features of those municipalities such as area, population, and other attributes).Representation of Data: The format or schema used to express the data (e.g., a table with rows and columns, CSV format, JSON format).Storage of Data: The physical or digital medium and file format used to save the data (e.g., flat text file, spreadsheet, database).\nThis is our most basic data representation scheme: a collection of tuples, where each tuple element (the table column) contains measurements of some features of an object (the table lines).In this example we have a tuple on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may not be unique).We can easily imagine some operation on the tuples, e.g. find all values for a specific combination of “Localidade do gasto” and “Nome Função”.\u000bThis is the “Emendas Parlamentares” dataset, we’ll play with it later! Get it at https://portaldatransparencia.gov.br/download-de-dados/emendas-parlamentares\nThis is our most basic data representation scheme: a collection of tuples. In this example we also have tuples on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may be unique).It is easy to consider combination or aggregation operations on the tuples, for example, counting the immigrants from a country in a state, regardless of sex.This is the “Migration” dataset, we’ll play with it later!\nTime series are data: there is a temporal index that set an explicit order to it. Usually are represented as tuples of time -&gt; value1, value2, valueN. We expect the time to be unique.\nYet another collection of tuples but with a twist. Not only there are tuples for each participant in a project but the mapping of participants into projects is a graph, a different type of data that can be represented by tuples.This is the “CAPES-Projetos” dataset, we’ll play with it later!\nHere’s a region of the table that corresponds to a single project. We can consider each combination of ID_PESSOA within the same ID_PROJETO to be vertices in a network with the project being an edge.The additional material on https://rafaeldcsantos.github.io/CAP-394 shows how we can do it.\nThis is a single entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)Multimedia data cannot be used directly into machine learning algorithms: we must extract features of it beforehand, which often is not a trivial task.Movies and audio are other types of multimedia data which also requires feature extraction.\nWhen dealing with data obtained from the Web try to get also a data dictionary, a document that describes what the data contains (metadata!)\u000bOne common example of feature creation from data: if we have a timestamp, we can process it to get fields for days, hours, etc., possibly expanding to get information for days of the week and working hours, for example.Context is much more subtle: it is not part of data nor metadata, but may give important information about it.\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf\nBasically, we would like our data to look like a spreadsheet: each row represents an observation; each column represents one (and only one!) measure or variable on that observation. For this analogy to work we expect that the first row in the spreadsheet contains the names of the variables!Please refer to R for Data Science (2e), https://r4ds.hadley.nz/, for a more detailed description.\nDoes it look like a spreadsheet? It is probably tidy (but check for the single-value-in-a-cell rule!)Images can be considered tidy, but the semantics are very different – we don’t use pixels as values, usually there are some preprocessing steps first.\nIn wide format data, each variable is spread out across columns. It is better for reading by humans, some visualization and machine learning tasks. It can have missing data in its cells.\u000bIn long format data, each row is an observation and there are fewer columns. It is better for subsetting, it is tidier and more economic.\nOur supermarket data looks like a spreadsheet, looks tidy but could be made tidier – at what cost?Tidiness is also somehow relative. Consider what is a record: in this example it is an item being sold so the data is tidy-ish. If we consider transactions as the data represented than the data is somehow less tidy.\nWe have the data, possibly raw, hopefully tidy. We may need to change its shape to process it. Let’s talk about data shapes and transformations!\nLet’s think about the parliamentary amendments (”Emendas Parlamentares”) dataset. Each entry happened in one date (year), had an author, a code for the function, a place for the destination of the amendment and a value. On a 4-dimensional data cube we could have Year, Author, Code, Place for the dimensions and Value as the value stored in the cell. Using this way of representation, we could do some simple operations that allows exploration of the data, but on the other hand, conceptually, the cube could be made of mostly empty cells.\nWe can slice a N-dimensional data cube creating a (N-1)-dimensional data cube. Consider this as an operation that selects all records where a feature is equal to a single value. Considering the amendments example, we can slice the data cube getting all records for a particular year or for a particular place for the destination. The resulting dataset can be considered a new one (e.g. Amendments for 2014) and the feature used for the slicing can be discarded, since all data will have the same value for this feature.\nThis is basically the same – slicing can be done in any dimensions of the data cube. This would be the slicing of all amendments that will be used in a specific location. \u000bWe can, of course, slice again cubes that were already sliced further reducing its dimension (e.g. all amendment for a year and to be used in a place).\nDicing is slightly different from slicing: when we dice a data cube, we keep the same number of dimensions but get a reduced data set. Consider for example getting all amendments for a range of years and set of locations. The resulting data needs to keep the features used for dicing since we may have more than one value for those.\nPivoting data is a bit more complex: it uses the values of the features to create new columns and is usually done to summarize data.When we pivot data, we can use different operations to create the pivoted table cell’s values. In this case we could use “sum” to sum the values for all cells with the same Local, Categoria and Ano.\nHere is another example of pivoting: each project in the “CAPES-Projetos” dataset is spread out into several lines. I could pivot those to count how many members of each type each project contains. We’ll see in the examples!\nDrilling up is basically aggregation and counting. Drilling down is the opposite. We can only drill down if there is data to be used to show more details.For example, I cannot drill down the table in this slide because I don’t have how to add more information to get finer details. But if I had the full date instead of they year I could break it into day, month, year and do something conceptually similar to a drill down."
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-3-Data.html#slides",
    "href": "Resources/Slides/CAP394-2024-3-Data.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "Today’s lecture will be about Data.\n\n\n\n\n\n\nHere’s are our main topics.\n\n\n\n\n\n\nJust as an exercise, try to explain data yourself. Avoid self-references!\n\n\n\n\n\n\nA very, very short definition of “data” for our purposes (“analyzing data”). It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\n\n\n\n\n\n\nRaw Data is the most common data type on the Web (and off it too) – data that was collected and put online or stored, without any preprocessing, annotations or even metadata.\n\n\n\n\n\n\nI got this from “An overview of a dataset digitized by citizen science volunteers – the 1900-1910 Daily Weather Reports“ (https://blogs.reading.ac.uk/weather-and-climate-at-reading/2020/an-overview-of-a-dataset-digitized-by-citizen-science-volunteers-the-1900-1910-daily-weather-reports/) – very interesting read! \u000bHumans collected the data by hand, then, many years later, digitized it using Citizen Science.\n\n\n\n\n\n\nThis are the daily measures of the level on Rio Ladário, part of the Paraguay River Basin. You can get its PDF from the “Centro deHidrografia e Navegação do Oeste - MARINHA DO BRASIL” website (https://www.marinha.mil.br/chn-6/?q=alturaAnterioresRios).There is probably a spreadsheet version of this PDF somewhere. But it is usable?The raw (useful!) data is somewhere at https://www.snirh.gov.br/hidroweb/serieshistoricas\n\n\n\n\n\n\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The data is shown as a HTML table. The chart is part of a report for the Engineering’s III Evaluation Group.Tables can be represented in text files, spreadsheets, XML and other formats.\u000bBut it is still not perfect data for us – how to we query those?\n\n\n\n\n\n\nFrom the Portal de Imigração Laboral of MINISTÉRIO DA JUSTIÇA E SEGURANÇA PÚBLICA (https://portaldeimigracao.mj.gov.br/pt/dados/dados-consolidados1) we can get data from immigrants in Brazil. The “plano tabular” is a spreadsheet with summaries but may not be useful for most queries – we need the full data set that was used to create those summaries.We can get it from the DataMigra portal (https://www.datamigraweb.unb.br/#/public), option “SOLICITANTES DE REFÚGIO”. \n\n\n\n\n\n\nThis section is a general discussion of what data can represent measures from the world, how it represents and how it is stored. It is a prelude to some notes on data transformation eventually leading to EDA and machine learning.In most of the cases we would like to deal with tables, but this is not always possible…\n\n\n\n\n\n\nHere’s an example of the different ways to consider data:Structure of Data: How the data is logically organized (e.g., rows representing municipalities and columns representing features of those municipalities such as area, population, and other attributes).Representation of Data: The format or schema used to express the data (e.g., a table with rows and columns, CSV format, JSON format).Storage of Data: The physical or digital medium and file format used to save the data (e.g., flat text file, spreadsheet, database).\n\n\n\n\n\n\nThis is our most basic data representation scheme: a collection of tuples, where each tuple element (the table column) contains measurements of some features of an object (the table lines).In this example we have a tuple on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may not be unique).We can easily imagine some operation on the tuples, e.g. find all values for a specific combination of “Localidade do gasto” and “Nome Função”.\u000bThis is the “Emendas Parlamentares” dataset, we’ll play with it later! Get it at https://portaldatransparencia.gov.br/download-de-dados/emendas-parlamentares\n\n\n\n\n\n\nThis is our most basic data representation scheme: a collection of tuples. In this example we also have tuples on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may be unique).It is easy to consider combination or aggregation operations on the tuples, for example, counting the immigrants from a country in a state, regardless of sex.This is the “Migration” dataset, we’ll play with it later!\n\n\n\n\n\n\nTime series are data: there is a temporal index that set an explicit order to it. Usually are represented as tuples of time -&gt; value1, value2, valueN. We expect the time to be unique.\n\n\n\n\n\n\nYet another collection of tuples but with a twist. Not only there are tuples for each participant in a project but the mapping of participants into projects is a graph, a different type of data that can be represented by tuples.This is the “CAPES-Projetos” dataset, we’ll play with it later!\n\n\n\n\n\n\nHere’s a region of the table that corresponds to a single project. We can consider each combination of ID_PESSOA within the same ID_PROJETO to be vertices in a network with the project being an edge.The additional material on https://rafaeldcsantos.github.io/CAP-394 shows how we can do it.\n\n\n\n\n\n\nThis is a single entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\n\n\n\n\n\n\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\n\n\n\n\n\n\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)Multimedia data cannot be used directly into machine learning algorithms: we must extract features of it beforehand, which often is not a trivial task.Movies and audio are other types of multimedia data which also requires feature extraction.\n\n\n\n\n\n\nWhen dealing with data obtained from the Web try to get also a data dictionary, a document that describes what the data contains (metadata!)\u000bOne common example of feature creation from data: if we have a timestamp, we can process it to get fields for days, hours, etc., possibly expanding to get information for days of the week and working hours, for example.Context is much more subtle: it is not part of data nor metadata, but may give important information about it.\n\n\n\n\n\n\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf\n\n\n\n\n\n\nBasically, we would like our data to look like a spreadsheet: each row represents an observation; each column represents one (and only one!) measure or variable on that observation. For this analogy to work we expect that the first row in the spreadsheet contains the names of the variables!Please refer to R for Data Science (2e), https://r4ds.hadley.nz/, for a more detailed description.\n\n\n\n\n\n\nDoes it look like a spreadsheet? It is probably tidy (but check for the single-value-in-a-cell rule!)Images can be considered tidy, but the semantics are very different – we don’t use pixels as values, usually there are some preprocessing steps first.\n\n\n\n\n\n\nIn wide format data, each variable is spread out across columns. It is better for reading by humans, some visualization and machine learning tasks. It can have missing data in its cells.\u000bIn long format data, each row is an observation and there are fewer columns. It is better for subsetting, it is tidier and more economic.\n\n\n\n\n\n\nOur supermarket data looks like a spreadsheet, looks tidy but could be made tidier – at what cost?Tidiness is also somehow relative. Consider what is a record: in this example it is an item being sold so the data is tidy-ish. If we consider transactions as the data represented than the data is somehow less tidy.\n\n\n\n\n\n\nWe have the data, possibly raw, hopefully tidy. We may need to change its shape to process it. Let’s talk about data shapes and transformations!\n\n\n\n\n\n\nLet’s think about the parliamentary amendments (”Emendas Parlamentares”) dataset. Each entry happened in one date (year), had an author, a code for the function, a place for the destination of the amendment and a value. On a 4-dimensional data cube we could have Year, Author, Code, Place for the dimensions and Value as the value stored in the cell. Using this way of representation, we could do some simple operations that allows exploration of the data, but on the other hand, conceptually, the cube could be made of mostly empty cells.\n\n\n\n\n\n\nWe can slice a N-dimensional data cube creating a (N-1)-dimensional data cube. Consider this as an operation that selects all records where a feature is equal to a single value. Considering the amendments example, we can slice the data cube getting all records for a particular year or for a particular place for the destination. The resulting dataset can be considered a new one (e.g. Amendments for 2014) and the feature used for the slicing can be discarded, since all data will have the same value for this feature.\n\n\n\n\n\n\nThis is basically the same – slicing can be done in any dimensions of the data cube. This would be the slicing of all amendments that will be used in a specific location. \u000bWe can, of course, slice again cubes that were already sliced further reducing its dimension (e.g. all amendment for a year and to be used in a place).\n\n\n\n\n\n\nDicing is slightly different from slicing: when we dice a data cube, we keep the same number of dimensions but get a reduced data set. Consider for example getting all amendments for a range of years and set of locations. The resulting data needs to keep the features used for dicing since we may have more than one value for those.\n\n\n\n\n\n\nPivoting data is a bit more complex: it uses the values of the features to create new columns and is usually done to summarize data.When we pivot data, we can use different operations to create the pivoted table cell’s values. In this case we could use “sum” to sum the values for all cells with the same Local, Categoria and Ano.\n\n\n\n\n\n\nHere is another example of pivoting: each project in the “CAPES-Projetos” dataset is spread out into several lines. I could pivot those to count how many members of each type each project contains. We’ll see in the examples!\n\n\n\n\n\n\nDrilling up is basically aggregation and counting. Drilling down is the opposite. We can only drill down if there is data to be used to show more details.For example, I cannot drill down the table in this slide because I don’t have how to add more information to get finer details. But if I had the full date instead of they year I could break it into day, month, year and do something conceptually similar to a drill down.\n\n\n\n\n\n\nToday’s lecture will be about Data.\nHere’s are our main topics.\nJust as an exercise, try to explain data yourself. Avoid self-references!\nA very, very short definition of “data” for our purposes (“analyzing data”). It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\nRaw Data is the most common data type on the Web (and off it too) – data that was collected and put online or stored, without any preprocessing, annotations or even metadata.\nI got this from “An overview of a dataset digitized by citizen science volunteers – the 1900-1910 Daily Weather Reports“ (https://blogs.reading.ac.uk/weather-and-climate-at-reading/2020/an-overview-of-a-dataset-digitized-by-citizen-science-volunteers-the-1900-1910-daily-weather-reports/) – very interesting read! \u000bHumans collected the data by hand, then, many years later, digitized it using Citizen Science.\nThis are the daily measures of the level on Rio Ladário, part of the Paraguay River Basin. You can get its PDF from the “Centro deHidrografia e Navegação do Oeste - MARINHA DO BRASIL” website (https://www.marinha.mil.br/chn-6/?q=alturaAnterioresRios).There is probably a spreadsheet version of this PDF somewhere. But it is usable?The raw (useful!) data is somewhere at https://www.snirh.gov.br/hidroweb/serieshistoricas\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The data is shown as a HTML table. The chart is part of a report for the Engineering’s III Evaluation Group.Tables can be represented in text files, spreadsheets, XML and other formats.\u000bBut it is still not perfect data for us – how to we query those?\nFrom the Portal de Imigração Laboral of MINISTÉRIO DA JUSTIÇA E SEGURANÇA PÚBLICA (https://portaldeimigracao.mj.gov.br/pt/dados/dados-consolidados1) we can get data from immigrants in Brazil. The “plano tabular” is a spreadsheet with summaries but may not be useful for most queries – we need the full data set that was used to create those summaries.We can get it from the DataMigra portal (https://www.datamigraweb.unb.br/#/public), option “SOLICITANTES DE REFÚGIO”. \nThis section is a general discussion of what data can represent measures from the world, how it represents and how it is stored. It is a prelude to some notes on data transformation eventually leading to EDA and machine learning.In most of the cases we would like to deal with tables, but this is not always possible…\nHere’s an example of the different ways to consider data:Structure of Data: How the data is logically organized (e.g., rows representing municipalities and columns representing features of those municipalities such as area, population, and other attributes).Representation of Data: The format or schema used to express the data (e.g., a table with rows and columns, CSV format, JSON format).Storage of Data: The physical or digital medium and file format used to save the data (e.g., flat text file, spreadsheet, database).\nThis is our most basic data representation scheme: a collection of tuples, where each tuple element (the table column) contains measurements of some features of an object (the table lines).In this example we have a tuple on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may not be unique).We can easily imagine some operation on the tuples, e.g. find all values for a specific combination of “Localidade do gasto” and “Nome Função”.\u000bThis is the “Emendas Parlamentares” dataset, we’ll play with it later! Get it at https://portaldatransparencia.gov.br/download-de-dados/emendas-parlamentares\nThis is our most basic data representation scheme: a collection of tuples. In this example we also have tuples on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may be unique).It is easy to consider combination or aggregation operations on the tuples, for example, counting the immigrants from a country in a state, regardless of sex.This is the “Migration” dataset, we’ll play with it later!\nTime series are data: there is a temporal index that set an explicit order to it. Usually are represented as tuples of time -&gt; value1, value2, valueN. We expect the time to be unique.\nYet another collection of tuples but with a twist. Not only there are tuples for each participant in a project but the mapping of participants into projects is a graph, a different type of data that can be represented by tuples.This is the “CAPES-Projetos” dataset, we’ll play with it later!\nHere’s a region of the table that corresponds to a single project. We can consider each combination of ID_PESSOA within the same ID_PROJETO to be vertices in a network with the project being an edge.The additional material on https://rafaeldcsantos.github.io/CAP-394 shows how we can do it.\nThis is a single entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)Multimedia data cannot be used directly into machine learning algorithms: we must extract features of it beforehand, which often is not a trivial task.Movies and audio are other types of multimedia data which also requires feature extraction.\nWhen dealing with data obtained from the Web try to get also a data dictionary, a document that describes what the data contains (metadata!)\u000bOne common example of feature creation from data: if we have a timestamp, we can process it to get fields for days, hours, etc., possibly expanding to get information for days of the week and working hours, for example.Context is much more subtle: it is not part of data nor metadata, but may give important information about it.\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf\nBasically, we would like our data to look like a spreadsheet: each row represents an observation; each column represents one (and only one!) measure or variable on that observation. For this analogy to work we expect that the first row in the spreadsheet contains the names of the variables!Please refer to R for Data Science (2e), https://r4ds.hadley.nz/, for a more detailed description.\nDoes it look like a spreadsheet? It is probably tidy (but check for the single-value-in-a-cell rule!)Images can be considered tidy, but the semantics are very different – we don’t use pixels as values, usually there are some preprocessing steps first.\nIn wide format data, each variable is spread out across columns. It is better for reading by humans, some visualization and machine learning tasks. It can have missing data in its cells.\u000bIn long format data, each row is an observation and there are fewer columns. It is better for subsetting, it is tidier and more economic.\nOur supermarket data looks like a spreadsheet, looks tidy but could be made tidier – at what cost?Tidiness is also somehow relative. Consider what is a record: in this example it is an item being sold so the data is tidy-ish. If we consider transactions as the data represented than the data is somehow less tidy.\nWe have the data, possibly raw, hopefully tidy. We may need to change its shape to process it. Let’s talk about data shapes and transformations!\nLet’s think about the parliamentary amendments (”Emendas Parlamentares”) dataset. Each entry happened in one date (year), had an author, a code for the function, a place for the destination of the amendment and a value. On a 4-dimensional data cube we could have Year, Author, Code, Place for the dimensions and Value as the value stored in the cell. Using this way of representation, we could do some simple operations that allows exploration of the data, but on the other hand, conceptually, the cube could be made of mostly empty cells.\nWe can slice a N-dimensional data cube creating a (N-1)-dimensional data cube. Consider this as an operation that selects all records where a feature is equal to a single value. Considering the amendments example, we can slice the data cube getting all records for a particular year or for a particular place for the destination. The resulting dataset can be considered a new one (e.g. Amendments for 2014) and the feature used for the slicing can be discarded, since all data will have the same value for this feature.\nThis is basically the same – slicing can be done in any dimensions of the data cube. This would be the slicing of all amendments that will be used in a specific location. \u000bWe can, of course, slice again cubes that were already sliced further reducing its dimension (e.g. all amendment for a year and to be used in a place).\nDicing is slightly different from slicing: when we dice a data cube, we keep the same number of dimensions but get a reduced data set. Consider for example getting all amendments for a range of years and set of locations. The resulting data needs to keep the features used for dicing since we may have more than one value for those.\nPivoting data is a bit more complex: it uses the values of the features to create new columns and is usually done to summarize data.When we pivot data, we can use different operations to create the pivoted table cell’s values. In this case we could use “sum” to sum the values for all cells with the same Local, Categoria and Ano.\nHere is another example of pivoting: each project in the “CAPES-Projetos” dataset is spread out into several lines. I could pivot those to count how many members of each type each project contains. We’ll see in the examples!\nDrilling up is basically aggregation and counting. Drilling down is the opposite. We can only drill down if there is data to be used to show more details.For example, I cannot drill down the table in this slide because I don’t have how to add more information to get finer details. But if I had the full date instead of they year I could break it into day, month, year and do something conceptually similar to a drill down."
  },
  {
    "objectID": "51-Projects-Amendments.html",
    "href": "51-Projects-Amendments.html",
    "title": "Parliamentary Amendments",
    "section": "",
    "text": "A parliamentary amendment is a proposal made by a member of parliament or a legislator to specify provisions in a bill or legislation that is being considered by the parliament. This term is commonly used in the legislative processes of various countries, including Brazil, often to earmark funds for specific projects.\nThis chapter presents a basic (and incomplete!) example of a data science approach to analysis of a very simple parliamentary amendment database. The data was obtained from a government itself.\nThe examples in this chapter show how even a simple database can be explored and used to give ideas for more interesting questions that may require more data.",
    "crumbs": [
      "Projects",
      "Parliamentary Amendments"
    ]
  },
  {
    "objectID": "51-Projects-Amendments.html#functions-and-subfunctions",
    "href": "51-Projects-Amendments.html#functions-and-subfunctions",
    "title": "Parliamentary Amendments",
    "section": "Functions and Subfunctions",
    "text": "Functions and Subfunctions\nFunctions (Nome Função) and subfunctions (Nome Subfunção) are the categories of application of the amendments’ values. Let’s take a look at their combinations and frequency.\n\ncfunctions = df.groupby([\"Nome Função\",\"Nome Subfunção\"]).size().reset_index(name='Count')\ncfunctions\n\n\n\n\n\n\n\n\n\nNome Função\nNome Subfunção\nCount\n\n\n\n\n0\nAdministração\nAdministração geral\n26\n\n\n1\nAdministração\nComunicação social\n4\n\n\n2\nAdministração\nControle interno\n4\n\n\n3\nAdministração\nDesenvolvimento científico\n1\n\n\n4\nAdministração\nDireitos individuais, coletivos e difusos\n2\n\n\n...\n...\n...\n...\n\n\n204\nUrbanismo\nOutras transferências\n2\n\n\n205\nUrbanismo\nPlanejamento e orçamento\n2\n\n\n206\nUrbanismo\nServiços urbanos\n7\n\n\n207\nUrbanismo\nTransportes coletivos urbanos\n46\n\n\n208\nUrbanismo\ninfra-estrutura urbana\n4090\n\n\n\n\n209 rows × 3 columns\n\n\n\n\nWe can create sunburst charts to show the distribution of amendments by functions and subfunctions. For consistency we will create a colormap for the sunburst charts that ensure the use of the same color for the same function:\n\nfunctionsU = cfunctions[\"Nome Função\"].unique()\ncolors = px.colors.qualitative.Plotly\n# Create a dictionary to map each \"Nome Função\" to a specific color\ncolormap = {function: colors[i % len(colors)] for i, function in enumerate(functionsU)}\n\nWith the grouped data and colormap we can create the sunburst chart with the code below:\n\n# Create a Sunburst chart\nsunburstC = px.sunburst(cfunctions, \n                        path=['Nome Função', 'Nome Subfunção'], \n                        values='Count', \n                        title='Number of Amendments by Function and Subfunction',\n                        labels={'Nome Função': 'Function', 'Nome Subfunção': \\\n                            'Subfunction', 'Count': 'Number of Amendments'},\n                        color='Nome Função',\n                        color_discrete_map=colormap)\n# Display the chart\nsunburstC.show()\n\n                                                \n\n\nLet’s do the same chart but considering the total amount of the amendments. All we need to do is create a different grouping of the data:\n\nvfunctions = df.groupby([\"Nome Função\", \"Nome Subfunção\"])[\"Valor\"].sum().reset_index(name='Amount')\nvfunctions\n\n\n\n\n\n\n\n\n\nNome Função\nNome Subfunção\nAmount\n\n\n\n\n0\nAdministração\nAdministração geral\n2.741300e+07\n\n\n1\nAdministração\nComunicação social\n7.571335e+05\n\n\n2\nAdministração\nControle interno\n9.034404e+06\n\n\n3\nAdministração\nDesenvolvimento científico\n7.836601e+06\n\n\n4\nAdministração\nDireitos individuais, coletivos e difusos\n1.955050e+06\n\n\n...\n...\n...\n...\n\n\n204\nUrbanismo\nOutras transferências\n6.500000e+05\n\n\n205\nUrbanismo\nPlanejamento e orçamento\n2.629205e+06\n\n\n206\nUrbanismo\nServiços urbanos\n6.343775e+06\n\n\n207\nUrbanismo\nTransportes coletivos urbanos\n5.076996e+08\n\n\n208\nUrbanismo\ninfra-estrutura urbana\n1.762114e+10\n\n\n\n\n209 rows × 3 columns\n\n\n\n\nThen plot the sunburst chart (using the same colormap as the previous chart to make comparisons easier):\n\n# Create a Sunburst chart\nsunburstV = px.sunburst(vfunctions, \n                        path=['Nome Função', 'Nome Subfunção'], \n                        values='Amount', \n                        title='Total Amount of Amendments by Function and Subfunction',\n                        labels={'Nome Função': 'Function', 'Nome Subfunção': \\\n                            'Subfunction', 'Count': 'Number of Amendments'},\n                        color='Nome Função',\n                        color_discrete_map=colormap)\n# Display the chart\nsunburstV.show()\n\n                                                \n\n\n\n\nClass Discussion\n\nDo a quick comparison between the two plots and find differences in the visibility of their slices.",
    "crumbs": [
      "Projects",
      "Parliamentary Amendments"
    ]
  },
  {
    "objectID": "51-Projects-Amendments.html#countrywide-amendments",
    "href": "51-Projects-Amendments.html#countrywide-amendments",
    "title": "Parliamentary Amendments",
    "section": "Countrywide Amendments",
    "text": "Countrywide Amendments\nHow many types of Nome Função (main function) do we have for countrywide amendments?\n\nfunctionCounts = dfCountry[\"Nome Função\"].value_counts()\nfunctionCounts\n\nNome Função\nSaúde                    1431\nDefesa nacional          1035\nDireitos da cidadania     318\nSegurança pública         266\nAgricultura               229\nCultura                   193\nEducação                  157\nEncargos especiais        146\nAssistência social        121\nCiência e Tecnologia      107\nOrganização agrária       104\nDesporto e lazer           98\nGestão ambiental           66\nTrabalho                   64\nUrbanismo                  58\nTransporte                 47\nMúltiplo                   41\nAdministração              28\nComércio e serviços        24\nComunicações               16\nPrevidência social         14\nRelações exteriores        13\nIndústria                  11\nSaneamento                  7\nEnergia                     6\nEssencial à justiça         2\nHabitação                   2\nName: count, dtype: int64\n\n\nAre all those functions spread evenly through the years? Let’s compare them with a plot. First we group the data by Nome Função and Ano da Emenda counting how many amendments we had in each group:\n\ncpy = dfCountry.groupby([\"Nome Função\",\"Ano da Emenda\"]).size().\\\n        reset_index(name='Count')\ncpy\n\n\n\n\n\n\n\n\n\nNome Função\nAno da Emenda\nCount\n\n\n\n\n0\nAdministração\n2016\n3\n\n\n1\nAdministração\n2017\n6\n\n\n2\nAdministração\n2019\n2\n\n\n3\nAdministração\n2020\n9\n\n\n4\nAdministração\n2022\n3\n\n\n...\n...\n...\n...\n\n\n208\nUrbanismo\n2020\n11\n\n\n209\nUrbanismo\n2021\n9\n\n\n210\nUrbanismo\n2022\n16\n\n\n211\nUrbanismo\n2023\n1\n\n\n212\nUrbanismo\n2024\n3\n\n\n\n\n213 rows × 3 columns\n\n\n\n\nThen we create a faceted line plot, with a facet for each Nome Função.\n\n# Create a faceted line plot\nfig = px.line(cpy, \n              x='Ano da Emenda', \n              y='Count', \n              color='Nome Função',\n              facet_col='Nome Função', \n              facet_col_wrap=3,\n              title='Amendments per Year per Function',\n              labels={'Ano da Emenda': 'Year', 'Count': 'Amendments', \n                      'Nome Função': ''},\n              markers=True)\n# Update layout for better spacing and readability\nfig.update_layout(\n    height=1020,\n    showlegend=False,\n    margin=dict(t=80) \n)\n# Remove the automatic subplot titles\nfor annotation in fig['layout']['annotations']:\n    annotation['text'] = annotation['text'].split(\"=\")[-1]\n# Update x-axes to show ticks and labels for all subplots\nyears = cpy[\"Ano da Emenda\"].unique()\nyears.sort()\nfig.update_xaxes(showticklabels=True,tickvals=years,tickangle=90,\n                 tickfont=dict(size=10))  \nfig.update_yaxes(title_font=dict(size=10))\n# Display the plot\nfig.show()\n\n                                                \n\n\nNot all functions had amendments for each year in our data – some have gaps, while some happened only during a short period.\nThe plot above counted the number of amendments. Let’s modify it so it shows the total value of the amendments, under the same conditions. First we group the data:\n\n# Group by \"Nome Função\" and \"Ano da Emenda\" and sum the values\nspy = dfCountry.groupby([\"Nome Função\", \"Ano da Emenda\"])[\"Valor\"].sum().\\\n        reset_index(name='Total Value')\nspy\n\n\n\n\n\n\n\n\n\nNome Função\nAno da Emenda\nTotal Value\n\n\n\n\n0\nAdministração\n2016\n2.144076e+07\n\n\n1\nAdministração\n2017\n4.561856e+08\n\n\n2\nAdministração\n2019\n2.229047e+07\n\n\n3\nAdministração\n2020\n4.494842e+07\n\n\n4\nAdministração\n2022\n2.306679e+06\n\n\n...\n...\n...\n...\n\n\n208\nUrbanismo\n2020\n1.779921e+09\n\n\n209\nUrbanismo\n2021\n2.782494e+09\n\n\n210\nUrbanismo\n2022\n4.040173e+08\n\n\n211\nUrbanismo\n2023\n3.000242e+09\n\n\n212\nUrbanismo\n2024\n5.061627e+08\n\n\n\n\n213 rows × 3 columns\n\n\n\n\nThen we do the same faceted plot:\n\n# Create a faceted line plot\nfig = px.line(spy, \n              x='Ano da Emenda', \n              y='Total Value', \n              color='Nome Função',\n              facet_col='Nome Função', \n              facet_col_wrap=3,\n              title='Amendments Total Value per Year per Function',\n              labels={'Ano da Emenda': 'Year', 'Count': 'Amendmts', 'Nome Função': ''},\n              markers=True)\n# Update layout for better spacing and readability\nfig.update_layout(\n    height=990,\n    showlegend=False,\n    margin=dict(t=80)     \n)\n# Remove the automatic subplot titles\nfor annotation in fig['layout']['annotations']:\n    annotation['text'] = annotation['text'].split(\"=\")[-1]\n# Update x-axes to show ticks and labels for all subplots\nyears = spy[\"Ano da Emenda\"].unique()\nyears.sort()\nfig.update_xaxes(showticklabels=True,tickvals=years,tickangle=90,tickfont=dict(size=10))  \nfig.update_yaxes(title_font=dict(size=10))\n# Display the plot\nfig.show()\n\n                                                \n\n\n\n\nClass Discussion\n\nDo a quick comparison between the two plots to find functions and years with a large value but small number of amendments.\n\n\n\n \n\nDo the same faceted plot for your region, state and county. Point differences between the number of amendments and values.",
    "crumbs": [
      "Projects",
      "Parliamentary Amendments"
    ]
  },
  {
    "objectID": "51-Projects-Amendments.html#amendments-per-state",
    "href": "51-Projects-Amendments.html#amendments-per-state",
    "title": "Parliamentary Amendments",
    "section": "Amendments per State",
    "text": "Amendments per State\nLet’s take a look at the amendments per state dataframe and some subsets of it. Just for fun let’s use choropleths – maps that represent variables.\nWe can use a simple 3-step process to create a choropleth: first get one GeoJSON file ready for our purposes, then prepare our data so it matches what is on the GeoJSON then create the plot itself.\nA quick search on the web will show several sites with GeoJSON data that can be used for our project. Two are Moisés Lima’s brazil-states-geojson at Kaggle, and Giuliano Macedo geodata-br-states at Github. We’ll use this second one, which was downloaded and stored locally here.\nThen we need to prepare the subset of our data that contains states’ amendments only, and also rename the Localidade do gasto field so it contains only the states’ name in uppercase:\n\ndfStatesPrep = dfStates.groupby([\"Localidade do gasto\"]).size().reset_index(name='Count')\ndfStatesPrep['Localidade do gasto'] = \\\n  dfStatesPrep['Localidade do gasto'].str.replace(r'\\ \\(UF\\)', '', regex=True)\ndfStatesPrep\n\n\n\n\n\n\n\n\n\nLocalidade do gasto\nCount\n\n\n\n\n0\nACRE\n813\n\n\n1\nALAGOAS\n784\n\n\n2\nAMAPÁ\n690\n\n\n3\nAMAZONAS\n602\n\n\n4\nBAHIA\n2924\n\n\n5\nCEARÁ\n1452\n\n\n6\nDISTRITO FEDERAL\n961\n\n\n7\nESPÍRITO SANTO\n1228\n\n\n8\nGOIÁS\n1690\n\n\n9\nMARANHÃO\n1006\n\n\n10\nMATO GROSSO\n610\n\n\n11\nMATO GROSSO DO SUL\n830\n\n\n12\nMINAS GERAIS\n4413\n\n\n13\nPARANÁ\n2487\n\n\n14\nPARAÍBA\n1153\n\n\n15\nPARÁ\n1246\n\n\n16\nPERNAMBUCO\n2063\n\n\n17\nPIAUÍ\n780\n\n\n18\nRIO DE JANEIRO\n2932\n\n\n19\nRIO GRANDE DO NORTE\n1176\n\n\n20\nRIO GRANDE DO SUL\n2801\n\n\n21\nRONDÔNIA\n595\n\n\n22\nRORAIMA\n531\n\n\n23\nSANTA CATARINA\n1437\n\n\n24\nSERGIPE\n827\n\n\n25\nSÃO PAULO\n5216\n\n\n26\nTOCANTINS\n729\n\n\n\n\n\n\n\n\nThe GeoJSON file contains coordinates for the shapes of the object it contains and also some properties for each object. In our case we have a property named Estado that contains the state’s names. We will load the GeoJSON file and change the values of the Estado property so these will also be in uppercase:\n\nwith open(\"Resources/Data/Emendas/br_states.json\") as f:\n    geojson_data = json.load(f)\nfor feature in geojson_data['features']:\n    feature['properties']['Estado'] = feature['properties']['Estado'].upper()\n\nNow that we prepared the data and the GeoJSON file we can create the choropleth with Plotly with this code:\n\nfig = px.choropleth(\n    dfStatesPrep, # The dataframe\n    geojson=geojson_data, # The GeoJSON data\n    # These two parameters identify the field on the dataframe and \n    # property on the GeoJSON data that must match.\n    locations='Localidade do gasto', \n    featureidkey=\"properties.Estado\", \n    color='Count', \n    hover_name='Localidade do gasto',\n    hover_data=['Count'],\n    title='Number of Amendments per State',\n    color_continuous_scale=\"Viridis\"\n)\n# Update layout for better spacing and readability\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_layout(margin={\"r\":0,\"t\":35,\"l\":0,\"b\":0})\n# Display the map\nfig.show()\n\n                                                \n\n\n\n\n \n\nDo a similar choropleth for the total amount of amendments per state.\n\n\n\n \n\nDo a similar choropleth for a subset of the States’ data for a specific function (get a popular one!), showing either the number of amendments or total amount.\n\n\n\n \n\nFind a GeoJSON file for your state, and create a choropleth for all counties in your state with the total amount of amendments for a specific function.\n\n\n\n \n\nGet the population of states (and regions) from Wikipedia: Lista de unidades federativas do Brasil por população or Federative units of Brazil. Calculate and plot the number of amendments and total value per state considering the population. Which charts can help visualize and compare this data?",
    "crumbs": [
      "Projects",
      "Parliamentary Amendments"
    ]
  },
  {
    "objectID": "51-Projects-Amendments.html#amendments-per-author",
    "href": "51-Projects-Amendments.html#amendments-per-author",
    "title": "Parliamentary Amendments",
    "section": "Amendments per Author",
    "text": "Amendments per Author\nHow’s the distribution of amendments per Código do Autor da Emenda?\n\ncountAdE = df[\"Código do Autor da Emenda\"].value_counts()\n# Print the result\nprint(countAdE)\n\nCódigo do Autor da Emenda\nS/I     2481\n8100     331\n2776     190\n1775     186\n2616     182\n        ... \n2748       1\n1640       1\n2854       1\n2503       1\n4371       1\nName: count, Length: 1551, dtype: int64\n\n\nAmendments with author code S/I (Sem Informação , without information) are anonymous, therefore intriguing. Let’s explore it a bit, first creating two subsets of the original dataframe (and checking how many amendments’ authors are S/I)\n\ndfSI = df[df[\"Código do Autor da Emenda\"] == \"S/I\"]\ndfWI = df[df[\"Código do Autor da Emenda\"] != \"S/I\"]\nrecordSI = len(dfSI)\nprint(f\"Total records in dfSI DataFrame: {recordSI}\")\n\nTotal records in dfSI DataFrame: 2481\n\n\nIs the distribution of the unidentified authors’ amendments different from the ones from identified authors? Let’s check with sunburts plots, comparing the amounts by function and subfunction!\nFirst we create the groupings:\n\nSIdist = dfSI.groupby([\"Nome Função\",\"Nome Subfunção\"])[\"Valor\"].sum().reset_index(name='Amount')\nWIdist = dfWI.groupby([\"Nome Função\",\"Nome Subfunção\"])[\"Valor\"].sum().reset_index(name='Amount')\n\nNow the sunburst charts:\n\nsunburstSI = px.sunburst(SIdist, \n                         path=['Nome Função', 'Nome Subfunção'], \n                         values='Amount', \n                         title='Amount of SI Amendments by Function and Subfunction',\n                         labels={'Nome Função': 'Function', 'Nome Subfunção': \\\n                             'Subfunction', 'Count': 'Number of Amendments'},\n                         color='Nome Função',\n                         color_discrete_map=colormap)\n# Display the chart\nsunburstV.show()\n\n                                                \n\n\n\n# Create a Sunburst chart\nsunburstV = px.sunburst(WIdist, \n                        path=['Nome Função', 'Nome Subfunção'], \n                        values='Amount', \n                        title='Amount of WI Amendments by Function and Subfunction',\n                        labels={'Nome Função': 'Function', 'Nome Subfunção': \\\n                            'Subfunction', 'Count': 'Number of Amendments'},\n                        color='Nome Função',\n                        color_discrete_map=colormap)\n# Display the chart\nsunburstV.show()\n\n                                                \n\n\n\n\n \n\nDistributions of amendments with and without the identification of the author are somehow similar, but some functions have a different order (total amount). Identify those and dig on the data for the values.\n\n\n\n \n\nAmendments authors’ can be identified by their names or by the – but we filtered this information on the Reading and Preprocessing section. Redo the filtered dataframes so we can have one for individual amendments, one for unidentified authors’ amendments and one for amendments where the author’s names start with “BANCADA DA” (a Party Parliamentary Group or Wing). These three subsets must complement each other in the sense that no record should appear in more than one subset and no records is out of any subset.\n\n\n\n \n\nBased on the results of the previous exercise, consider that most “Bancadas” are associated to a state. Are all amendments from a particular state group destined for that state? Analyze the cases where this happens and where it doesn’t.\n\nLet’s take a last look at the identified/non-identified amendments. How’s their count per year?\nFirst we group the separate dataframes and label them so we can use them in the same chart:\n\n# For the ones without identification\ngrDfSI = dfSI.groupby(\"Ano da Emenda\").size().reset_index(name='Count')\ngrDfSI['Type'] = 'Without Id.'\n# For the ones with identification\ngrDfWI = dfWI.groupby(\"Ano da Emenda\").size().reset_index(name='Count')\ngrDfWI['Type'] = 'With Id.'\n# Combine them\nbothDfs = pd.concat([grDfSI,grDfWI])\n\nNow we plot the total of amendments per year:\n\nfig = px.line(bothDfs, x=\"Ano da Emenda\", y=\"Count\", color='Type',\n              title=\"Total Amount of Amendments per Year\",\n              labels={\"Ano da Emenda\": \"Year\", \"Count\": \"Number of Amendments\", \"Type\": \"Type\"})\nfig.show()\n\n                                                \n\n\nNo amendments without the authors’ identification were submitted after 2020!\nGoing back to the distribution of the amendments per author, we can see that there are some authors (or groups) that are more prolific. How many amendments they create per year? Let’s consider the top 12 authors:\n\ntop12 = dfWI[\"Código do Autor da Emenda\"].value_counts().nlargest(12).index\ndftop12 = dfWI[dfWI[\"Código do Autor da Emenda\"].isin(top12)]\ndfG = dftop12.groupby([\"Código do Autor da Emenda\", \"Ano da Emenda\"]).size().\\\n               reset_index(name='Count')\n\nNow let’s plot how many amendments each authored per year.\n\nfig = px.line(dfG, x=\"Ano da Emenda\", y=\"Count\", color=\"Código do Autor da Emenda\",\n              markers=True,\n              title=\"Number of Amendments per Year by Top 12 Authors\",\n              labels={\"Ano da Emenda\": \"Year\", \"Count\": \"Number of Amendments\",\n                      \"Código do Autor da Emenda\": \"Author\"})\nfig.update_yaxes(type='log')\n# Display the plot\nfig.show()\n\n                                                \n\n\n\n\n \n\nWho is the author whose code is 8100? Should we consider this data? If not, filter the amendments from this author and redo the chart.",
    "crumbs": [
      "Projects",
      "Parliamentary Amendments"
    ]
  },
  {
    "objectID": "51-Projects-Amendments.html#amendments-per-functions-and-years",
    "href": "51-Projects-Amendments.html#amendments-per-functions-and-years",
    "title": "Parliamentary Amendments",
    "section": "Amendments per Functions and Years",
    "text": "Amendments per Functions and Years\nLet’s pick up some interesting functions (Defesa nacional, Educação and Saúde) and compare the total value of the amendments they received per year. We’ll consider amendments that were destined to the whole country.\n\ndfDefense = dfCountry[dfCountry[\"Nome Função\"] == \"Defesa nacional\"]\ndfDefenseY = dfDefense.groupby(\"Ano da Emenda\")[\"Valor\"].sum().reset_index()\ndfDefenseY['Type'] = 'Defesa Nacional'\ndfEducation = dfCountry[dfCountry[\"Nome Função\"] == \"Educação\"]\ndfEducationY = dfEducation.groupby(\"Ano da Emenda\")[\"Valor\"].sum().reset_index()\ndfEducationY['Type'] = 'Educação'\ndfHealth = dfCountry[dfCountry[\"Nome Função\"] == \"Saúde\"]\ndfHealthY = dfHealth.groupby(\"Ano da Emenda\")[\"Valor\"].sum().reset_index()\ndfHealthY['Type'] = 'Saúde'\n# Combine them\nbothDfs = pd.concat([dfDefenseY,dfEducationY,dfHealthY])\n\nThen create the chart:\n\nfig = px.line(bothDfs, x=\"Ano da Emenda\", y=\"Valor\", color='Type',\n              title=\"Total Amount of Amendments per Year\",\n              labels={\"Ano da Emenda\": \"Year\", \"Count\": \"Number of Amendments\",\\\n                      \"Type\": \"Type\"})\nfig.show()\n\n                                                \n\n\n\n\n \n\nHow would the information on this chart compare to amendments destined for states or regions?",
    "crumbs": [
      "Projects",
      "Parliamentary Amendments"
    ]
  },
  {
    "objectID": "50-Projects-Intro.html",
    "href": "50-Projects-Intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis section will contain some Data Science Projects that are used in our lectures.\nProjects are somehow complex tasks around a specific dataset that requires data wrangling, organization, EDA, visualization, and in some cases application of machine learning techniques.\nMost projects will include ideas for class discussions and several exercises.\nProjects that are ready to be explored are on the sidebar on the left.",
    "crumbs": [
      "Projects",
      "Introduction"
    ]
  },
  {
    "objectID": "80-Ideas-Intro.html",
    "href": "80-Ideas-Intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis section will contain some Data Science ideas that may be used in our lectures.\nIdeas are similar to projects but weren’t thoroughly explored. They will require more effort on data acquisition, organization, etc. before EDA and analysis can be done.\nIdeas that are ready to be explored are on the sidebar on the left.",
    "crumbs": [
      "Ideas Bank",
      "Introduction"
    ]
  },
  {
    "objectID": "03-Lectures-Data.html",
    "href": "03-Lectures-Data.html",
    "title": "Data",
    "section": "",
    "text": "This is the material (slides and notes) of the third lecture on the course.\n\n\n\n\n\n\nToday’s lecture will be about Data.\n\n\n\n\n\n\nHere’s are our main topics.\n\n\n\n\n\n\nJust as an exercise, try to explain data yourself. Avoid self-references!\n\n\n\n\n\n\nA very, very short definition of “data” for our purposes (“analyzing data”). It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\n\n\n\n\n\n\nRaw Data is the most common data type on the Web (and off it too) – data that was collected and put online or stored, without any preprocessing, annotations or even metadata.\n\n\n\n\n\n\nI got this from “An overview of a dataset digitized by citizen science volunteers – the 1900-1910 Daily Weather Reports“ (https://blogs.reading.ac.uk/weather-and-climate-at-reading/2020/an-overview-of-a-dataset-digitized-by-citizen-science-volunteers-the-1900-1910-daily-weather-reports/) – very interesting read! \u000bHumans collected the data by hand, then, many years later, digitized it using Citizen Science.\n\n\n\n\n\n\nThis are the daily measures of the level on Rio Ladário, part of the Paraguay River Basin. You can get its PDF from the “Centro deHidrografia e Navegação do Oeste - MARINHA DO BRASIL” website (https://www.marinha.mil.br/chn-6/?q=alturaAnterioresRios).There is probably a spreadsheet version of this PDF somewhere. But it is usable?The raw (useful!) data is somewhere at https://www.snirh.gov.br/hidroweb/serieshistoricas\n\n\n\n\n\n\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The data is shown as a HTML table. The chart is part of a report for the Engineering’s III Evaluation Group.Tables can be represented in text files, spreadsheets, XML and other formats.\u000bBut it is still not perfect data for us – how to we query those?\n\n\n\n\n\n\nFrom the Portal de Imigração Laboral of MINISTÉRIO DA JUSTIÇA E SEGURANÇA PÚBLICA (https://portaldeimigracao.mj.gov.br/pt/dados/dados-consolidados1) we can get data from immigrants in Brazil. The “plano tabular” is a spreadsheet with summaries but may not be useful for most queries – we need the full data set that was used to create those summaries.We can get it from the DataMigra portal (https://www.datamigraweb.unb.br/#/public), option “SOLICITANTES DE REFÚGIO”. \n\n\n\n\n\n\nThis section is a general discussion of what data can represent measures from the world, how it represents and how it is stored. It is a prelude to some notes on data transformation eventually leading to EDA and machine learning.In most of the cases we would like to deal with tables, but this is not always possible…\n\n\n\n\n\n\nHere’s an example of the different ways to consider data:Structure of Data: How the data is logically organized (e.g., rows representing municipalities and columns representing features of those municipalities such as area, population, and other attributes).Representation of Data: The format or schema used to express the data (e.g., a table with rows and columns, CSV format, JSON format).Storage of Data: The physical or digital medium and file format used to save the data (e.g., flat text file, spreadsheet, database).\n\n\n\n\n\n\nThis is our most basic data representation scheme: a collection of tuples, where each tuple element (the table column) contains measurements of some features of an object (the table lines).In this example we have a tuple on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may not be unique).We can easily imagine some operation on the tuples, e.g. find all values for a specific combination of “Localidade do gasto” and “Nome Função”.\u000bThis is the “Emendas Parlamentares” dataset, we’ll play with it later! Get it at https://portaldatransparencia.gov.br/download-de-dados/emendas-parlamentares\n\n\n\n\n\n\nThis is our most basic data representation scheme: a collection of tuples. In this example we also have tuples on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may be unique).It is easy to consider combination or aggregation operations on the tuples, for example, counting the immigrants from a country in a state, regardless of sex.This is the “Migration” dataset, we’ll play with it later!\n\n\n\n\n\n\nTime series are data: there is a temporal index that set an explicit order to it. Usually are represented as tuples of time -&gt; value1, value2, valueN. We expect the time to be unique.\n\n\n\n\n\n\nYet another collection of tuples but with a twist. Not only there are tuples for each participant in a project but the mapping of participants into projects is a graph, a different type of data that can be represented by tuples.This is the “CAPES-Projetos” dataset, we’ll play with it later!\n\n\n\n\n\n\nHere’s a region of the table that corresponds to a single project. We can consider each combination of ID_PESSOA within the same ID_PROJETO to be vertices in a network with the project being an edge.The additional material on https://rafaeldcsantos.github.io/CAP-394 shows how we can do it.\n\n\n\n\n\n\nThis is a single entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\n\n\n\n\n\n\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\n\n\n\n\n\n\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)Multimedia data cannot be used directly into machine learning algorithms: we must extract features of it beforehand, which often is not a trivial task.Movies and audio are other types of multimedia data which also requires feature extraction.\n\n\n\n\n\n\nWhen dealing with data obtained from the Web try to get also a data dictionary, a document that describes what the data contains (metadata!)\u000bOne common example of feature creation from data: if we have a timestamp, we can process it to get fields for days, hours, etc., possibly expanding to get information for days of the week and working hours, for example.Context is much more subtle: it is not part of data nor metadata, but may give important information about it.\n\n\n\n\n\n\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf\n\n\n\n\n\n\nBasically, we would like our data to look like a spreadsheet: each row represents an observation; each column represents one (and only one!) measure or variable on that observation. For this analogy to work we expect that the first row in the spreadsheet contains the names of the variables!Please refer to R for Data Science (2e), https://r4ds.hadley.nz/, for a more detailed description.\n\n\n\n\n\n\nDoes it look like a spreadsheet? It is probably tidy (but check for the single-value-in-a-cell rule!)Images can be considered tidy, but the semantics are very different – we don’t use pixels as values, usually there are some preprocessing steps first.\n\n\n\n\n\n\nIn wide format data, each variable is spread out across columns. It is better for reading by humans, some visualization and machine learning tasks. It can have missing data in its cells.\u000bIn long format data, each row is an observation and there are fewer columns. It is better for subsetting, it is tidier and more economic.\n\n\n\n\n\n\nOur supermarket data looks like a spreadsheet, looks tidy but could be made tidier – at what cost?Tidiness is also somehow relative. Consider what is a record: in this example it is an item being sold so the data is tidy-ish. If we consider transactions as the data represented than the data is somehow less tidy.\n\n\n\n\n\n\nWe have the data, possibly raw, hopefully tidy. We may need to change its shape to process it. Let’s talk about data shapes and transformations!\n\n\n\n\n\n\nLet’s think about the parliamentary amendments (”Emendas Parlamentares”) dataset. Each entry happened in one date (year), had an author, a code for the function, a place for the destination of the amendment and a value. On a 4-dimensional data cube we could have Year, Author, Code, Place for the dimensions and Value as the value stored in the cell. Using this way of representation, we could do some simple operations that allows exploration of the data, but on the other hand, conceptually, the cube could be made of mostly empty cells.\n\n\n\n\n\n\nWe can slice a N-dimensional data cube creating a (N-1)-dimensional data cube. Consider this as an operation that selects all records where a feature is equal to a single value. Considering the amendments example, we can slice the data cube getting all records for a particular year or for a particular place for the destination. The resulting dataset can be considered a new one (e.g. Amendments for 2014) and the feature used for the slicing can be discarded, since all data will have the same value for this feature.\n\n\n\n\n\n\nThis is basically the same – slicing can be done in any dimensions of the data cube. This would be the slicing of all amendments that will be used in a specific location. \u000bWe can, of course, slice again cubes that were already sliced further reducing its dimension (e.g. all amendment for a year and to be used in a place).\n\n\n\n\n\n\nDicing is slightly different from slicing: when we dice a data cube, we keep the same number of dimensions but get a reduced data set. Consider for example getting all amendments for a range of years and set of locations. The resulting data needs to keep the features used for dicing since we may have more than one value for those.\n\n\n\n\n\n\nPivoting data is a bit more complex: it uses the values of the features to create new columns and is usually done to summarize data.When we pivot data, we can use different operations to create the pivoted table cell’s values. In this case we could use “sum” to sum the values for all cells with the same Local, Categoria and Ano.\n\n\n\n\n\n\nHere is another example of pivoting: each project in the “CAPES-Projetos” dataset is spread out into several lines. I could pivot those to count how many members of each type each project contains. We’ll see in the examples!\n\n\n\n\n\n\nDrilling up is basically aggregation and counting. Drilling down is the opposite. We can only drill down if there is data to be used to show more details.For example, I cannot drill down the table in this slide because I don’t have how to add more information to get finer details. But if I had the full date instead of they year I could break it into day, month, year and do something conceptually similar to a drill down.\n\n\n\n\n\n\nSee the Parliamentary Amendments project!\n\n\n\nToday’s lecture will be about Data.\nHere’s are our main topics.\nJust as an exercise, try to explain data yourself. Avoid self-references!\nA very, very short definition of “data” for our purposes (“analyzing data”). It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\nRaw Data is the most common data type on the Web (and off it too) – data that was collected and put online or stored, without any preprocessing, annotations or even metadata.\nI got this from “An overview of a dataset digitized by citizen science volunteers – the 1900-1910 Daily Weather Reports“ (https://blogs.reading.ac.uk/weather-and-climate-at-reading/2020/an-overview-of-a-dataset-digitized-by-citizen-science-volunteers-the-1900-1910-daily-weather-reports/) – very interesting read! \u000bHumans collected the data by hand, then, many years later, digitized it using Citizen Science.\nThis are the daily measures of the level on Rio Ladário, part of the Paraguay River Basin. You can get its PDF from the “Centro deHidrografia e Navegação do Oeste - MARINHA DO BRASIL” website (https://www.marinha.mil.br/chn-6/?q=alturaAnterioresRios).There is probably a spreadsheet version of this PDF somewhere. But it is usable?The raw (useful!) data is somewhere at https://www.snirh.gov.br/hidroweb/serieshistoricas\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The data is shown as a HTML table. The chart is part of a report for the Engineering’s III Evaluation Group.Tables can be represented in text files, spreadsheets, XML and other formats.\u000bBut it is still not perfect data for us – how to we query those?\nFrom the Portal de Imigração Laboral of MINISTÉRIO DA JUSTIÇA E SEGURANÇA PÚBLICA (https://portaldeimigracao.mj.gov.br/pt/dados/dados-consolidados1) we can get data from immigrants in Brazil. The “plano tabular” is a spreadsheet with summaries but may not be useful for most queries – we need the full data set that was used to create those summaries.We can get it from the DataMigra portal (https://www.datamigraweb.unb.br/#/public), option “SOLICITANTES DE REFÚGIO”. \nThis section is a general discussion of what data can represent measures from the world, how it represents and how it is stored. It is a prelude to some notes on data transformation eventually leading to EDA and machine learning.In most of the cases we would like to deal with tables, but this is not always possible…\nHere’s an example of the different ways to consider data:Structure of Data: How the data is logically organized (e.g., rows representing municipalities and columns representing features of those municipalities such as area, population, and other attributes).Representation of Data: The format or schema used to express the data (e.g., a table with rows and columns, CSV format, JSON format).Storage of Data: The physical or digital medium and file format used to save the data (e.g., flat text file, spreadsheet, database).\nThis is our most basic data representation scheme: a collection of tuples, where each tuple element (the table column) contains measurements of some features of an object (the table lines).In this example we have a tuple on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may not be unique).We can easily imagine some operation on the tuples, e.g. find all values for a specific combination of “Localidade do gasto” and “Nome Função”.\u000bThis is the “Emendas Parlamentares” dataset, we’ll play with it later! Get it at https://portaldatransparencia.gov.br/download-de-dados/emendas-parlamentares\nThis is our most basic data representation scheme: a collection of tuples. In this example we also have tuples on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may be unique).It is easy to consider combination or aggregation operations on the tuples, for example, counting the immigrants from a country in a state, regardless of sex.This is the “Migration” dataset, we’ll play with it later!\nTime series are data: there is a temporal index that set an explicit order to it. Usually are represented as tuples of time -&gt; value1, value2, valueN. We expect the time to be unique.\nYet another collection of tuples but with a twist. Not only there are tuples for each participant in a project but the mapping of participants into projects is a graph, a different type of data that can be represented by tuples.This is the “CAPES-Projetos” dataset, we’ll play with it later!\nHere’s a region of the table that corresponds to a single project. We can consider each combination of ID_PESSOA within the same ID_PROJETO to be vertices in a network with the project being an edge.The additional material on https://rafaeldcsantos.github.io/CAP-394 shows how we can do it.\nThis is a single entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)Multimedia data cannot be used directly into machine learning algorithms: we must extract features of it beforehand, which often is not a trivial task.Movies and audio are other types of multimedia data which also requires feature extraction.\nWhen dealing with data obtained from the Web try to get also a data dictionary, a document that describes what the data contains (metadata!)\u000bOne common example of feature creation from data: if we have a timestamp, we can process it to get fields for days, hours, etc., possibly expanding to get information for days of the week and working hours, for example.Context is much more subtle: it is not part of data nor metadata, but may give important information about it.\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf\nBasically, we would like our data to look like a spreadsheet: each row represents an observation; each column represents one (and only one!) measure or variable on that observation. For this analogy to work we expect that the first row in the spreadsheet contains the names of the variables!Please refer to R for Data Science (2e), https://r4ds.hadley.nz/, for a more detailed description.\nDoes it look like a spreadsheet? It is probably tidy (but check for the single-value-in-a-cell rule!)Images can be considered tidy, but the semantics are very different – we don’t use pixels as values, usually there are some preprocessing steps first.\nIn wide format data, each variable is spread out across columns. It is better for reading by humans, some visualization and machine learning tasks. It can have missing data in its cells.\u000bIn long format data, each row is an observation and there are fewer columns. It is better for subsetting, it is tidier and more economic.\nOur supermarket data looks like a spreadsheet, looks tidy but could be made tidier – at what cost?Tidiness is also somehow relative. Consider what is a record: in this example it is an item being sold so the data is tidy-ish. If we consider transactions as the data represented than the data is somehow less tidy.\nWe have the data, possibly raw, hopefully tidy. We may need to change its shape to process it. Let’s talk about data shapes and transformations!\nLet’s think about the parliamentary amendments (”Emendas Parlamentares”) dataset. Each entry happened in one date (year), had an author, a code for the function, a place for the destination of the amendment and a value. On a 4-dimensional data cube we could have Year, Author, Code, Place for the dimensions and Value as the value stored in the cell. Using this way of representation, we could do some simple operations that allows exploration of the data, but on the other hand, conceptually, the cube could be made of mostly empty cells.\nWe can slice a N-dimensional data cube creating a (N-1)-dimensional data cube. Consider this as an operation that selects all records where a feature is equal to a single value. Considering the amendments example, we can slice the data cube getting all records for a particular year or for a particular place for the destination. The resulting dataset can be considered a new one (e.g. Amendments for 2014) and the feature used for the slicing can be discarded, since all data will have the same value for this feature.\nThis is basically the same – slicing can be done in any dimensions of the data cube. This would be the slicing of all amendments that will be used in a specific location. \u000bWe can, of course, slice again cubes that were already sliced further reducing its dimension (e.g. all amendment for a year and to be used in a place).\nDicing is slightly different from slicing: when we dice a data cube, we keep the same number of dimensions but get a reduced data set. Consider for example getting all amendments for a range of years and set of locations. The resulting data needs to keep the features used for dicing since we may have more than one value for those.\nPivoting data is a bit more complex: it uses the values of the features to create new columns and is usually done to summarize data.When we pivot data, we can use different operations to create the pivoted table cell’s values. In this case we could use “sum” to sum the values for all cells with the same Local, Categoria and Ano.\nHere is another example of pivoting: each project in the “CAPES-Projetos” dataset is spread out into several lines. I could pivot those to count how many members of each type each project contains. We’ll see in the examples!\nDrilling up is basically aggregation and counting. Drilling down is the opposite. We can only drill down if there is data to be used to show more details.For example, I cannot drill down the table in this slide because I don’t have how to add more information to get finer details. But if I had the full date instead of they year I could break it into day, month, year and do something conceptually similar to a drill down.",
    "crumbs": [
      "Lectures",
      "Data"
    ]
  },
  {
    "objectID": "03-Lectures-Data.html#slides",
    "href": "03-Lectures-Data.html#slides",
    "title": "Data",
    "section": "",
    "text": "Today’s lecture will be about Data.\n\n\n\n\n\n\nHere’s are our main topics.\n\n\n\n\n\n\nJust as an exercise, try to explain data yourself. Avoid self-references!\n\n\n\n\n\n\nA very, very short definition of “data” for our purposes (“analyzing data”). It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\n\n\n\n\n\n\nRaw Data is the most common data type on the Web (and off it too) – data that was collected and put online or stored, without any preprocessing, annotations or even metadata.\n\n\n\n\n\n\nI got this from “An overview of a dataset digitized by citizen science volunteers – the 1900-1910 Daily Weather Reports“ (https://blogs.reading.ac.uk/weather-and-climate-at-reading/2020/an-overview-of-a-dataset-digitized-by-citizen-science-volunteers-the-1900-1910-daily-weather-reports/) – very interesting read! \u000bHumans collected the data by hand, then, many years later, digitized it using Citizen Science.\n\n\n\n\n\n\nThis are the daily measures of the level on Rio Ladário, part of the Paraguay River Basin. You can get its PDF from the “Centro deHidrografia e Navegação do Oeste - MARINHA DO BRASIL” website (https://www.marinha.mil.br/chn-6/?q=alturaAnterioresRios).There is probably a spreadsheet version of this PDF somewhere. But it is usable?The raw (useful!) data is somewhere at https://www.snirh.gov.br/hidroweb/serieshistoricas\n\n\n\n\n\n\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The data is shown as a HTML table. The chart is part of a report for the Engineering’s III Evaluation Group.Tables can be represented in text files, spreadsheets, XML and other formats.\u000bBut it is still not perfect data for us – how to we query those?\n\n\n\n\n\n\nFrom the Portal de Imigração Laboral of MINISTÉRIO DA JUSTIÇA E SEGURANÇA PÚBLICA (https://portaldeimigracao.mj.gov.br/pt/dados/dados-consolidados1) we can get data from immigrants in Brazil. The “plano tabular” is a spreadsheet with summaries but may not be useful for most queries – we need the full data set that was used to create those summaries.We can get it from the DataMigra portal (https://www.datamigraweb.unb.br/#/public), option “SOLICITANTES DE REFÚGIO”. \n\n\n\n\n\n\nThis section is a general discussion of what data can represent measures from the world, how it represents and how it is stored. It is a prelude to some notes on data transformation eventually leading to EDA and machine learning.In most of the cases we would like to deal with tables, but this is not always possible…\n\n\n\n\n\n\nHere’s an example of the different ways to consider data:Structure of Data: How the data is logically organized (e.g., rows representing municipalities and columns representing features of those municipalities such as area, population, and other attributes).Representation of Data: The format or schema used to express the data (e.g., a table with rows and columns, CSV format, JSON format).Storage of Data: The physical or digital medium and file format used to save the data (e.g., flat text file, spreadsheet, database).\n\n\n\n\n\n\nThis is our most basic data representation scheme: a collection of tuples, where each tuple element (the table column) contains measurements of some features of an object (the table lines).In this example we have a tuple on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may not be unique).We can easily imagine some operation on the tuples, e.g. find all values for a specific combination of “Localidade do gasto” and “Nome Função”.\u000bThis is the “Emendas Parlamentares” dataset, we’ll play with it later! Get it at https://portaldatransparencia.gov.br/download-de-dados/emendas-parlamentares\n\n\n\n\n\n\nThis is our most basic data representation scheme: a collection of tuples. In this example we also have tuples on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may be unique).It is easy to consider combination or aggregation operations on the tuples, for example, counting the immigrants from a country in a state, regardless of sex.This is the “Migration” dataset, we’ll play with it later!\n\n\n\n\n\n\nTime series are data: there is a temporal index that set an explicit order to it. Usually are represented as tuples of time -&gt; value1, value2, valueN. We expect the time to be unique.\n\n\n\n\n\n\nYet another collection of tuples but with a twist. Not only there are tuples for each participant in a project but the mapping of participants into projects is a graph, a different type of data that can be represented by tuples.This is the “CAPES-Projetos” dataset, we’ll play with it later!\n\n\n\n\n\n\nHere’s a region of the table that corresponds to a single project. We can consider each combination of ID_PESSOA within the same ID_PROJETO to be vertices in a network with the project being an edge.The additional material on https://rafaeldcsantos.github.io/CAP-394 shows how we can do it.\n\n\n\n\n\n\nThis is a single entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\n\n\n\n\n\n\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\n\n\n\n\n\n\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)Multimedia data cannot be used directly into machine learning algorithms: we must extract features of it beforehand, which often is not a trivial task.Movies and audio are other types of multimedia data which also requires feature extraction.\n\n\n\n\n\n\nWhen dealing with data obtained from the Web try to get also a data dictionary, a document that describes what the data contains (metadata!)\u000bOne common example of feature creation from data: if we have a timestamp, we can process it to get fields for days, hours, etc., possibly expanding to get information for days of the week and working hours, for example.Context is much more subtle: it is not part of data nor metadata, but may give important information about it.\n\n\n\n\n\n\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf\n\n\n\n\n\n\nBasically, we would like our data to look like a spreadsheet: each row represents an observation; each column represents one (and only one!) measure or variable on that observation. For this analogy to work we expect that the first row in the spreadsheet contains the names of the variables!Please refer to R for Data Science (2e), https://r4ds.hadley.nz/, for a more detailed description.\n\n\n\n\n\n\nDoes it look like a spreadsheet? It is probably tidy (but check for the single-value-in-a-cell rule!)Images can be considered tidy, but the semantics are very different – we don’t use pixels as values, usually there are some preprocessing steps first.\n\n\n\n\n\n\nIn wide format data, each variable is spread out across columns. It is better for reading by humans, some visualization and machine learning tasks. It can have missing data in its cells.\u000bIn long format data, each row is an observation and there are fewer columns. It is better for subsetting, it is tidier and more economic.\n\n\n\n\n\n\nOur supermarket data looks like a spreadsheet, looks tidy but could be made tidier – at what cost?Tidiness is also somehow relative. Consider what is a record: in this example it is an item being sold so the data is tidy-ish. If we consider transactions as the data represented than the data is somehow less tidy.\n\n\n\n\n\n\nWe have the data, possibly raw, hopefully tidy. We may need to change its shape to process it. Let’s talk about data shapes and transformations!\n\n\n\n\n\n\nLet’s think about the parliamentary amendments (”Emendas Parlamentares”) dataset. Each entry happened in one date (year), had an author, a code for the function, a place for the destination of the amendment and a value. On a 4-dimensional data cube we could have Year, Author, Code, Place for the dimensions and Value as the value stored in the cell. Using this way of representation, we could do some simple operations that allows exploration of the data, but on the other hand, conceptually, the cube could be made of mostly empty cells.\n\n\n\n\n\n\nWe can slice a N-dimensional data cube creating a (N-1)-dimensional data cube. Consider this as an operation that selects all records where a feature is equal to a single value. Considering the amendments example, we can slice the data cube getting all records for a particular year or for a particular place for the destination. The resulting dataset can be considered a new one (e.g. Amendments for 2014) and the feature used for the slicing can be discarded, since all data will have the same value for this feature.\n\n\n\n\n\n\nThis is basically the same – slicing can be done in any dimensions of the data cube. This would be the slicing of all amendments that will be used in a specific location. \u000bWe can, of course, slice again cubes that were already sliced further reducing its dimension (e.g. all amendment for a year and to be used in a place).\n\n\n\n\n\n\nDicing is slightly different from slicing: when we dice a data cube, we keep the same number of dimensions but get a reduced data set. Consider for example getting all amendments for a range of years and set of locations. The resulting data needs to keep the features used for dicing since we may have more than one value for those.\n\n\n\n\n\n\nPivoting data is a bit more complex: it uses the values of the features to create new columns and is usually done to summarize data.When we pivot data, we can use different operations to create the pivoted table cell’s values. In this case we could use “sum” to sum the values for all cells with the same Local, Categoria and Ano.\n\n\n\n\n\n\nHere is another example of pivoting: each project in the “CAPES-Projetos” dataset is spread out into several lines. I could pivot those to count how many members of each type each project contains. We’ll see in the examples!\n\n\n\n\n\n\nDrilling up is basically aggregation and counting. Drilling down is the opposite. We can only drill down if there is data to be used to show more details.For example, I cannot drill down the table in this slide because I don’t have how to add more information to get finer details. But if I had the full date instead of they year I could break it into day, month, year and do something conceptually similar to a drill down.",
    "crumbs": [
      "Lectures",
      "Data"
    ]
  },
  {
    "objectID": "03-Lectures-Data.html#code-for-examples-used-in-this-lecture",
    "href": "03-Lectures-Data.html#code-for-examples-used-in-this-lecture",
    "title": "Data",
    "section": "",
    "text": "See the Parliamentary Amendments project!\n\n\n\nToday’s lecture will be about Data.\nHere’s are our main topics.\nJust as an exercise, try to explain data yourself. Avoid self-references!\nA very, very short definition of “data” for our purposes (“analyzing data”). It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\nRaw Data is the most common data type on the Web (and off it too) – data that was collected and put online or stored, without any preprocessing, annotations or even metadata.\nI got this from “An overview of a dataset digitized by citizen science volunteers – the 1900-1910 Daily Weather Reports“ (https://blogs.reading.ac.uk/weather-and-climate-at-reading/2020/an-overview-of-a-dataset-digitized-by-citizen-science-volunteers-the-1900-1910-daily-weather-reports/) – very interesting read! \u000bHumans collected the data by hand, then, many years later, digitized it using Citizen Science.\nThis are the daily measures of the level on Rio Ladário, part of the Paraguay River Basin. You can get its PDF from the “Centro deHidrografia e Navegação do Oeste - MARINHA DO BRASIL” website (https://www.marinha.mil.br/chn-6/?q=alturaAnterioresRios).There is probably a spreadsheet version of this PDF somewhere. But it is usable?The raw (useful!) data is somewhere at https://www.snirh.gov.br/hidroweb/serieshistoricas\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The data is shown as a HTML table. The chart is part of a report for the Engineering’s III Evaluation Group.Tables can be represented in text files, spreadsheets, XML and other formats.\u000bBut it is still not perfect data for us – how to we query those?\nFrom the Portal de Imigração Laboral of MINISTÉRIO DA JUSTIÇA E SEGURANÇA PÚBLICA (https://portaldeimigracao.mj.gov.br/pt/dados/dados-consolidados1) we can get data from immigrants in Brazil. The “plano tabular” is a spreadsheet with summaries but may not be useful for most queries – we need the full data set that was used to create those summaries.We can get it from the DataMigra portal (https://www.datamigraweb.unb.br/#/public), option “SOLICITANTES DE REFÚGIO”. \nThis section is a general discussion of what data can represent measures from the world, how it represents and how it is stored. It is a prelude to some notes on data transformation eventually leading to EDA and machine learning.In most of the cases we would like to deal with tables, but this is not always possible…\nHere’s an example of the different ways to consider data:Structure of Data: How the data is logically organized (e.g., rows representing municipalities and columns representing features of those municipalities such as area, population, and other attributes).Representation of Data: The format or schema used to express the data (e.g., a table with rows and columns, CSV format, JSON format).Storage of Data: The physical or digital medium and file format used to save the data (e.g., flat text file, spreadsheet, database).\nThis is our most basic data representation scheme: a collection of tuples, where each tuple element (the table column) contains measurements of some features of an object (the table lines).In this example we have a tuple on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may not be unique).We can easily imagine some operation on the tuples, e.g. find all values for a specific combination of “Localidade do gasto” and “Nome Função”.\u000bThis is the “Emendas Parlamentares” dataset, we’ll play with it later! Get it at https://portaldatransparencia.gov.br/download-de-dados/emendas-parlamentares\nThis is our most basic data representation scheme: a collection of tuples. In this example we also have tuples on the format V1, V2, V3 .. VN, Value (where the combination V1, VN… may be unique).It is easy to consider combination or aggregation operations on the tuples, for example, counting the immigrants from a country in a state, regardless of sex.This is the “Migration” dataset, we’ll play with it later!\nTime series are data: there is a temporal index that set an explicit order to it. Usually are represented as tuples of time -&gt; value1, value2, valueN. We expect the time to be unique.\nYet another collection of tuples but with a twist. Not only there are tuples for each participant in a project but the mapping of participants into projects is a graph, a different type of data that can be represented by tuples.This is the “CAPES-Projetos” dataset, we’ll play with it later!\nHere’s a region of the table that corresponds to a single project. We can consider each combination of ID_PESSOA within the same ID_PROJETO to be vertices in a network with the project being an edge.The additional material on https://rafaeldcsantos.github.io/CAP-394 shows how we can do it.\nThis is a single entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)Multimedia data cannot be used directly into machine learning algorithms: we must extract features of it beforehand, which often is not a trivial task.Movies and audio are other types of multimedia data which also requires feature extraction.\nWhen dealing with data obtained from the Web try to get also a data dictionary, a document that describes what the data contains (metadata!)\u000bOne common example of feature creation from data: if we have a timestamp, we can process it to get fields for days, hours, etc., possibly expanding to get information for days of the week and working hours, for example.Context is much more subtle: it is not part of data nor metadata, but may give important information about it.\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf\nBasically, we would like our data to look like a spreadsheet: each row represents an observation; each column represents one (and only one!) measure or variable on that observation. For this analogy to work we expect that the first row in the spreadsheet contains the names of the variables!Please refer to R for Data Science (2e), https://r4ds.hadley.nz/, for a more detailed description.\nDoes it look like a spreadsheet? It is probably tidy (but check for the single-value-in-a-cell rule!)Images can be considered tidy, but the semantics are very different – we don’t use pixels as values, usually there are some preprocessing steps first.\nIn wide format data, each variable is spread out across columns. It is better for reading by humans, some visualization and machine learning tasks. It can have missing data in its cells.\u000bIn long format data, each row is an observation and there are fewer columns. It is better for subsetting, it is tidier and more economic.\nOur supermarket data looks like a spreadsheet, looks tidy but could be made tidier – at what cost?Tidiness is also somehow relative. Consider what is a record: in this example it is an item being sold so the data is tidy-ish. If we consider transactions as the data represented than the data is somehow less tidy.\nWe have the data, possibly raw, hopefully tidy. We may need to change its shape to process it. Let’s talk about data shapes and transformations!\nLet’s think about the parliamentary amendments (”Emendas Parlamentares”) dataset. Each entry happened in one date (year), had an author, a code for the function, a place for the destination of the amendment and a value. On a 4-dimensional data cube we could have Year, Author, Code, Place for the dimensions and Value as the value stored in the cell. Using this way of representation, we could do some simple operations that allows exploration of the data, but on the other hand, conceptually, the cube could be made of mostly empty cells.\nWe can slice a N-dimensional data cube creating a (N-1)-dimensional data cube. Consider this as an operation that selects all records where a feature is equal to a single value. Considering the amendments example, we can slice the data cube getting all records for a particular year or for a particular place for the destination. The resulting dataset can be considered a new one (e.g. Amendments for 2014) and the feature used for the slicing can be discarded, since all data will have the same value for this feature.\nThis is basically the same – slicing can be done in any dimensions of the data cube. This would be the slicing of all amendments that will be used in a specific location. \u000bWe can, of course, slice again cubes that were already sliced further reducing its dimension (e.g. all amendment for a year and to be used in a place).\nDicing is slightly different from slicing: when we dice a data cube, we keep the same number of dimensions but get a reduced data set. Consider for example getting all amendments for a range of years and set of locations. The resulting data needs to keep the features used for dicing since we may have more than one value for those.\nPivoting data is a bit more complex: it uses the values of the features to create new columns and is usually done to summarize data.When we pivot data, we can use different operations to create the pivoted table cell’s values. In this case we could use “sum” to sum the values for all cells with the same Local, Categoria and Ano.\nHere is another example of pivoting: each project in the “CAPES-Projetos” dataset is spread out into several lines. I could pivot those to count how many members of each type each project contains. We’ll see in the examples!\nDrilling up is basically aggregation and counting. Drilling down is the opposite. We can only drill down if there is data to be used to show more details.For example, I cannot drill down the table in this slide because I don’t have how to add more information to get finer details. But if I had the full date instead of they year I could break it into day, month, year and do something conceptually similar to a drill down.",
    "crumbs": [
      "Lectures",
      "Data"
    ]
  },
  {
    "objectID": "20-Cookbook-Intro.html",
    "href": "20-Cookbook-Intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis section will contain some examples of common data science tasks in Python.\nTopics covered will eventually include:\n\nReading CSV and XLSX and writing CSV data.\nSlicing and dicing.\nPivoting data.",
    "crumbs": [
      "Cookbook",
      "Introduction"
    ]
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-0-AboutCourse.html",
    "href": "Resources/Slides/CAP394-2024-0-AboutCourse.html",
    "title": "CAP-394",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!"
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-0-AboutCourse.html#slides",
    "href": "Resources/Slides/CAP394-2024-0-AboutCourse.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!"
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-2-Skills.html",
    "href": "Resources/Slides/CAP394-2024-2-Skills.html",
    "title": "CAP-394",
    "section": "",
    "text": "This is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\n\n\n\n\n\n\nBasically we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\n\n\n\n\n\n\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\n\n\n\n\n\n\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\n\n\n\n\n\n\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \n\n\n\n\n\n\nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\n\n\n\n\n\n\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\n\n\n\n\n\n\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\n\n\n\n\n\n\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\n\n\n\n\n\n\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\n\n\n\n\n\n\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\n\n\n\n\n\n\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\n\n\n\n\n\n\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\n\n\n\n\n\n\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\n\n\n\n\n\n\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\n\n\n\n\n\n\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\n\n\n\n\n\n\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\n\n\n\n\n\n\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\n\n\n\n\n\n\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please writecode in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\n\n\n\n\n\n\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\n\n\n\n\n\n\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\n\n\n\n\n\n\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\n\n\n\n\n\n\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\n\n\n\n\n\n\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\n\n\n\n\n\n\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\n\n\n\n\n\n\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\n\n\n\n\n\n\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\n\n\n\n\n\n\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\n\n\n\n\n\n\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\n\n\n\n\n\n\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\n\n\n\n\n\n\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\n\n\n\n\n\n\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\n\n\n\n\n\n\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\n\n\n\n\n\n\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\n\n\n\n\n\n\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\n\n\n\n\n\n\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\n\n\n\n\n\n\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\n\n\n\n\n\n\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\n\n\n\n\n\n\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\n\n\n\n\n\n\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\n\n\n\n\n\n\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\nBasically we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please writecode in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!"
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-2-Skills.html#slides",
    "href": "Resources/Slides/CAP394-2024-2-Skills.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "This is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\n\n\n\n\n\n\nBasically we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\n\n\n\n\n\n\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\n\n\n\n\n\n\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\n\n\n\n\n\n\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \n\n\n\n\n\n\nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\n\n\n\n\n\n\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\n\n\n\n\n\n\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\n\n\n\n\n\n\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\n\n\n\n\n\n\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\n\n\n\n\n\n\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\n\n\n\n\n\n\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\n\n\n\n\n\n\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\n\n\n\n\n\n\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\n\n\n\n\n\n\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\n\n\n\n\n\n\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\n\n\n\n\n\n\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\n\n\n\n\n\n\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\n\n\n\n\n\n\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please writecode in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\n\n\n\n\n\n\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\n\n\n\n\n\n\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\n\n\n\n\n\n\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\n\n\n\n\n\n\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\n\n\n\n\n\n\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\n\n\n\n\n\n\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\n\n\n\n\n\n\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\n\n\n\n\n\n\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\n\n\n\n\n\n\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\n\n\n\n\n\n\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\n\n\n\n\n\n\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\n\n\n\n\n\n\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\n\n\n\n\n\n\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\n\n\n\n\n\n\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\n\n\n\n\n\n\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\n\n\n\n\n\n\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\n\n\n\n\n\n\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\n\n\n\n\n\n\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\n\n\n\n\n\n\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\n\n\n\n\n\n\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\n\n\n\n\n\n\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\n\n\n\n\n\n\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\nBasically we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please writecode in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About this Course",
    "section": "",
    "text": "These pages contains the material used for the course Introduction to Data Science, part of the curriculum of the Graduate Program in Applied Computing, offeredy by the Brazilian National Institute for Space Research.\nThe course is usually offered in the second term of each year, exclusively on-site. Albeit the material is in English, classes are usually taught in Portuguese.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "About this Course",
    "section": "Syllabus",
    "text": "Syllabus\n    What is Data Science? Why Data Science exists as a discipline?     The role of a Data Scientist. Other roles and tasks in Data Science.     Data Collection and Discovery. Data Provenance and Annotation.      Data Representation: Flat Files, Databases, Tidy Data.     Introduction to Analytics, Exploratory Data Analysis and Machine Learning.     Reproducible Research. Data-based products.      Examples of applications, case studies and project development.\n\nBibliography\nFor a good all-around reference in Data Science see The Art of Data Science: A Guide for Anyone who works with Data (Peng and Matsui 2015) or Doing Data Science: Straight Talk from the Frontline (O’Neil and Schutt 2013).\nFor information on data scientists profiles see Analyzing the Analyzers: An Introspective Survey of Data Scientists and Their Work (Harris, Murphy, and Vaisman 2013) or Data scientists at work (Gutierrez 2014).\n\n\nUnder Construction\n\nThis material is still under construction (during the second term of 2024).",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#Schedule",
    "href": "index.html#Schedule",
    "title": "About this Course",
    "section": "Schedule for 2024",
    "text": "Schedule for 2024\nFor the second term of 2024 our schedule is Fridays, from 1:30PM to 5:30PM.\n\n\n\n\n\n\n\nDay\nTopics\n\n\n\n\nJune 14\nIntroduction to the course. Reading of About the Course and What is Data Science? lectures, rules for evaluation.\n\n\nJune 21\nReading of Skills for Data Science lecture. Q&A about the first homework (if needed).\n\n\nJune 28\nLectures suspended today\n\n\nJuly 5\nReading of Data. Presentation and comments on the first homework.\n\n\nJuly 12\nReading of the Supermarket Basket Analysis case.Presentation and comments on the second homework.\n\n\nJuly 19\nLectures suspended today\n\n\nJuly 26\nLectures suspended today\n\n\nAugust 2\nShort introduction to models and classification (lecture notes still under construction)\n\n\nAugust 9\nLet’s talk about the capstone project?\n\n\nAugust 16\nPresentation and comments on the third homework.\n\n\nAugust 23\nModels, classification and clustering (probably)\n\n\nAugust 30\nPresentation and comments on the Capstone Project.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#homework",
    "href": "index.html#homework",
    "title": "About this Course",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\n\nAssignment\nDue date (before that day’s lecture!)\n\n\n\n\nHomework #1\nJune 28 July 5\n\n\nHomework #2\nJuly 19\n\n\nTBD\nAugust 9\n\n\nCapstone Project\nAugust 30",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "01-Lectures-WhatIs.html",
    "href": "01-Lectures-WhatIs.html",
    "title": "What is Data Science?",
    "section": "",
    "text": "This is the material (slides and notes) of the first lecture on the course.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.",
    "crumbs": [
      "Lectures",
      "What is Data Science?"
    ]
  },
  {
    "objectID": "01-Lectures-WhatIs.html#slides",
    "href": "01-Lectures-WhatIs.html#slides",
    "title": "What is Data Science?",
    "section": "",
    "text": "This is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.",
    "crumbs": [
      "Lectures",
      "What is Data Science?"
    ]
  },
  {
    "objectID": "00-Lectures-Intro.html",
    "href": "00-Lectures-Intro.html",
    "title": "About the Course",
    "section": "",
    "text": "This is the material (slides and notes) of the introduction to the course.\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!",
    "crumbs": [
      "Lectures",
      "About the Course"
    ]
  },
  {
    "objectID": "00-Lectures-Intro.html#slides",
    "href": "00-Lectures-Intro.html#slides",
    "title": "About the Course",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!",
    "crumbs": [
      "Lectures",
      "About the Course"
    ]
  },
  {
    "objectID": "53A-Sales.html",
    "href": "53A-Sales.html",
    "title": "Are there temporal patterns on the sales data?",
    "section": "",
    "text": "import pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport plotly.express as px\nimport plotly.graph_objects as go\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n\n# Read the data from a file into a dataframe, with proper character encoding\nsupermDF = pd.read_csv(\"Resources/Data/SupermarketCSV/transactions.csv\",\n                        encoding=\"iso-8859-1\")\nsupermDF['DateTime'] = pd.to_datetime(supermDF['Date'].astype(str),format='%Y%m%d%H%M%S')\n\n\nprint(supermDF.head(10))\n\n                       FileID  StoreID  POSID    TransactionID  \\\n0  050501/075954_01090841.xml        3      1  075954_01090841   \n1  050501/075954_01090841.xml        3      1  075954_01090841   \n2  050501/075954_01090841.xml        3      1  075954_01090841   \n3  050501/075956_01090842.xml        3      1  075956_01090842   \n4  050501/075956_01090842.xml        3      1  075956_01090842   \n5  050501/080011_01090843.xml        3      1  080011_01090843   \n6  050501/080011_01090843.xml        3      1  080011_01090843   \n7  050501/080011_01090843.xml        3      1  080011_01090843   \n8  050501/080011_02003462.xml        3      2  080011_02003462   \n9  050501/080011_02003463.xml        3      2  080011_02003463   \n\n             Date  itemOrder             Description  UnitPrice  Quantity  \\\n0  20050501080242          1  VELA FESTLAR FELIZ ANI       0.78       1.0   \n1  20050501080242          2  VELA FESTLAR FELIZ ANI       0.78       1.0   \n2  20050501080242          3  ESCOVA D. TEK DURA           1.35       1.0   \n3  20050501080351          1  SUSTAGEN 380G KIDS CHO       7.89       1.0   \n4  20050501080351          2  PAO FRANCES                  0.17       6.0   \n5  20050501080420          1  PAO FRANCES                  0.17       6.0   \n6  20050501080420          2  LEITE PAST. SERRAMAR S       1.15       1.0   \n7  20050501080420          3  FARINHA TRIGO LILI KG        0.89       1.0   \n8  20050501074105          1  OLEO P/ MOTOR HAVOLINE       6.22       1.0   \n9  20050501074136          1  MARGARINA SOYA 500G C/       1.19       1.0   \n\n   Amount Unit   PaymentMethod  TotalPaid  TotalDue  Change Status  \\\n0    0.78   Un  Dinheiro             4.00      2.91    1.09      T   \n1    0.78   Un  Dinheiro             4.00      2.91    1.09      T   \n2    1.35   Un  Dinheiro             4.00      2.91    1.09      T   \n3    7.89   Un  Dinheiro            10.00      8.91    1.09      T   \n4    1.02   Un  Dinheiro            10.00      8.91    1.09      T   \n5    1.02   Un  Dinheiro             3.06      3.06    0.00      T   \n6    1.15   Un  Dinheiro             3.06      3.06    0.00      T   \n7    0.89   Un  Dinheiro             3.06      3.06    0.00      T   \n8    6.22   Un  Dinheiro            10.00      6.22    3.78      T   \n9    1.19   Un  Dinheiro             5.00      3.08    1.92      T   \n\n             DateTime  \n0 2005-05-01 08:02:42  \n1 2005-05-01 08:02:42  \n2 2005-05-01 08:02:42  \n3 2005-05-01 08:03:51  \n4 2005-05-01 08:03:51  \n5 2005-05-01 08:04:20  \n6 2005-05-01 08:04:20  \n7 2005-05-01 08:04:20  \n8 2005-05-01 07:41:05  \n9 2005-05-01 07:41:36  \n\n\nWe can use EDA to investigate several different aspects of the data. Let’s consider the sale of a particular item, the most sold, \"PAO FRANCES           \". Is there a temporal pattern on its sale? We can answer that with a two-dimensional histogram that shows the amount of units sold by day and time.\nFirst let’s create a subset of the data containing only the item in question. From this subset let’s also keep only the relevant fields. Let’s also create columns for day of the month and time of the day:\n\n# Get only the first itens.\nfiltered_df = supermDF[supermDF['itemOrder'] == 1]\nfiltered_df['DayOfMonth'] = filtered_df['DateTime'].dt.day\nfiltered_df['HourOfDay'] = filtered_df['DateTime'].dt.hour\n\n/var/folders/m8/dc_5nrhd5wqc0pxv77f9t9140000gn/T/ipykernel_39822/1004461388.py:3: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/m8/dc_5nrhd5wqc0pxv77f9t9140000gn/T/ipykernel_39822/1004461388.py:4: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n::: {#9e006bf7 .cell .custompython# Extract hour and day execution_count=5}\nhistoTransactions = filtered_df.groupby([\"HourOfDay\",\"DayOfMonth\"]).count().reset_index() \n:::\n\n# Reorders the two-dimensional histogram as a matrix-like dataframe.\nhmap = histoTransactions.pivot(index=\"HourOfDay\", columns=\"DayOfMonth\", values=\"TotalDue\").fillna(0)\n# Use those indexes for the row and column indexes.\ndays = list(range(1, 32))\nhours = list(range(24))\n# Apply those indexes to the dataframe, filling nonexistent (na) values with zeros.\nhmap = hmap.reindex(index=hours).reindex(columns=days).fillna(0)\nhmap.to_csv(\"transactions-number-heatmap.csv\")\n\nNow that we have our well-formed two-dimensional dataframe we can plot it as a heat map.\n\n# Create the heatmap.\nfig = go.Figure(data=go.Heatmap(\n    z=hmap.values,\n    x=hmap.columns,\n    y=hmap.index,\n    colorscale='Jet',\n    text=hmap.values,\n    texttemplate=\"%{text:.0f}\",  # Format text to display as integers\n    textfont={\"size\":10},\n    hoverinfo='z',  # Display value on hover\n    showscale=True\n))\n# Update layout to add thin lines around each cell.\n# This was suggested by Code Copilot!\nfig.update_traces(\n    zmin=0, zmax=hmap.values.max(),  \n    xgap=1,  \n    ygap=1,  \n    colorbar=dict(tickfont=dict(size=10))  \n)\n# Customize the layout.\nfig.update_layout(\n    title='Heatmap of Number of Transactions',\n    xaxis_title='Day of Month',\n    yaxis_title='Hour of Day',\n    xaxis=dict(\n        tickmode='linear',\n        dtick=1,  \n        tickfont=dict(size=10),\n        showgrid=False,\n        zeroline=False # Remove grid lines\n    ),\n    yaxis=dict(\n        tickmode='linear',\n        dtick=1, \n        tickfont=dict(size=10),\n        showgrid=False,\n        zeroline=False # Remove grid lines\n    ),\n    plot_bgcolor='#808080', \n)\n# Invert the Y-axis.\nfig.update_yaxes(autorange='reversed')\n# Show the plot.\nfig.show()\n\n                                                \n\n\n\nhistoValues = filtered_df.groupby([\"HourOfDay\",\"DayOfMonth\"])['TotalDue'].sum().reset_index()\n\n\n# Reorders the two-dimensional histogram as a matrix-like dataframe.\nhmap = histoValues.pivot(index=\"HourOfDay\", columns=\"DayOfMonth\", values=\"TotalDue\").fillna(0)\n# Use those indexes for the row and column indexes.\ndays = list(range(1, 32))\nhours = list(range(24))\n# Apply those indexes to the dataframe, filling nonexistent (na) values with zeros.\nhmap = hmap.reindex(index=hours).reindex(columns=days).fillna(0)\nhmap.to_csv(\"transactions-totaldue-heatmap.csv\")\n\nNow that we have our well-formed two-dimensional dataframe we can plot it as a heat map.\n\n# Create the heatmap.\nfig = go.Figure(data=go.Heatmap(\n    z=hmap.values,\n    x=hmap.columns,\n    y=hmap.index,\n    colorscale='Jet',\n    text=hmap.values,\n    texttemplate=\"%{text:.0f}\",  # Format text to display as integers\n    textfont={\"size\":10},\n    hoverinfo='z',  # Display value on hover\n    showscale=True\n))\n# Update layout to add thin lines around each cell.\n# This was suggested by Code Copilot!\nfig.update_traces(\n    zmin=0, zmax=hmap.values.max(),  \n    xgap=1,  \n    ygap=1,  \n    colorbar=dict(tickfont=dict(size=10))  \n)\n# Customize the layout.\nfig.update_layout(\n    title='Heatmap of Total Due for Transactions',\n    xaxis_title='Day of Month',\n    yaxis_title='Hour of Day',\n    xaxis=dict(\n        tickmode='linear',\n        dtick=1,  \n        tickfont=dict(size=10),\n        showgrid=False,\n        zeroline=False # Remove grid lines\n    ),\n    yaxis=dict(\n        tickmode='linear',\n        dtick=1, \n        tickfont=dict(size=10),\n        showgrid=False,\n        zeroline=False # Remove grid lines\n    ),\n    plot_bgcolor='#808080', \n)\n# Invert the Y-axis.\nfig.update_yaxes(autorange='reversed')\n# Show the plot.\nfig.show()\n\n                                                \n\n\n\ncerv = supermDF[supermDF['Description'].str.contains(\"BEB. CERV.\", case=False, na=False)]\ncerv['DayOfMonth'] = cerv['DateTime'].dt.day\ncerv['HourOfDay'] = cerv['DateTime'].dt.hour\nhistoValues = cerv.groupby([\"HourOfDay\",\"DayOfMonth\"])['TotalDue'].sum().reset_index()\n\n/var/folders/m8/dc_5nrhd5wqc0pxv77f9t9140000gn/T/ipykernel_39822/3765139457.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/m8/dc_5nrhd5wqc0pxv77f9t9140000gn/T/ipykernel_39822/3765139457.py:3: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# Reorders the two-dimensional histogram as a matrix-like dataframe.\nhmap = histoValues.pivot(index=\"HourOfDay\", columns=\"DayOfMonth\", values=\"TotalDue\").fillna(0)\n# Use those indexes for the row and column indexes.\ndays = list(range(1, 32))\nhours = list(range(24))\n# Apply those indexes to the dataframe, filling nonexistent (na) values with zeros.\nhmap = hmap.reindex(index=hours).reindex(columns=days).fillna(0)\n\nNow that we have our well-formed two-dimensional dataframe we can plot it as a heat map.\n\n# Create the heatmap.\nfig = go.Figure(data=go.Heatmap(\n    z=hmap.values,\n    x=hmap.columns,\n    y=hmap.index,\n    colorscale='Jet',\n    text=hmap.values,\n    texttemplate=\"%{text:.0f}\",  # Format text to display as integers\n    textfont={\"size\":10},\n    hoverinfo='z',  # Display value on hover\n    showscale=True\n))\n# Update layout to add thin lines around each cell.\n# This was suggested by Code Copilot!\nfig.update_traces(\n    zmin=0, zmax=hmap.values.max(),  \n    xgap=1,  \n    ygap=1,  \n    colorbar=dict(tickfont=dict(size=10))  \n)\n# Customize the layout.\nfig.update_layout(\n    title='Heatmap of Total Sales for Beer',\n    xaxis_title='Day of Month',\n    yaxis_title='Hour of Day',\n    xaxis=dict(\n        tickmode='linear',\n        dtick=1,  \n        tickfont=dict(size=10),\n        showgrid=False,\n        zeroline=False # Remove grid lines\n    ),\n    yaxis=dict(\n        tickmode='linear',\n        dtick=1, \n        tickfont=dict(size=10),\n        showgrid=False,\n        zeroline=False # Remove grid lines\n    ),\n    plot_bgcolor='#808080', \n)\n# Invert the Y-axis.\nfig.update_yaxes(autorange='reversed')\n# Show the plot.\nfig.show()\n\n                                                \n\n\n\ndef countContains(df, substring):\n    matching_descriptions = df[df['Description'].str.contains(substring, case=False, na=False)]\n    description_counts = matching_descriptions.groupby('Description').size().reset_index(name='Count')\n    return description_counts\n\n\nprint(\"Forma\",countContains(supermDF,\"FORMA\"))\nprint(\"Refrigerante\",countContains(supermDF,\"BEB. REF.\"))\nprint(\"Cerveja\",countContains(supermDF,\"BEB. CERV.\"))\n\nForma                Description  Count\n0   FORMA BOLO RAMOS QUAD.      2\n1   FORMA EIRILAR PUDIM S/      2\n2   FORMA FESTLAR N5 BRANC      5\n3   FORMA FESTLAR N6 AZUL       3\n4   FORMA FESTLAR N6 BRANC      8\n5   FORMA FESTLAR N6 ROSA      10\n6   FORMA GELO JAGUAR R 61      2\n7   FORMA GELO PLASVALE C/      1\n8   FORMA MARINEX C.816228      1\n9   FORMA MARINEX TORTA 63      1\n10  FORMA PIZZA EIRILAR CO      1\n11  FORMA PUDIM EIRILAR S/      5\n12  FORMA RETANGULAR ARARA      4\n13  PAO ENERGIA FORMA 500G    192\n14  PAO FORMA                 674\n15  PAO PANCO FORMA JUNIOR     38\n16  PAO PANCO FORMA LIGHT      20\n17  PAO PANCO FORMA TICA      571\n18  PAO VENEZA FORMA 500G     141\n19  PAO WICK BOLD FORMA        40\n20  REV. BOA FORMA              1\nRefrigerante                Description  Count\n0   BEB. REF.ANTARTICA 2LT    480\n1   BEB. REF.ANTARTICA 350    112\n2   BEB. REF.ANTARTICA 600    116\n3   BEB. REF.ANTARTICA GUA    124\n4   BEB. REF.BRAHMA SUKITA     39\n5   BEB. REF.COCA COLA  29      2\n6   BEB. REF.COCA COLA  35    483\n7   BEB. REF.COCA COLA  60    221\n8   BEB. REF.COCA COLA 1,5    205\n9   BEB. REF.COCA COLA 1L     348\n10  BEB. REF.COCA COLA 2,5    220\n11  BEB. REF.COCA COLA 2L    3303\n12  BEB. REF.COCA COLA L.     142\n13  BEB. REF.COCA COLA L.2    217\n14  BEB. REF.COCA COLA L.L     17\n15  BEB. REF.COCA COLA LIG     14\n16  BEB. REF.FANTA LAR 600     36\n17  BEB. REF.FANTA LAR MIX     50\n18  BEB. REF.FANTA LARANJA    677\n19  BEB. REF.FANTA UVA  35     54\n20  BEB. REF.FANTA UVA  60     71\n21  BEB. REF.FANTA UVA 2L     293\n22  BEB. REF.KUAT  350ML       57\n23  BEB. REF.KUAT  350ML L     19\n24  BEB. REF.KUAT  600ML       50\n25  BEB. REF.KUAT 1,5L          5\n26  BEB. REF.KUAT 2L          390\n27  BEB. REF.KUAT 2L LIGHT     73\n28  BEB. REF.PEPSI 2L          48\n29  BEB. REF.PEPSI 600ML        6\n30  BEB. REF.PEPSI LT 350M     29\n31  BEB. REF.PIRACAIA GUAR   1453\n32  BEB. REF.PIRACAIA LARA    411\n33  BEB. REF.PIRACAIA LIMA     57\n34  BEB. REF.PIRACAIA TUTI    104\n35  BEB. REF.PIRACAIA UVA      68\n36  BEB. REF.SCHIN  350ML      12\n37  BEB. REF.SPRITE  350ML     36\n38  BEB. REF.SPRITE  600ML     29\n39  BEB. REF.SPRITE 2L        144\n40  BEB. REF.SPRITE 350ML      17\n41  BEB. REF.SPRITE DIET 2     57\nCerveja                Description  Count\n0   BEB. CERV.BAVARIA 355M      2\n1   BEB. CERV.BAVARIA LT 3    415\n2   BEB. CERV.BAVARIA PREM      3\n3   BEB. CERV.BRAHMA LT 12      5\n4   BEB. CERV.BRAHMA LT 35    124\n5   BEB. CERV.CARACU L.N.       8\n6   BEB. CERV.CARACU LT 35     62\n7   BEB. CERV.GLACIAL 600M      1\n8   BEB. CERV.HEINEKEN 350      1\n9   BEB. CERV.KAISER 350ML    140\n10  BEB. CERV.KAISER 355 M      4\n11  BEB. CERV.PRIMUS 600ML      5\n12  BEB. CERV.SANTA CERVA       1\n13  BEB. CERV.SCHIN 350ML       5\n14  BEB. CERV.SCHIN LN TEQ      3\n15  BEB. CERV.SCHIN S/A LN      1\n16  BEB. CERV.SCHIN S/ALC.      1\n17  BEB. CERV.SCHINCARIOL      15\n18  BEB. CERV.SKOL 600ML      187\n19  BEB. CERV.SKOL BEATS L      2\n20  BEB. CERV.SKOL LN 355M     12\n21  BEB. CERV.SKOL LT 350M    178\n22  BEB. CERV.SKOL LT 473M     83\n23  BEB. CERV.XINGU 350ML       3\n24  BEB. CERV.XINGU 355ML       6\n\n\n\nimport plotly.graph_objects as go\nimport matplotlib.cm as cm\nimport matplotlib.colors as mcolors\n\ndata = pd.read_csv(\"transactions-number-heatmap.csv\")\n\n# Prepare the data for plotting\nhourly_data = data.set_index('HourOfDay')\n\ncmap = cm.get_cmap('viridis', len(hourly_data.columns))\ncolors = [mcolors.to_hex(cmap(i)) for i in range(len(hourly_data.columns))]\n\n\n# Create the plot\nfig = go.Figure()\n\n# Add a line for each day with a color ramp\nfor i, day in enumerate(hourly_data.columns):\n    fig.add_trace(go.Scatter(\n        x=hourly_data.index,\n        y=hourly_data[day],\n        mode='lines',\n        line=dict(color=colors[i]),\n        name=f'Day {day}',\n        showlegend=False  # Disable the legend\n    ))\n\n# Calculate the total number of transactions per hour across all days\ntotal_transactions_per_hour = hourly_data.sum(axis=1)\n\n# Calculate the weighted average hour\nhours = hourly_data.index.values\nweighted_average_hour = (total_transactions_per_hour * hours).sum() / total_transactions_per_hour.sum()\n\n# Add a thick vertical red line for the weighted average hour\nfig.add_trace(go.Scatter(\n    x=[weighted_average_hour, weighted_average_hour],\n    y=[0, hourly_data.values.max()],\n    mode='lines',\n    line=dict(color='red', width=4),\n    name='Peak Hour',\n    showlegend=True  # Show legend for the average hour line\n))\n\n# Customize the layout\nfig.update_layout(\n    title='Hourly Transactions for Each Day of the Month',\n    xaxis_title='Hour of the Day',\n    yaxis_title='Number of Transactions',\n    xaxis=dict(tickmode='linear', tick0=0, dtick=1),\n    hovermode='x unified'\n)\n\n# Show the plot\nfig.show()\n\n/var/folders/m8/dc_5nrhd5wqc0pxv77f9t9140000gn/T/ipykernel_39822/710920284.py:10: MatplotlibDeprecationWarning:\n\nThe get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n\n\n\n                                                \n\n\n\nhourly_data = data.set_index('HourOfDay')\nholidays = ['1', '8', '15', '22', '26', '29']\nworkdays = [day for day in hourly_data.columns if day not in holidays]\n\n# Split the data into workdays and holidays\nworkdays_data = hourly_data[workdays]\nholidays_data = hourly_data[holidays]\n\n\nimport pandas as pd\nimport plotly.graph_objects as go\nimport matplotlib.cm as cm\nimport matplotlib.colors as mcolors\n\n\n# Prepare the data\nhourly_data = data.set_index('HourOfDay')\n\n# Define holidays and workdays based on the given information\nholidays = ['1', '8', '15', '22', '26', '29']\nworkdays = [day for day in hourly_data.columns if day not in holidays]\n\n# Split the data into workdays and holidays\nworkdays_data = hourly_data[workdays]\nholidays_data = hourly_data[holidays]\n\n# Create a color map using greenish hues for workdays and reddish hues for holidays\nworkday_colors = [mcolors.to_hex(cm.Greens(i / len(workdays_data.columns))) for i in range(len(workdays_data.columns))]\nholiday_colors = [mcolors.to_hex(cm.Reds(i / len(holidays_data.columns))) for i in range(len(holidays_data.columns))]\n\n# Create the plot\nfig = go.Figure()\n\n# Add a line for each workday\nfor i, day in enumerate(workdays_data.columns):\n    fig.add_trace(go.Scatter(\n        x=workdays_data.index,\n        y=workdays_data[day],\n        mode='lines',\n        line=dict(color=workday_colors[i]),\n        name=f'Workday {day}',\n        showlegend=False  # Disable the legend\n    ))\n\n# Add a line for each holiday\nfor i, day in enumerate(holidays_data.columns):\n    fig.add_trace(go.Scatter(\n        x=holidays_data.index,\n        y=holidays_data[day],\n        mode='lines',\n        line=dict(color=holiday_colors[i]),\n        name=f'Holiday {day}',\n        showlegend=False  # Disable the legend\n    ))\n\n# Customize the layout\nfig.update_layout(\n    title='Hourly Transactions for Workdays and Holidays',\n    xaxis_title='Hour of the Day',\n    yaxis_title='Number of Transactions',\n    xaxis=dict(tickmode='linear', tick0=0, dtick=1),\n    hovermode='x unified'\n)\n\n# Show the plot\nfig.show()\n\n                                                \n\n\n\nimport pandas as pd\nimport plotly.graph_objects as go\nimport matplotlib.cm as cm\nimport matplotlib.colors as mcolors\n\n# Load the data\ndata = pd.read_csv(\"transactions-number-heatmap.csv\")\n\n# Prepare the data for plotting\nhourly_data = data.set_index('HourOfDay')\n\n# Define holidays and workdays based on the given information\nholidays = ['1', '8', '15', '22', '26', '29']\nworkdays = [day for day in hourly_data.columns if day not in holidays]\n\n# Split the data into workdays and holidays\nworkdays_data = hourly_data[workdays]\nholidays_data = hourly_data[holidays]\n\n# Calculate the total number of transactions per hour for each subset\ntotal_transactions_per_hour_workdays = workdays_data.sum(axis=1)\ntotal_transactions_per_hour_holidays = holidays_data.sum(axis=1)\n\n# Calculate the weighted average hour for each subset\nhours = hourly_data.index.values\nweighted_average_hour_workdays = (total_transactions_per_hour_workdays * hours).sum() / total_transactions_per_hour_workdays.sum()\nweighted_average_hour_holidays = (total_transactions_per_hour_holidays * hours).sum() / total_transactions_per_hour_holidays.sum()\n\n# Create a color map using greenish hues for workdays and reddish hues for holidays\nworkday_colors = [mcolors.to_hex(cm.Greens(i / len(workdays_data.columns))) for i in range(len(workdays_data.columns))]\nholiday_colors = [mcolors.to_hex(cm.Reds(i / len(holidays_data.columns))) for i in range(len(holidays_data.columns))]\n\n# Create the plot\nfig = go.Figure()\n\n# Add a line for each workday\nfor i, day in enumerate(workdays_data.columns):\n    fig.add_trace(go.Scatter(\n        x=workdays_data.index,\n        y=workdays_data[day],\n        mode='lines',\n        line=dict(color=workday_colors[i]),\n        name=f'Workday {day}',\n        showlegend=False  # Disable the legend\n    ))\n\n# Add a line for each holiday\nfor i, day in enumerate(holidays_data.columns):\n    fig.add_trace(go.Scatter(\n        x=holidays_data.index,\n        y=holidays_data[day],\n        mode='lines',\n        line=dict(color=holiday_colors[i]),\n        name=f'Holiday {day}',\n        showlegend=False  # Disable the legend\n    ))\n\n# Add a thick vertical red line for the weighted average hour of workdays\nfig.add_trace(go.Scatter(\n    x=[weighted_average_hour_workdays, weighted_average_hour_workdays],\n    y=[0, hourly_data.values.max()],\n    mode='lines',\n    line=dict(color='darkgreen', width=4),\n    name='Average Hour Workdays',\n    showlegend=True  # Show legend for the average hour line\n))\n\n# Add a thick vertical purple line for the weighted average hour of holidays\nfig.add_trace(go.Scatter(\n    x=[weighted_average_hour_holidays, weighted_average_hour_holidays],\n    y=[0, hourly_data.values.max()],\n    mode='lines',\n    line=dict(color='darkred', width=4),\n    name='Average Hour Holidays',\n    showlegend=True  # Show legend for the average hour line\n))\n\n# Customize the layout\nfig.update_layout(\n    title='Hourly Transactions with Average Hours for Workdays and Holidays',\n    xaxis_title='Hour of the Day',\n    yaxis_title='Number of Transactions',\n    xaxis=dict(tickmode='linear', tick0=0, dtick=1),\n    hovermode='x unified'\n)\n\n# Show the plot\nfig.show()"
  }
]