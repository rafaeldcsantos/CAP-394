[
  {
    "objectID": "00-Homework-02.html",
    "href": "00-Homework-02.html",
    "title": "Homework #2",
    "section": "",
    "text": "How to deliver\n\nThis homework requires reading of the Supermarket Basket Analysis case study. Please deliver the answer to this homework either as an online notebook link or as a single, correctly formatted PDF file created from the notebook. Code, results (text or figures) and comments are required.\n\n\n\nChoose three exercises on the Supermarket Basket Analysis case study that are about counting and solve them.\n\n\n\nWe found some strange patterns on the Supermarket Basket Analysis case study – e.g. very large transactions, large amounts of items bought in some days. Check if those patterns are specific to a payment method or store branch or status of the transaction."
  },
  {
    "objectID": "00-Homework-02.html#playing-data-detective-part-1.",
    "href": "00-Homework-02.html#playing-data-detective-part-1.",
    "title": "Homework #2",
    "section": "",
    "text": "Choose three exercises on the Supermarket Basket Analysis case study that are about counting and solve them."
  },
  {
    "objectID": "00-Homework-02.html#playing-data-detective-part-2.",
    "href": "00-Homework-02.html#playing-data-detective-part-2.",
    "title": "Homework #2",
    "section": "",
    "text": "We found some strange patterns on the Supermarket Basket Analysis case study – e.g. very large transactions, large amounts of items bought in some days. Check if those patterns are specific to a payment method or store branch or status of the transaction."
  },
  {
    "objectID": "00-Lectures-Intro.html",
    "href": "00-Lectures-Intro.html",
    "title": "About the Course",
    "section": "",
    "text": "This is the material (slides and notes) of the introduction to the course.\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!",
    "crumbs": [
      "Lectures",
      "About the Course"
    ]
  },
  {
    "objectID": "00-Lectures-Intro.html#slides",
    "href": "00-Lectures-Intro.html#slides",
    "title": "About the Course",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!",
    "crumbs": [
      "Lectures",
      "About the Course"
    ]
  },
  {
    "objectID": "01-Lectures-WhatIs.html",
    "href": "01-Lectures-WhatIs.html",
    "title": "What is Data Science?",
    "section": "",
    "text": "This is the material (slides and notes) of the first lecture on the course.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.",
    "crumbs": [
      "Lectures",
      "What is Data Science?"
    ]
  },
  {
    "objectID": "01-Lectures-WhatIs.html#slides",
    "href": "01-Lectures-WhatIs.html#slides",
    "title": "What is Data Science?",
    "section": "",
    "text": "This is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.",
    "crumbs": [
      "Lectures",
      "What is Data Science?"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About this Course",
    "section": "",
    "text": "These pages contains the material used for the course Introduction to Data Science, part of the curriculum of the Graduate Program in Applied Computing, offeredy by the Brazilian National Institute for Space Research.\nThe course is usually offered in the second term of each year, exclusively on-site. Albeit the material is in English, classes are usually taught in Portuguese.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "About this Course",
    "section": "Syllabus",
    "text": "Syllabus\n    What is Data Science? Why Data Science exists as a discipline?     The role of a Data Scientist. Other roles and tasks in Data Science.     Data Collection and Discovery. Data Provenance and Annotation.      Data Representation: Flat Files, Databases, Tidy Data.     Introduction to Analytics, Exploratory Data Analysis and Machine Learning.     Reproducible Research. Data-based products.      Examples of applications, case studies and project development.\n\nBibliography\nFor a good all-around reference in Data Science see The Art of Data Science: A Guide for Anyone who works with Data (Peng and Matsui 2015) or Doing Data Science: Straight Talk from the Frontline (O’Neil and Schutt 2013).\nFor information on data scientists profiles see Analyzing the Analyzers: An Introspective Survey of Data Scientists and Their Work (Harris, Murphy, and Vaisman 2013) or Data scientists at work (Gutierrez 2014).\n\n\nUnder Construction\n\nThis material is still under construction (during the second term of 2024).",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#Schedule",
    "href": "index.html#Schedule",
    "title": "About this Course",
    "section": "Schedule for 2024",
    "text": "Schedule for 2024\nFor the second term of 2024 our schedule is Fridays, from 1:30PM to 5:30PM.\n\n\n\n\n\n\n\nDay\nTopics\n\n\n\n\nJune 14\nIntroduction to the course. Reading of About the Course and What is Data Science? lectures, rules for evaluation.\n\n\nJune 21\nReading of Skills for Data Science lecture. Q&A about the first homework (if needed).\n\n\nJune 28\nLectures suspended today\n\n\nJuly 5\nTBD (possibly “Data”). Reading of the Supermarket Basket Analysis case. Presentation and comments on the first homework.\n\n\nJuly 12\nTBD.Presentation and comments on the second homework.\n\n\nJuly 19\n\n\n\nJuly 26\nPresentation and comments on the third homework.\n\n\nAugust 2\n\n\n\nAugust 9\nPresentation and comments on the fourth homework.\n\n\nAugust 16\n\n\n\nAugust 23\n\n\n\nAugust 30\nPresentation and comments on the Capstone Project.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#homework",
    "href": "index.html#homework",
    "title": "About this Course",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\n\nAssignment\nDue date (before that day’s lecture!)\n\n\n\n\nHomework #1\nJune 28 July 5\n\n\nHomework #2\nJuly 12\n\n\nTBD\nJuly 26\n\n\nTBD\nAugust 9\n\n\nCapstone Project - TBD\nAugust 30",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-2-Skills.html",
    "href": "Resources/Slides/CAP394-2024-2-Skills.html",
    "title": "CAP-394",
    "section": "",
    "text": "This is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\n\n\n\n\n\n\nBasically we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\n\n\n\n\n\n\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\n\n\n\n\n\n\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\n\n\n\n\n\n\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \n\n\n\n\n\n\nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\n\n\n\n\n\n\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\n\n\n\n\n\n\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\n\n\n\n\n\n\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\n\n\n\n\n\n\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\n\n\n\n\n\n\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\n\n\n\n\n\n\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\n\n\n\n\n\n\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\n\n\n\n\n\n\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\n\n\n\n\n\n\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\n\n\n\n\n\n\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\n\n\n\n\n\n\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\n\n\n\n\n\n\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\n\n\n\n\n\n\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please writecode in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\n\n\n\n\n\n\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\n\n\n\n\n\n\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\n\n\n\n\n\n\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\n\n\n\n\n\n\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\n\n\n\n\n\n\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\n\n\n\n\n\n\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\n\n\n\n\n\n\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\n\n\n\n\n\n\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\n\n\n\n\n\n\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\n\n\n\n\n\n\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\n\n\n\n\n\n\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\n\n\n\n\n\n\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\n\n\n\n\n\n\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\n\n\n\n\n\n\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\n\n\n\n\n\n\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\n\n\n\n\n\n\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\n\n\n\n\n\n\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\n\n\n\n\n\n\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\n\n\n\n\n\n\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\n\n\n\n\n\n\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\n\n\n\n\n\n\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\n\n\n\n\n\n\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\nBasically we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please writecode in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!"
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-2-Skills.html#slides",
    "href": "Resources/Slides/CAP394-2024-2-Skills.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "This is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\n\n\n\n\n\n\nBasically we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\n\n\n\n\n\n\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\n\n\n\n\n\n\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\n\n\n\n\n\n\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \n\n\n\n\n\n\nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\n\n\n\n\n\n\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\n\n\n\n\n\n\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\n\n\n\n\n\n\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\n\n\n\n\n\n\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\n\n\n\n\n\n\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\n\n\n\n\n\n\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\n\n\n\n\n\n\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\n\n\n\n\n\n\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\n\n\n\n\n\n\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\n\n\n\n\n\n\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\n\n\n\n\n\n\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\n\n\n\n\n\n\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\n\n\n\n\n\n\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please writecode in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\n\n\n\n\n\n\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\n\n\n\n\n\n\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\n\n\n\n\n\n\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\n\n\n\n\n\n\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\n\n\n\n\n\n\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\n\n\n\n\n\n\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\n\n\n\n\n\n\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\n\n\n\n\n\n\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\n\n\n\n\n\n\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\n\n\n\n\n\n\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\n\n\n\n\n\n\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\n\n\n\n\n\n\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\n\n\n\n\n\n\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\n\n\n\n\n\n\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\n\n\n\n\n\n\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\n\n\n\n\n\n\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\n\n\n\n\n\n\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\n\n\n\n\n\n\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\n\n\n\n\n\n\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\n\n\n\n\n\n\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\n\n\n\n\n\n\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\n\n\n\n\n\n\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\nBasically we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please writecode in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!"
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-0-AboutCourse.html",
    "href": "Resources/Slides/CAP394-2024-0-AboutCourse.html",
    "title": "CAP-394",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!"
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-0-AboutCourse.html#slides",
    "href": "Resources/Slides/CAP394-2024-0-AboutCourse.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!"
  },
  {
    "objectID": "50-Projects-Intro.html",
    "href": "50-Projects-Intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis section will contain some Data Science Projects that are used in our lectures.\nProjects are somehow complex tasks around a specific dataset that requires data wrangling, organization, EDA, visualization, and in some cases application of machine learning techniques.\nMost projects will include ideas for class discussions and several exercises.\nProjects that are ready to be explored are on the sidebar on the left.",
    "crumbs": [
      "Projects",
      "Introduction"
    ]
  },
  {
    "objectID": "80-Ideas-Intro.html",
    "href": "80-Ideas-Intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis section will contain some Data Science ideas that may be used in our lectures.\nIdeas are similar to projects but weren’t thoroughly explored. They will require more effort on data acquisition, organization, etc. before EDA and analysis can be done.\nIdeas that are ready to be explored are on the sidebar on the left.",
    "crumbs": [
      "Ideas Bank",
      "Introduction"
    ]
  },
  {
    "objectID": "03-Lectures-Data.html",
    "href": "03-Lectures-Data.html",
    "title": "Data",
    "section": "",
    "text": "This is the material (slides and notes) of the third lecture on the course.",
    "crumbs": [
      "Lectures",
      "Data"
    ]
  },
  {
    "objectID": "03-Lectures-Data.html#slides",
    "href": "03-Lectures-Data.html#slides",
    "title": "Data",
    "section": "Slides",
    "text": "Slides\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nHere’s are our main topics.\n\n\n\n\n\n\nJust as an exercise, try to explain data yourself. Avoid self-references!\n\n\n\n\n\n\nA very, very short definition of “data”. It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\n\n\n\n\n\n\nRaw Data is data we collect from the real word and that will be used in our explorations and analyses. There are many different formats for data, therefore many approaches to deal with it.It is important (and somehow obvious) to explain that we mean digital data – data that is stored In files or similar resources in a computer, that can be read by a program or copied to other computers. Data in paper, even if it is structured, is not suited for us unless it is preprocessed (turned into digital data).Even digital data may not be ready for analysis. The figure on the right shows a table (in a PDF document) with all water levels in a monitoring station for 2022, but that would be hard to import to a tabular format.\n\n\n\n\n\n\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The histogram is part of a report for the Engineerings III Evaluation Group.Interesting fact: the data on the table was generated from other data, this is quite common!Tables can be represented in text files, spreadsheets, XML and other formats.\n\n\n\n\n\n\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)\n\n\n\n\n\n\nTime series are data: there is a temporal index that set an explicit order to it. Time series may be stored as tables or spreadsheets or text files.\n\n\n\n\n\n\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\n\n\n\n\n\n\nThis is an entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\n\n\n\n\n\n\nThere are many more variations and combinations of data types. Image time series can be represented as series of images or time series with temporal and spatial indexes. Trajectories are time series with location data associated to each point. Graphs and networks are collection of edges and vertices that can be stored as text files or specialized formats. Some tasks require the mixture of data types: logs are semi-structured text files with an explicit time index, for example.\n\n\n\n\n\n\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf",
    "crumbs": [
      "Lectures",
      "Data"
    ]
  },
  {
    "objectID": "03-Lectures-Data.html#code-for-examples-used-in-this-lecture",
    "href": "03-Lectures-Data.html#code-for-examples-used-in-this-lecture",
    "title": "Data",
    "section": "Code for examples used in this lecture",
    "text": "Code for examples used in this lecture\n\nParliamentary Amendment\nA parliamentary amendment is a proposal made by a member of parliament or a legislator to specify provisions in a bill or legislation that is being considered by the parliament. This term is commonly used in the legislative processes of various countries, including Brazil, often to earmark funds for specific projects.\nWe can download a list of parliamentary amendments as a CSV file from the Federal Government Transparency Open Data Portal (Dados Abertos | Portal da Transparência do Governo Federal).\nThe first 15 lines of that file are shown below:\n\"Código da Emenda\";\"Ano da Emenda\";\"Tipo de Emenda\";\"Código do Autor da Emenda\";\"Nome do Autor da Emenda\";\"Número da emenda\";\"Localidade do gasto\";\"Código Função\";\"Nome Função\";\"Código Subfunção\";\"Nome Subfunção\";\"Valor Empenhado\";\"Valor Liquidado\";\"Valor Pago\";\"Valor Restos A Pagar Inscritos\";\"Valor Restos A Pagar Cancelados\";\"Valor Restos A Pagar Pagos\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"POMBAL - PB\";\"15\";\"Urbanismo\";\"451\";\"infra-estrutura urbana\";\"150000,00\";\"0,00\";\"0,00\";\"0,00\";\"146950,00\";\"3050,00\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"CAIÇARA - RS\";\"10\";\"Saúde\";\"302\";\"Assistência hospitalar e ambulatorial\";\"250000,00\";\"0,00\";\"0,00\";\"0,00\";\"0,00\";\"250000,00\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"PIAUÍ (UF)\";\"10\";\"Saúde\";\"572\";\"Desenvolvimento tecnológico e engenharia\";\"896692,97\";\"0,00\";\"0,00\";\"0,00\";\"0,00\";\"896692,97\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"SANTANA DE PARNAÍBA - SP\";\"15\";\"Urbanismo\";\"451\";\"infra-estrutura urbana\";\"500000,00\";\"0,00\";\"0,00\";\"0,00\";\"493100,00\";\"6900,00\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"PALMITINHO - RS\";\"10\";\"Saúde\";\"302\";\"Assistência hospitalar e ambulatorial\";\"250000,00\";\"0,00\";\"0,00\";\"0,00\";\"0,00\";\"250000,00\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"JANDIRA - SP\";\"10\";\"Saúde\";\"302\";\"Assistência hospitalar e ambulatorial\";\"1199996,15\";\"0,00\";\"0,00\";\"0,00\";\"499996,15\";\"700000,00\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"FUNILÂNDIA - MG\";\"20\";\"Agricultura\";\"608\";\"Promoção da produção agropecuária\";\"398737,97\";\"0,00\";\"0,00\";\"0,00\";\"10000,00\";\"388737,97\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"VARGEM GRANDE PAULISTA - SP\";\"15\";\"Urbanismo\";\"451\";\"infra-estrutura urbana\";\"250000,00\";\"0,00\";\"0,00\";\"0,00\";\"0,00\";\"250000,00\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"RESENDE - RJ\";\"27\";\"Desporto e lazer\";\"812\";\"Desporto comunitário\";\"692500,00\";\"0,00\";\"0,00\";\"0,00\";\"285187,50\";\"407312,50\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"BURITIRAMA - BA\";\"27\";\"Desporto e lazer\";\"812\";\"Desporto comunitário\";\"400000,00\";\"0,00\";\"0,00\";\"0,00\";\"0,00\";\"400000,00\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"JARDIM - MS\";\"10\";\"Saúde\";\"302\";\"Assistência hospitalar e ambulatorial\";\"3380000,00\";\"0,00\";\"0,00\";\"0,00\";\"2600000,00\";\"780000,00\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"LUZ - MG\";\"10\";\"Saúde\";\"302\";\"Assistência hospitalar e ambulatorial\";\"250000,00\";\"0,00\";\"0,00\";\"0,00\";\"0,00\";\"250000,00\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"MATOZINHOS - MG\";\"10\";\"Saúde\";\"301\";\"Atenção básica\";\"199890,00\";\"0,00\";\"0,00\";\"0,00\";\"0,00\";\"199890,00\"\n\"Sem informaç\";\"2014\";\"Emenda Individual - Transferências\";\"S/I\";\"Sem informação\";\"S/I\";\"MOCOCA - SP\";\"10\";\"Saúde\";\"301\";\"Atenção básica\";\"498940,80\";\"0,00\";\"0,00\";\"0,00\";\"0,00\";\"498940,80\"\nWe can notice that the file is a semicolon-separated file. Its encoding is ISO-8859. To read it into a dataframe in Python (Rossum and Jr. 2001) and Pandas (McKinney 2012) we first need to import the libraries:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nNow we can read the file:\n\nfile = 'Resources/Data/Emendas/Emendas.csv'\n# Read the CSV file using the specified separator and encoding.\ndf = pd.read_csv(file, sep=';', encoding='ISO-8859-1')\n# Display the head of the dataframe.\nprint(df.head())\n\n  Código da Emenda  Ano da Emenda                      Tipo de Emenda  \\\n0     Sem informaç           2014  Emenda Individual - Transferências   \n1     Sem informaç           2014  Emenda Individual - Transferências   \n2     Sem informaç           2014  Emenda Individual - Transferências   \n3     Sem informaç           2014  Emenda Individual - Transferências   \n4     Sem informaç           2014  Emenda Individual - Transferências   \n\n  Código do Autor da Emenda Nome do Autor da Emenda Número da emenda  \\\n0                       S/I          Sem informação              S/I   \n1                       S/I          Sem informação              S/I   \n2                       S/I          Sem informação              S/I   \n3                       S/I          Sem informação              S/I   \n4                       S/I          Sem informação              S/I   \n\n        Localidade do gasto Código Função Nome Função Código Subfunção  \\\n0               POMBAL - PB            15   Urbanismo              451   \n1              CAIÇARA - RS            10       Saúde              302   \n2                PIAUÍ (UF)            10       Saúde              572   \n3  SANTANA DE PARNAÍBA - SP            15   Urbanismo              451   \n4           PALMITINHO - RS            10       Saúde              302   \n\n                             Nome Subfunção Valor Empenhado Valor Liquidado  \\\n0                    infra-estrutura urbana       150000,00            0,00   \n1     Assistência hospitalar e ambulatorial       250000,00            0,00   \n2  Desenvolvimento tecnológico e engenharia       896692,97            0,00   \n3                    infra-estrutura urbana       500000,00            0,00   \n4     Assistência hospitalar e ambulatorial       250000,00            0,00   \n\n  Valor Pago Valor Restos A Pagar Inscritos Valor Restos A Pagar Cancelados  \\\n0       0,00                           0,00                       146950,00   \n1       0,00                           0,00                            0,00   \n2       0,00                           0,00                            0,00   \n3       0,00                           0,00                       493100,00   \n4       0,00                           0,00                            0,00   \n\n  Valor Restos A Pagar Pagos  \n0                    3050,00  \n1                  250000,00  \n2                  896692,97  \n3                    6900,00  \n4                  250000,00  \n\n\nWe need to filter out some columns.\nhttps://www1.siop.planejamento.gov.br/mto/doku.php/mto2024\npages 229-233\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nHere’s are our main topics.\nJust as an exercise, try to explain data yourself. Avoid self-references!\nA very, very short definition of “data”. It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\nRaw Data is data we collect from the real word and that will be used in our explorations and analyses. There are many different formats for data, therefore many approaches to deal with it.It is important (and somehow obvious) to explain that we mean digital data – data that is stored In files or similar resources in a computer, that can be read by a program or copied to other computers. Data in paper, even if it is structured, is not suited for us unless it is preprocessed (turned into digital data).Even digital data may not be ready for analysis. The figure on the right shows a table (in a PDF document) with all water levels in a monitoring station for 2022, but that would be hard to import to a tabular format.\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The histogram is part of a report for the Engineerings III Evaluation Group.Interesting fact: the data on the table was generated from other data, this is quite common!Tables can be represented in text files, spreadsheets, XML and other formats.\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)\nTime series are data: there is a temporal index that set an explicit order to it. Time series may be stored as tables or spreadsheets or text files.\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\nThis is an entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\nThere are many more variations and combinations of data types. Image time series can be represented as series of images or time series with temporal and spatial indexes. Trajectories are time series with location data associated to each point. Graphs and networks are collection of edges and vertices that can be stored as text files or specialized formats. Some tasks require the mixture of data types: logs are semi-structured text files with an explicit time index, for example.\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf",
    "crumbs": [
      "Lectures",
      "Data"
    ]
  },
  {
    "objectID": "20-Cookbook-Intro.html",
    "href": "20-Cookbook-Intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis section will contain some examples of common data science tasks in Python.\nTopics covered will eventually include:\n\nReading CSV and XLSX and writing CSV data.\nSlicing and dicing.\nPivoting data.",
    "crumbs": [
      "Cookbook",
      "Introduction"
    ]
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-3-Data.html",
    "href": "Resources/Slides/CAP394-2024-3-Data.html",
    "title": "CAP-394",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nHere’s are our main topics.\n\n\n\n\n\n\nJust as an exercise, try to explain data yourself. Avoid self-references!\n\n\n\n\n\n\nA very, very short definition of “data”. It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\n\n\n\n\n\n\nRaw Data is data we collect from the real word and that will be used in our explorations and analyses. There are many different formats for data, therefore many approaches to deal with it.It is important (and somehow obvious) to explain that we mean digital data – data that is stored In files or similar resources in a computer, that can be read by a program or copied to other computers. Data in paper, even if it is structured, is not suited for us unless it is preprocessed (turned into digital data).Even digital data may not be ready for analysis. The figure on the right shows a table (in a PDF document) with all water levels in a monitoring station for 2022, but that would be hard to import to a tabular format.\n\n\n\n\n\n\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The histogram is part of a report for the Engineerings III Evaluation Group.Interesting fact: the data on the table was generated from other data, this is quite common!Tables can be represented in text files, spreadsheets, XML and other formats.\n\n\n\n\n\n\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)\n\n\n\n\n\n\nTime series are data: there is a temporal index that set an explicit order to it. Time series may be stored as tables or spreadsheets or text files.\n\n\n\n\n\n\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\n\n\n\n\n\n\nThis is an entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\n\n\n\n\n\n\nThere are many more variations and combinations of data types. Image time series can be represented as series of images or time series with temporal and spatial indexes. Trajectories are time series with location data associated to each point. Graphs and networks are collection of edges and vertices that can be stored as text files or specialized formats. Some tasks require the mixture of data types: logs are semi-structured text files with an explicit time index, for example.\n\n\n\n\n\n\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nHere’s are our main topics.\nJust as an exercise, try to explain data yourself. Avoid self-references!\nA very, very short definition of “data”. It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\nRaw Data is data we collect from the real word and that will be used in our explorations and analyses. There are many different formats for data, therefore many approaches to deal with it.It is important (and somehow obvious) to explain that we mean digital data – data that is stored In files or similar resources in a computer, that can be read by a program or copied to other computers. Data in paper, even if it is structured, is not suited for us unless it is preprocessed (turned into digital data).Even digital data may not be ready for analysis. The figure on the right shows a table (in a PDF document) with all water levels in a monitoring station for 2022, but that would be hard to import to a tabular format.\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The histogram is part of a report for the Engineerings III Evaluation Group.Interesting fact: the data on the table was generated from other data, this is quite common!Tables can be represented in text files, spreadsheets, XML and other formats.\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)\nTime series are data: there is a temporal index that set an explicit order to it. Time series may be stored as tables or spreadsheets or text files.\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\nThis is an entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\nThere are many more variations and combinations of data types. Image time series can be represented as series of images or time series with temporal and spatial indexes. Trajectories are time series with location data associated to each point. Graphs and networks are collection of edges and vertices that can be stored as text files or specialized formats. Some tasks require the mixture of data types: logs are semi-structured text files with an explicit time index, for example.\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf"
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-3-Data.html#slides",
    "href": "Resources/Slides/CAP394-2024-3-Data.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nHere’s are our main topics.\n\n\n\n\n\n\nJust as an exercise, try to explain data yourself. Avoid self-references!\n\n\n\n\n\n\nA very, very short definition of “data”. It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\n\n\n\n\n\n\nRaw Data is data we collect from the real word and that will be used in our explorations and analyses. There are many different formats for data, therefore many approaches to deal with it.It is important (and somehow obvious) to explain that we mean digital data – data that is stored In files or similar resources in a computer, that can be read by a program or copied to other computers. Data in paper, even if it is structured, is not suited for us unless it is preprocessed (turned into digital data).Even digital data may not be ready for analysis. The figure on the right shows a table (in a PDF document) with all water levels in a monitoring station for 2022, but that would be hard to import to a tabular format.\n\n\n\n\n\n\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The histogram is part of a report for the Engineerings III Evaluation Group.Interesting fact: the data on the table was generated from other data, this is quite common!Tables can be represented in text files, spreadsheets, XML and other formats.\n\n\n\n\n\n\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)\n\n\n\n\n\n\nTime series are data: there is a temporal index that set an explicit order to it. Time series may be stored as tables or spreadsheets or text files.\n\n\n\n\n\n\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\n\n\n\n\n\n\nThis is an entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\n\n\n\n\n\n\nThere are many more variations and combinations of data types. Image time series can be represented as series of images or time series with temporal and spatial indexes. Trajectories are time series with location data associated to each point. Graphs and networks are collection of edges and vertices that can be stored as text files or specialized formats. Some tasks require the mixture of data types: logs are semi-structured text files with an explicit time index, for example.\n\n\n\n\n\n\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nHere’s are our main topics.\nJust as an exercise, try to explain data yourself. Avoid self-references!\nA very, very short definition of “data”. It uses “information” which can be interpreted as “organized data”.Another definition is “collection of values that convey information”. \u000b\nRaw Data is data we collect from the real word and that will be used in our explorations and analyses. There are many different formats for data, therefore many approaches to deal with it.It is important (and somehow obvious) to explain that we mean digital data – data that is stored In files or similar resources in a computer, that can be read by a program or copied to other computers. Data in paper, even if it is structured, is not suited for us unless it is preprocessed (turned into digital data).Even digital data may not be ready for analysis. The figure on the right shows a table (in a PDF document) with all water levels in a monitoring station for 2022, but that would be hard to import to a tabular format.\nBoth figures show data – in a very organized way. The table is the list of graduate courses approved by CAPES, the figure is a histogram plot based on the data (with some small differences due to the time the data was collected). The histogram is part of a report for the Engineerings III Evaluation Group.Interesting fact: the data on the table was generated from other data, this is quite common!Tables can be represented in text files, spreadsheets, XML and other formats.\nImages are data that are spatially organized, and for which we expect some coherence (e.g visible patterns).These are Synthetic-aperture radar images. Images on the first column show waves; on the second column we have some biological slicks (algae?) and the images on the third column show icebergs. Images are from a dataset associated with the paper “A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel‐1 wave model”.There are many ways to represent image data in a file format, and many formats that allow the specific representation of specialized data (e.g. color images, 2D signals, multispectral images…)\nTime series are data: there is a temporal index that set an explicit order to it. Time series may be stored as tables or spreadsheets or text files.\nA structured text data. XML is a format that allows representation of structured and semi-structured data using tags and hierarchies. JSON is a similar format. These formats can be read (with some difficulty…) by humans and computers. They are usually stored in text files.\nThis is an entry from a log of SQL queries submitted to an astronomy database. It is textual data, semistructured, that could be converted into tabular data with some tricks. We may see some of those later.Logs are also usually stored in text files.MAKIYAMA, V. H. Text mining applied to SQL queries: a case study for SDSS SkyServer. 2015. 75 p. IBI: &lt;8JMKD3MGP3W34P/3K6JNQ8&gt;. (sid.inpe.br/mtc-m21b/2015/08.31.17.43-TDI). Dissertação (Mestrado em Computação Aplicada) - Instituto Nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2015. Disponível em: http://urlib.net/ibi/8JMKD3MGP3W34P/3K6JNQ8.\nThere are many more variations and combinations of data types. Image time series can be represented as series of images or time series with temporal and spatial indexes. Trajectories are time series with location data associated to each point. Graphs and networks are collection of edges and vertices that can be stored as text files or specialized formats. Some tasks require the mixture of data types: logs are semi-structured text files with an explicit time index, for example.\nTidy Data is the best possible representation of data for exploration and analysis. It allow us to keep an organized representation of the data. This is not always possible, as we’ll see.The best reference for Tidy Data is “Tidy Data”, Hadley Wickham, https://vita.had.co.nz/papers/tidy-data.pdf"
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-1-WhatIs.html",
    "href": "Resources/Slides/CAP394-2024-1-WhatIs.html",
    "title": "CAP-394",
    "section": "",
    "text": "This is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data."
  },
  {
    "objectID": "Resources/Slides/CAP394-2024-1-WhatIs.html#slides",
    "href": "Resources/Slides/CAP394-2024-1-WhatIs.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "This is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data."
  },
  {
    "objectID": "30-Visualization-Intro.html",
    "href": "30-Visualization-Intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis section will contain some visualization examples.",
    "crumbs": [
      "Visualization",
      "Introduction"
    ]
  },
  {
    "objectID": "02-Lectures-Skills.html",
    "href": "02-Lectures-Skills.html",
    "title": "Skills for Data Science",
    "section": "",
    "text": "This is the material (slides and notes) of the second lecture on the course.",
    "crumbs": [
      "Lectures",
      "Skills for Data Science"
    ]
  },
  {
    "objectID": "02-Lectures-Skills.html#slides",
    "href": "02-Lectures-Skills.html#slides",
    "title": "Skills for Data Science",
    "section": "Slides",
    "text": "Slides\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\n\n\n\n\n\n\nBasically we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\n\n\n\n\n\n\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\n\n\n\n\n\n\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\n\n\n\n\n\n\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \n\n\n\n\n\n\nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\n\n\n\n\n\n\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\n\n\n\n\n\n\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\n\n\n\n\n\n\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\n\n\n\n\n\n\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\n\n\n\n\n\n\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\n\n\n\n\n\n\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\n\n\n\n\n\n\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\n\n\n\n\n\n\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\n\n\n\n\n\n\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\n\n\n\n\n\n\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\n\n\n\n\n\n\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\n\n\n\n\n\n\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\n\n\n\n\n\n\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please writecode in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\n\n\n\n\n\n\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\n\n\n\n\n\n\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\n\n\n\n\n\n\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\n\n\n\n\n\n\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\n\n\n\n\n\n\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\n\n\n\n\n\n\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\n\n\n\n\n\n\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\n\n\n\n\n\n\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\n\n\n\n\n\n\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\n\n\n\n\n\n\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\n\n\n\n\n\n\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\n\n\n\n\n\n\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\n\n\n\n\n\n\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\n\n\n\n\n\n\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\n\n\n\n\n\n\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\n\n\n\n\n\n\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\n\n\n\n\n\n\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\n\n\n\n\n\n\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\n\n\n\n\n\n\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\n\n\n\n\n\n\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\n\n\n\n\n\n\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\n\n\n\n\n\n\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!",
    "crumbs": [
      "Lectures",
      "Skills for Data Science"
    ]
  },
  {
    "objectID": "02-Lectures-Skills.html#code-for-examples-used-in-this-lecture",
    "href": "02-Lectures-Skills.html#code-for-examples-used-in-this-lecture",
    "title": "Skills for Data Science",
    "section": "Code for examples used in this lecture",
    "text": "Code for examples used in this lecture\n\nDiagram for Time Spent/Enjoyable Activities\nThe pie charts in the slides used in this lecture were copied from a dead link (will be redirected to another site), but since we have the percentages it would be fairly simple to reproduce it.\nFirst we import the (Johnson et al. 2018) library:\n\nimport plotly.graph_objects as go\n\nLet’s create the data structures in Python:\n\ntasks = ['Collecting Data', 'Cleaning and Organizing', 'Building Training Sets',\n         'Mining Data', 'Refining Algorithms', 'Other']\nperTime = [19, 60, 3, 9, 4, 5]\nperUnenjoy = [21, 57, 10, 3, 4, 5]\n\nI’d like to use a pastel color scheme:\n\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0', '#ffb3e6']\n\nLet’s create the chart for “most spent part” and set some layour options:\n\nfig_time = go.Figure(data=[go.Pie(\n    labels=tasks,\n    values=perTime,\n    hole=0.4,\n    marker=dict(colors=colors),\n    textinfo='percent',\n    textposition='outside',\n    hoverinfo='label+percent',\n    sort=False, # Disable sorting to keep the order consistent\n    textfont=dict(family=\"Arial Black\",size=18,color=\"#808080\") \n)])\n\nfig_time.update_layout(\n    title='What data scientists spend the most time doing',\n    showlegend=True,\n    legend=dict(title=\"Tasks\", x=1.15, y=0.5, \n                traceorder=\"normal\", font=dict(size=12))\n)\n\n                                                \n\n\nDo more or less the same for the “lest enjoyable part” chart:\n\nfig_unenjoy = go.Figure(data=[go.Pie(\n    labels=tasks,\n    values=perUnenjoy,\n    hole=0.4,\n    marker=dict(colors=colors),\n    textinfo='percent',\n    textposition='outside',\n    hoverinfo='label+percent',\n    sort=False,  # Disable sorting to keep the order consistent\n    textfont=dict(family=\"Arial Black\",size=18,color=\"#808080\") \n)])\n\nfig_unenjoy.update_layout(\n    title=\"What's the least enjoyable part of data science?\",\n    showlegend=True,\n    legend=dict(title=\"Tasks\", x=1.15, y=0.5, \n                traceorder=\"normal\", font=dict(size=12))\n)\n\n                                                \n\n\n\n\nAverage Temperature Basic Example\nIn this notebook we will do a very simple data science project: plot the Earth’s average land temperature and see if it is getting higher. The original data came from the Berkeley Earth site, an independent U.S. non-profit organization focused on environmental data science. The original data file can be downloaded here or here (a local copy). It is a text file with annual and five-year average temperatures and respective uncertainties.\nThe first 30 lines of that file are shown below:\n% This file contains a brief summary of the land-surface average results \n% produced by the Berkeley Averaging method.  Temperatures are in \n% Celsius and reported as anomalies relative to the Jan 1951-Dec 1980\n% average. Uncertainties represent the 95% confidence interval for \n% statistical and spatial undersampling effects.\n% \n% The current dataset presented here is described as: \n% \n%   Estimated Global Land-Surface TAVG based on the Complete Berkeley Dataset\n% \n% \n% This analysis was run on 07-Feb-2022 15:32:40\n% \n% Results are based on 50590 time series \n%   with 20945177 data points\n% \n% Estimated Jan 1951-Dec 1980 absolute temperature (C): 8.60 +/- 0.06\n% \n% \n% \n% Year, Annual Anomaly, Annual Unc., Five-year Anomaly, Five-year Unc.\n \n  1750      -1.220          NaN              NaN             NaN\n  1751      -1.311          NaN              NaN             NaN\n  1753      -0.955        1.005              NaN             NaN\n  1754      -0.379        0.934              NaN             NaN\n  1755      -0.698        0.980           -0.553           0.608\n  1756      -0.421        1.596           -0.831           0.586\n  1757      -0.310        0.896           -1.024           0.612\n  1758      -2.345        1.366           -1.347           0.882\nLet’s use Python (Rossum and Jr. 2001), Pandas (McKinney 2012) and Matplotlib (Hunter 2007) for the analysis scripts, which will read the data and plot the year versus the corrected annual average temperature, and after that let’s see if we can see a trend using a basic linear model.\nFirst we import the libraries we’re going to use.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nNow we can read the file into a data frame – more on this later in the course. If you want to reproduce this make sure you have the file in the right path. Note that we have to explicitely name the columns for this data set.\n\ninputfile = \"Resources/Data/AverageTemperature/Complete_TAVG_summary.txt\"\ndata = pd.read_csv(inputfile,sep=\"\\\\s+\",skiprows=22,header=None)\ndata.columns = [\"Year\",\"Annual.Anomaly\",\"Annual.Uncertainty\",\n                \"FiveYear.Anomaly\",\"FiveYear.Uncertainty\"]\n\nLet’s see how the data looks (as a data frame):\n\n data \n\n\n\n\n\n\n\n\n\nYear\nAnnual.Anomaly\nAnnual.Uncertainty\nFiveYear.Anomaly\nFiveYear.Uncertainty\n\n\n\n\n0\n1750\n-1.220\nNaN\nNaN\nNaN\n\n\n1\n1751\n-1.311\nNaN\nNaN\nNaN\n\n\n2\n1753\n-0.955\n1.005\nNaN\nNaN\n\n\n3\n1754\n-0.379\n0.934\nNaN\nNaN\n\n\n4\n1755\n-0.698\n0.980\n-0.553\n0.608\n\n\n...\n...\n...\n...\n...\n...\n\n\n266\n2017\n1.306\n0.035\n1.290\n0.037\n\n\n267\n2018\n1.145\n0.052\n1.345\n0.036\n\n\n268\n2019\n1.345\n0.048\n1.307\n0.035\n\n\n269\n2020\n1.499\n0.036\nNaN\nNaN\n\n\n270\n2021\n1.240\n0.038\nNaN\nNaN\n\n\n\n\n271 rows × 5 columns\n\n\n\n\nWe want to plot the temperature, but we have the anomaly. All we need to do is to add 8.65ºC to the anomaly (see a detailed explanation at this NASA’s Earth Observatory site or NOAA’s National Centers for Environmental Information).\n\ndata[\"Annual\"] = data[\"Annual.Anomaly\"]+8.65\n\nNow let’s plot the annual temperature against the year:\n\nax = data.plot(kind=\"line\",x=\"Year\",y=\"Annual\")\nplt.show()\n\n\n\n\n\n\n\n\nLet’s fit a linear regression model that will basically describe temperature a function of the year using the data we have. Don’t worry about the math details for the time being.\n\nmodel = LinearRegression()\nX = pd.DataFrame(data[\"Year\"])\nY = pd.DataFrame(data[\"Annual\"])\nmodel.fit(X,Y)\nY_pred = model.predict(X)\n\nY_pred contains the predicted temperature values from the years. We can now create another plot that shows the original data and the linear model as a straigth line in red.\n\nplt.plot(data[\"Year\"],data[\"Annual\"])\nplt.plot(data[\"Year\"],Y_pred,color=\"red\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\nBasically we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please writecode in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!",
    "crumbs": [
      "Lectures",
      "Skills for Data Science"
    ]
  },
  {
    "objectID": "00-Homework-01.html",
    "href": "00-Homework-01.html",
    "title": "Homework #1",
    "section": "",
    "text": "How to deliver\n\nPlease deliver the answer to this homework in a single PDF file. Use figures, but try to keep it under three pages.\n\n\n\nThink about your expectatives about this course. Which one(s) of the descriptions in What is a data scientist? 14 definitions of a data scientist! applies to you? Which one(s) doesn’t?\nA local PDF of that site is stored here.\n\n\n\nFind a source of open data on the web. Try to get a source of data that you can relate to. Write one or two paragraphs about what it contains, who publishes it, what it can used for.\nGet at least one of the files from that source. How easy/hard was it? How well-documented is it? Does it looks tidy?\n\n\nHint\n\nIf you cannot find good open data sources and if you can read Portuguese you can always check up some government data sources, e.g. Portal Brasileiro de Dados Abertos, Portal da Transparência - Controladoria Geral da União, Governo Aberto SP, Portal de Dados Abertos do Ministério da Educação, Catálogo de Dados Abertos do Senado Federal, ipeadata Dados Abertos da Agência Nacional de Águas e Saneamento Básico.\n\n\n\n\nCheck out Google Colab, Kaggle, SciServer or other online notebook environments, play with them a little and select one (or more!) to write notebooks for this class. Alternatively, if you prefer, you can run your code locally. This tutorial on Sharing Notebooks may help."
  },
  {
    "objectID": "00-Homework-01.html#identify-yourself",
    "href": "00-Homework-01.html#identify-yourself",
    "title": "Homework #1",
    "section": "",
    "text": "Think about your expectatives about this course. Which one(s) of the descriptions in What is a data scientist? 14 definitions of a data scientist! applies to you? Which one(s) doesn’t?\nA local PDF of that site is stored here."
  },
  {
    "objectID": "00-Homework-01.html#find-open-data.",
    "href": "00-Homework-01.html#find-open-data.",
    "title": "Homework #1",
    "section": "",
    "text": "Find a source of open data on the web. Try to get a source of data that you can relate to. Write one or two paragraphs about what it contains, who publishes it, what it can used for.\nGet at least one of the files from that source. How easy/hard was it? How well-documented is it? Does it looks tidy?\n\n\nHint\n\nIf you cannot find good open data sources and if you can read Portuguese you can always check up some government data sources, e.g. Portal Brasileiro de Dados Abertos, Portal da Transparência - Controladoria Geral da União, Governo Aberto SP, Portal de Dados Abertos do Ministério da Educação, Catálogo de Dados Abertos do Senado Federal, ipeadata Dados Abertos da Agência Nacional de Águas e Saneamento Básico."
  },
  {
    "objectID": "00-Homework-01.html#choose-an-environment.",
    "href": "00-Homework-01.html#choose-an-environment.",
    "title": "Homework #1",
    "section": "",
    "text": "Check out Google Colab, Kaggle, SciServer or other online notebook environments, play with them a little and select one (or more!) to write notebooks for this class. Alternatively, if you prefer, you can run your code locally. This tutorial on Sharing Notebooks may help."
  },
  {
    "objectID": "53-Projects-Supermarket.html",
    "href": "53-Projects-Supermarket.html",
    "title": "Supermarket Basket Analysis",
    "section": "",
    "text": "This chapter presents an extensive (but incomplete!) example of a data science approach to analysis of a supermarket transaction database. The data was obtained from an anonymous Brazilian supermarket and preprocessed to make some examples easier to follow and modify.\nThis chapter also serves as an example on how an exploratory report could be organized.",
    "crumbs": [
      "Projects",
      "Supermarket Basket Analysis"
    ]
  },
  {
    "objectID": "53-Projects-Supermarket.html#checking-and-adjusting-the-dataframe",
    "href": "53-Projects-Supermarket.html#checking-and-adjusting-the-dataframe",
    "title": "Supermarket Basket Analysis",
    "section": "Checking and adjusting the dataframe",
    "text": "Checking and adjusting the dataframe\nHow many records and fields did we just read? The shape of the dataframe can be used to answer this, showing the number of rows and columns.\n\nsupermDF.shape\n\n(363966, 16)\n\n\nCSV is a text-based format, when reading it into a dataframe Pandas guesses which is the best data type for the columns. Let’s check the data types for our dataframe:\n\nprint(supermDF.dtypes)\n\nFileID            object\nStoreID            int64\nPOSID              int64\nTransactionID     object\nDate               int64\nitemOrder          int64\nDescription       object\nUnitPrice        float64\nQuantity         float64\nAmount           float64\nUnit              object\nPaymentMethod     object\nTotalDue         float64\nTotalPaid        float64\nChange           float64\nStatus            object\ndtype: object\n\n\nThe field Date was read as an int64 value, let’s convert it to a real date/time field.\n\n# Convert the field Date to a string, which is used by pd.to_datetime with a specific\n# format to be converted into an array of datetime object, which is stored in our\n# dataframe using a new column name.\nsupermDF['DateTime'] = pd.to_datetime(supermDF['Date'].astype(str),format='%Y%m%d%H%M%S')",
    "crumbs": [
      "Projects",
      "Supermarket Basket Analysis"
    ]
  },
  {
    "objectID": "53-Projects-Supermarket.html#how-many-different-products-does-the-supermarket-sells",
    "href": "53-Projects-Supermarket.html#how-many-different-products-does-the-supermarket-sells",
    "title": "Supermarket Basket Analysis",
    "section": "How many different products does the supermarket sells?",
    "text": "How many different products does the supermarket sells?\nIn order to answer this question we would need access to different data: a complete catalogue of the products the supermarket sells. But we can get the answer to how many different products the supermarket sold by looking at the Description field and counting unique values. Here’s how:\n\n# Get the column Description, create a series with the unique values on it, \n# return the length of this series. \nlen(pd.unique(supermDF['Description']))\n\n4791\n\n\nThis one line of code slices the dataframe, selecting only the column/field Description and passes it as an argument to a the function unique of the Pandas library that eliminates all repeated items and returns a list, which then is used as an argument to len that counts the elements on that list.\nAnother way to get the number of different products sold by the supermarket is to get a count of occurrences of each item, which can be easily done with a call to value_counts function of a slice of a dataframe:\n\n# Show a summary of the counts for all values of the column Description.\nsupermDF['Description'].value_counts()\n\nDescription\nPAO FRANCES               19648\nACUCAR REFINADO A. ALE     5067\nOLEO SOJA SOYA 900ML P     4800\nQUEIJO MUSSARELA           4694\nFLV CEBOLA                 4342\n                          ...  \nCERA POLIFLOR LIQ. INC        1\nALCACHOFRA C\\10               1\nESP. HIKARI PAPRICA DO        1\nDOCE KISABOR P.PE MOLE        1\nBRINQ. FESTLAR 10UN AN        1\nName: count, Length: 4791, dtype: int64\n\n\nWe can check which are the top ten sellers (by quantity) with:\n\n# Show the top ten items of the counts for all values of the column Description.\nsupermDF['Description'].value_counts().head(10)\n\nDescription\nPAO FRANCES               19648\nACUCAR REFINADO A. ALE     5067\nOLEO SOJA SOYA 900ML P     4800\nQUEIJO MUSSARELA           4694\nFLV CEBOLA                 4342\nFLV TOMATE CARMEM          4178\nBEB. REF.COCA COLA 2L      3303\nFLV BANANA PRATA           3252\nFLV ALHO                   3196\nFLV LARANJA POCAN          3162\nName: count, dtype: int64\n\n\nAnd even filter by quantity using a range:\n\n# Store the counts in a series.\ncounts = supermDF['Description'].value_counts()\n# Filter those between two values.\ncounts[counts.between(1000,1200)]\n\nDescription\nAGUA SANIT. FAZ.ESPERA    1190\nLEITE COND. MOCA 395G     1153\nLEITE PAST. SERRAMAR S    1149\nGELATINA DABARRA 85 GR    1139\nFLV BATATA BOLINHA        1138\nBACON REZENDE KG          1138\nBOV.PALETA                1137\nREFRESCO DA BARRA 35G     1127\nFLV BATATA LAVADA         1122\nMARGARINA QUALY 500G C    1113\nBOV. CONTRA FILE          1100\nBOV.MUSCULO               1075\nFLV PIMENTAO VERDE        1048\nSAL ITA 1KG               1042\nMORTADELA MARBA           1039\nFLV TAKAGAKI ALFACE LI    1014\nLEITE COND. ITAMBE 395    1010\nName: count, dtype: int64\n\n\n\n\n \n\nHow many different payment methods are there in our data? Which is the most popular?\n\n\n\n \n\nHow many transactions failed? How many succeeded?",
    "crumbs": [
      "Projects",
      "Supermarket Basket Analysis"
    ]
  },
  {
    "objectID": "53-Projects-Supermarket.html#whats-the-average-shopping-cart-cost",
    "href": "53-Projects-Supermarket.html#whats-the-average-shopping-cart-cost",
    "title": "Supermarket Basket Analysis",
    "section": "What’s the average shopping cart cost?",
    "text": "What’s the average shopping cart cost?\nIn order to get this information we would need to scan all itens, adding the values for each transaction and calculate the average of these values. But the way that we chose to store the itens makes the first step easier: since all item entries on the dataframe contains information on the transaction it belongs to, we can get all itens where the field itemOrder is one (the first item on that transaction) and use the field TotalDue to get the transaction’s value.\nFirst we create a subset of our dataframe containing only the first item on the transaction:\n\n# Create a new dataframe with only the first item in each transaction. \nonlyFirstItems = supermDF.loc[supermDF['itemOrder'] == 1].copy()\n\nNow we can get the average transaction value with:\n\n# Prints the mean of all transactions' TotalDue.\nonlyFirstItems['TotalDue'].mean()\n\n36.371526707300696\n\n\nWe can even have some fun with subsets of the dataframe:\n\n# Get the mean of all transactions' TotalDue.\nts1 = onlyFirstItems.loc[onlyFirstItems['StoreID'] == 1,'TotalDue'].mean()\nts2 = onlyFirstItems.loc[onlyFirstItems['StoreID'] == 2,'TotalDue'].mean()\nts3 = onlyFirstItems.loc[onlyFirstItems['StoreID'] == 3,'TotalDue'].mean()\nts4 = onlyFirstItems.loc[onlyFirstItems['StoreID'] == 4,'TotalDue'].mean()\n# Format and print these values.\nprint(f\"Mean Total Due by Store: 1:{ts1:.2f}, 2:{ts2:.2f}, 3:{ts3:.2f}, 4:{ts4:.2f}.\")\n\nMean Total Due by Store: 1:36.91, 2:20.69, 3:50.56, 4:nan.\n\n\nAs expected there isn’t an average value for sales in the store number 4, since it does not exist:\n\n# Print a series with all unique values for StoreID.\npd.unique(supermDF['StoreID'])\n\narray([3, 2, 1])\n\n\n\n\n \n\nThe code onlyFirstItems.loc[onlyFirstItems['StoreID'] == '0002'] returns an empty dataframe. Verify and explain the reason.\n\nWe can also plot a simple histogram showing the distribution of the total values for the baskets/transactions. The key command is plt.hist that creates a histogram using the values of the TotalDue field:\n\n# Create the plot.\nfig = px.histogram(onlyFirstItems, x='TotalDue', nbins=20, \n          title='Histogram of total amount due for each transaction')\n# Update layout for labels and grid.\nfig.update_layout(\n    xaxis_title='Total Due',\n    yaxis_title='Frequency',\n    showlegend=False\n)\nfig.update_xaxes(showgrid=True)\nfig.update_yaxes(showgrid=True)\n# Display it.\nfig.show()\n\n                                                \n\n\nThis can’t be right – almost all transactions’ values are clumped together below 25.000 reais and the X axis indicates the existence of a transaction of around half a million reais. This is not consistent with what is expected of sales of a small supermarket.\nLet’s print the top ten transactions by value:\n\n# Create a list of columns we want to show.\nfields = ['TransactionID','TotalDue','TotalPaid','Change','Status']\n# Print the top 10 largest TotalDue values with the fields we chose.\nonlyFirstItems.nlargest(10,'TotalDue')[fields]\n\n\n\n\n\n\n\n\n\nTransactionID\nTotalDue\nTotalPaid\nChange\nStatus\n\n\n\n\n92896\n121443_04084497\n505050.0\n5.78\n505044.22\nT\n\n\n267595\n200441_04090425\n101100.0\n1.74\n101098.26\nT\n\n\n159154\n182523_04052926\n101050.0\n6.46\n101043.54\nT\n\n\n141321\n164130_04052857\n101010.1\n2.05\n101008.05\nT\n\n\n24458\n191520_03008081\n100100.0\n96.46\n100003.54\nT\n\n\n69744\n173627_02005291\n100100.0\n82.81\n100017.19\nT\n\n\n107012\n100425_03010508\n100100.0\n79.60\n100020.40\nT\n\n\n250981\n114813_02028185\n31060.0\n7.56\n31052.44\nT\n\n\n40004\n090409_04082972\n21010.0\n1.90\n21008.10\nT\n\n\n171630\n105814_04053199\n19000.0\n182.69\n18817.31\nT\n\n\n\n\n\n\n\n\nWe can see that there are several transactions with large total paid values and changes – we don’t have an explanation for that, but it is clear that we need to consider the TotalPaid field for our analysis. This is a lesson into not jumping on the analysis before better understanding the data, and on how basic EDA can show us some issues with the data.\n\n\n \n\nList basic information on all transactions above R$ 1.000,00.\n\n\n\n \n\nCreate a metric (boolean or numeric) that could be used to indicate a suspicious transaction (with large amounts for TotalDue and Change, for example.)\n\n\n\n \n\nIs there a temporal pattern on those transactions? Do they occur more in some days or hours of day?\n\n\n\n \n\nAre these transactions specific to one store or occur in all stores of the supermarket?\n\n\n\n \n\nAre all those transactions paid for with cash?\n\nLet’s recreate the histogram using TotalPaid but with more bins and a custom X axis this time. First we need to take the log of the TotalPaid field:\n\nepsilon = 0.0001\nonlyFirstItems['LogTotalPaid'] = np.log10(onlyFirstItems['TotalPaid']+epsilon)\n\nNow we can see how’s the distribution of TotalPaid in a log scale:\n\n# We will use custom tick values and labels.\ntickvals = np.log10([0.05,0.10,0.25,0.5,1,2,5,10,20,50,100,200,500,1000])\nticktext = ['0.05','0.10','0.25','0.5','1','2','5','10','20','50','100','200','500','1000']\n# Create the histogram.\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=onlyFirstItems['LogTotalPaid'],\n    nbinsx=50,\n    hoverinfo='skip' # Very hard to make a working hover info!\n))\n# Update layout.\nfig.update_layout(\n    title='Histogram of total amount paid in each transaction',\n    xaxis_title='Total Paid',\n    yaxis_title='Frequency',\n     xaxis=dict(\n        tickmode='array',\n        tickvals=tickvals,\n        ticktext=ticktext,\n        type='linear'  \n    ),\n    yaxis=dict(\n        type='linear',\n        tickformat=',d'\n    ),\n    showlegend=False\n)\n# Add grids.\nfig.update_xaxes(showgrid=True)\nfig.update_yaxes(showgrid=True)\n# Display it.\nfig.show()\n\n                                                \n\n\nThis is better, although we can see several transactions under one Real.\n\n\n \n\nWhat itens are for sale that cost under one Real? How many transactions are under one Real, how many under 20 cents of a Real?\n\nWe can also see that there is one transaction with value above 800 reais. Let’s take a closer look at this transaction.\n\n# Select only some fields for displaying.\nfields = ['TransactionID','Description','UnitPrice','Quantity','Amount']\n# Filter the dataframe locating only rows where TotalPaid is &gt;= 800, then sort the\n# filtered dataframe by Amount (from larger to smaller) then show the fields we chose.\nsupermDF.loc[supermDF['TotalPaid'] &gt;= 800].sort_values('Amount',ascending=False)[fields]\n\n\n\n\n\n\n\n\n\nTransactionID\nDescription\nUnitPrice\nQuantity\nAmount\n\n\n\n\n118981\n132950_03062294\nLEITE LV MILENIO 1L\n1.39\n24.00\n33.36\n\n\n118995\n132950_03062294\nBOTIJAO GAZ 13kg\n27.90\n1.00\n27.90\n\n\n118993\n132950_03062294\nBOTIJAO GAZ 13kg\n27.90\n1.00\n27.90\n\n\n119045\n132950_03062294\nCAFE PILAO 500G\n4.98\n4.00\n19.92\n\n\n119090\n132950_03062294\nBEB. CERV.SKOL LT 473M\n1.65\n12.00\n19.80\n\n\n...\n...\n...\n...\n...\n...\n\n\n118947\n132950_03062294\nERVILHA QUERO 200G\n0.69\n1.00\n0.69\n\n\n119008\n132950_03062294\nERVILHA QUERO 200G\n0.69\n1.00\n0.69\n\n\n118939\n132950_03062294\nFLV CENOURA\n1.19\n0.53\n0.63\n\n\n119025\n132950_03062294\nFLV PIMENTAO VERDE\n1.89\n0.27\n0.51\n\n\n118934\n132950_03062294\nFLV LIMAO TAITI\n0.45\n0.82\n0.37\n\n\n\n\n212 rows × 5 columns\n\n\n\n\nIt seems a large purchase, 212 itens in a transaction, but can still be considered normal.\n\n\n \n\nWe chose supermDF['TotalPaid'] &gt;= 800 as a filter, knowing from the histogram that there is only one transaction that costed more than 800 reais. But what if there was more than one transaction? How do I identify the transaction IDs of purchases above a certain value, and display them separately?",
    "crumbs": [
      "Projects",
      "Supermarket Basket Analysis"
    ]
  },
  {
    "objectID": "53-Projects-Supermarket.html#are-there-patterns-on-the-amounts-paid-by-the-costumers",
    "href": "53-Projects-Supermarket.html#are-there-patterns-on-the-amounts-paid-by-the-costumers",
    "title": "Supermarket Basket Analysis",
    "section": "Are there patterns on the amounts paid by the costumers?",
    "text": "Are there patterns on the amounts paid by the costumers?\nHow many items our costumers buy in each transaction? Are there patterns or distributions on this amount? Let’s explore!\nFirst let’s see a histogram of the distribution of the number of items per transaction. In the way our data is organized, we need to infer this number since it is not explicit. To count the number of items per transaction we can group the dataframe by TransactionID and get the number of records in each group:\n\n# Group records by TransactionID, select TransactionID to extract a metric (size).\nitemsIT = supermDF.groupby('TransactionID')['TransactionID'].transform('size')\n# Add it as a new column.\nsupermDF['itemsInTransaction'] = itemsIT\n# We want only one record per transaction!\nonlyFirstItems = supermDF.loc[supermDF['itemOrder'] == 1].copy()\n\nThen we can create a histogram to show the distribution of items per transaction:\n\n# Create histogram.\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=onlyFirstItems['itemsInTransaction'],\n    nbinsx=50\n))\n# Update layout.\nfig.update_layout(\n    title='Histogram of number of items in basket',\n    xaxis_title='Number of Items',\n    yaxis_title='Frequency',\n    yaxis=dict(\n        type='log',\n        tickformat='g'\n    ),\n    showlegend=False\n)\nfig.update_xaxes(showgrid=True)\nfig.update_yaxes(showgrid=True)\n# Display it.\nfig.show()\n\n                                                \n\n\nAs expected, most of the transactions contain a few items.\n\n\n \n\nTo count the number of items per transaction we could also get the maximum value for each itemOrder with the dataframe grouped by TransactionID. Try it!\n\nWe expect a sort of correlation between the number of items in a transaction and its total cost (TotalPaid). Let’s do a quick XY (or scatter) plot to verify this:\n\n# Create scatter plot using Plotly\nfig = px.scatter(\n    onlyFirstItems,\n    x='itemsInTransaction',\n    y='TotalPaid',\n    title='Scatter Plot of Number of Items in Cart versus Total Paid',\n    labels={\n        'itemsInTransaction': 'Number of Items in Cart',\n        'TotalPaid': 'Total Paid'\n    }\n)\n# Show the plot\nfig.show()\n\n                                                \n\n\nLet’s enhance the plot with a linear regression line. We need first to fit a model using scikit-learn (Pedregosa et al. (2011)):\n\n# Fit linear regression model.\nX = np.array(onlyFirstItems['itemsInTransaction']).reshape(-1, 1)\ny = np.array(onlyFirstItems['TotalPaid'])\nmodel = LinearRegression()\nmodel.fit(X, y)\ny_pred = model.predict(X)\n# Create the figure.\nfig = go.Figure()\n# Add scatter plot using Plotly Graph Objects.\nfig.add_trace(go.Scatter(\n    x=onlyFirstItems['itemsInTransaction'],\n    y=onlyFirstItems['TotalPaid'],\n    mode='markers',\n    name='Data Points',\n    showlegend=False,\n    marker=dict(opacity=0.6)  # Adjust marker opacity\n))\n# Add linear regression line using Plotly Graph Objects.\nfig.add_trace(go.Scatter(\n    x=onlyFirstItems['itemsInTransaction'],\n    y=y_pred,\n    mode='lines',\n    name='Linear Regression',\n    line=dict(color='red'),\n    showlegend=False,\n))\n# Update the figure.\nfig.update_layout(\n    title='Number of Items in Basket x Total Paid',\n    xaxis_title='Number of Items',\n    yaxis_title='Total Paid',\n    showlegend=False\n)\n# Show the plot.\nfig.show()\n\n                                                \n\n\nThere are some outliers but the total paid for each transaction seems correlated with the number of items in each cart.\n\n\n \n\nThe regression line may be heavily influenced by the transactions with only a few items or with a low paymend due. Redo the plot but using only transactions with total above a cutoff, or with transactions with more than a specific number of items, or both.\n\nAre those transactions paid by cash? Can we see any pattern or outlier on the payment method? Let’s use different colors to help discriminate the payment methods:\n\n# Create the scatter plot with Plotly Express.\nfig = px.scatter(\n    onlyFirstItems,\n    x=\"itemsInTransaction\",\n    y=\"TotalPaid\",\n    color=\"PaymentMethod\",\n    title='Scatter Plot of Number of Items in Cart versus Total Paid',\n    labels={\n        'itemsInTransaction': 'Number of Items in Cart',\n        'TotalPaid': 'Total Paid'\n    }\n)\n# Show the plot.\nfig.show()\n\n                                                \n\n\n\n\n \n\nThere is a payment category without a title. See how many transactions are in this situation and propose a way to deal with this data.\n\nThat wasn’t very useful, since there are many options of payments, some that are somehow similar, so we could aggregate them. Let’s create a new column on our dataframe to represent a simplified payment method.\n\n# A simple function that simply the categories of payment method.\ndef simplifyPayment(paymethod):\n    if paymethod.startswith(\"Dinheiro\"):\n        return \"Dinheiro\"\n    elif paymethod.startswith(\"Cartao\"):\n        return \"Cartao\"\n    elif paymethod.startswith(\"Cheque\"):\n        return \"Cheque\"\n    else:\n        return \"Outros\"\n# Now we can add a new column based on the value of PaymentMethod.\nonlyFirstItems['SimplifiedPaymentMethod'] = \\\n    onlyFirstItems['PaymentMethod'].apply(simplifyPayment)      \n\nWith this new column we can plot a more informative scatter plot:\n\n# Define a custom color palette.\ncustom_color_palette = {\n    'Dinheiro': 'yellowgreen',\n    'Cartao'  : 'slateblue',\n    'Cheque'  : 'magenta',\n    'Outros'  : 'lightgrey'  \n}\n# Create the scatter plot with Plotly Express.\nfig = px.scatter(\n    onlyFirstItems,\n    x=\"itemsInTransaction\",\n    y=\"TotalPaid\",\n    color=\"SimplifiedPaymentMethod\",\n    title='Scatter Plot of Number of Items in Cart versus Total Paid',\n    labels={\n        'itemsInTransaction': 'Number of Items in Cart',\n        'TotalPaid': 'Total Paid'\n    },\n    color_discrete_map=custom_color_palette\n)\n# Show the plot.\nfig.show()\n\n                                                \n\n\n\n\nClass Discussion\n\nWhat are the other payment method mappings that may reveal hidden information on this data?\n\n\n\n \n\nSome transactions’ values were about 500 reais and paid by cash – take a closer look to see what is being sold.\n\n\n\n \n\nCreate plots to explore the relationship between itemsInTransaction and TotalPaid but using other fields to set the color – e.g. are the payment patterns the same for each of the three stores?\n\n\n\n \n\nAre the payment patterns the same for different hours of the day? For different days on the week?",
    "crumbs": [
      "Projects",
      "Supermarket Basket Analysis"
    ]
  },
  {
    "objectID": "53-Projects-Supermarket.html#are-there-temporal-patterns-on-the-sales-data",
    "href": "53-Projects-Supermarket.html#are-there-temporal-patterns-on-the-sales-data",
    "title": "Supermarket Basket Analysis",
    "section": "Are there temporal patterns on the sales data?",
    "text": "Are there temporal patterns on the sales data?\nWe can use EDA to investigate several different aspects of the data. Let’s consider the sale of a particular item, the most sold, \"PAO FRANCES           \". Is there a temporal pattern on its sale? We can answer that with a two-dimensional histogram that shows the amount of units sold by day and time.\nFirst let’s create a subset of the data containing only the item in question. From this subset let’s also keep only the relevant fields. Let’s also create columns for day of the month and time of the day:\n\n# Get only items that are equal to \"PAO FRANCES           \"\nonlyBreadRolls = supermDF.loc[supermDF['Description'] == \"PAO FRANCES           \"]\n# Select only some of the columns.\nonlyBreadRolls = onlyBreadRolls[['StoreID','DateTime','Quantity']]\n# Extract the day of month from the DateTime column, store in new column.\nonlyBreadRolls['DayOfMonth'] = onlyBreadRolls['DateTime'].dt.day\n# Extract the hour of day from the DateTime column, store in new column.\nonlyBreadRolls['HourOfDay'] = onlyBreadRolls['DateTime'].dt.hour\n\nWith this dataframe we can create the two-dimensional histogram, which is done by counting all values for each combination of the fields DayOfMonth and HourOfDay (see Sum column based on another column in Pandas DataFrame).\n\n# Sum the amounts for each combination of DayOfMonth and HourOfDay.\nhistoFB = onlyBreadRolls.groupby([\"DayOfMonth\",\"HourOfDay\"]).Quantity.sum().reset_index() \n\nThis two-dimensional histogram contains an entry for the sum of the field Quantity each combination of the fields DayOfMonth and HourOfDay, but if there weren’t specific combinations of DayOfMonth and HourOfDay the entry will not be created – the result of grouby does not return a matrix or dataframe with all possible combinations, only with the existing ones.\n\n\n \n\nPrint the contents of histoFB to understand better what was created.\n\nThe results can be used to create a heat map (a good visual representation for a two-dimensional histogram) but empty cells will not be displayed. To create a better visualization we can pad the two-dimensional so all possible combinations of DayOfMonth and HourOfDay with the non-occurring combinations filled with zeros (See Creating a rectangular Heatmap from two columns of a Pandas dataframe, Pandas: How to Replace NaN Values in Pivot Table with Zeros, pandas.DataFrame.reindex).\n\n# Reorders the two-dimensional histogram as a matrix-like dataframe.\nhmap = histoFB.pivot(index=\"HourOfDay\", columns=\"DayOfMonth\", values=\"Quantity\").fillna(0)\n# Use those indexes for the row and column indexes.\ndays = list(range(1, 32))\nhours = list(range(24))\n# Apply those indexes to the datafrme, filling nonexistent (na) values with zeros.\nhmap = hmap.reindex(index=hours).reindex(columns=days).fillna(0)\n\nNow that we have our well-formed two-dimensional dataframe we can plot it as a heat map.\n\n# Create the heatmap.\nfig = go.Figure(data=go.Heatmap(\n    z=hmap.values,\n    x=hmap.columns,\n    y=hmap.index,\n    colorscale='Jet',\n    text=hmap.values,\n    texttemplate=\"%{text:.0f}\",  # Format text to display as integers\n    textfont={\"size\":10},\n    hoverinfo='z',  # Display value on hover\n    showscale=True\n))\n# Update layout to add thin lines around each cell.\n# This was suggested by Code Copilot!\nfig.update_traces(\n    zmin=0, zmax=hmap.values.max(),  \n    xgap=1,  \n    ygap=1,  \n    colorbar=dict(tickfont=dict(size=10))  \n)\n# Customize the layout.\nfig.update_layout(\n    title='Heatmap of Bread Rolls Sold',\n    xaxis_title='Day of Month',\n    yaxis_title='Hour of Day',\n    xaxis=dict(\n        tickmode='linear',\n        dtick=1,  \n        tickfont=dict(size=10),\n        showgrid=False,\n        zeroline=False # Remove grid lines\n    ),\n    yaxis=dict(\n        tickmode='linear',\n        dtick=1, \n        tickfont=dict(size=10),\n        showgrid=False,\n        zeroline=False # Remove grid lines\n    ),\n    plot_bgcolor='#808080', \n)\n# Invert the Y-axis.\nfig.update_yaxes(autorange='reversed')\n# Show the plot.\nfig.show()\n\n                                                \n\n\n\n\n \n\nVisualization is a very important component of EDA. The heat map summarized the sales of bread rolls over time, pointing to several interesting features. List some of those, with possible explanations. Consider that a feature could be the beginning of another exploration path to learn more about the data! Some of these possible paths will be used in the exercises below.\n\n\n\n \n\nWe used the color map ‘Jet’ which gives good-looking results but have some issues (colors for values under 350 are somehow similar). Try different color maps to see if others enhance some aspects of the data (see Built-in Continuous Color Scales in Python or Continuous Color Scales and Color Bars in Python).\n\n\n\n \n\nThe heat map shows the number of bread rolls sold by the whole supermarket (all stores). Recreate the heat maps for the different stores, one heat map per store. Do they appear to behave in a similar way?\n\n\n\n \n\nCreate new versions of the heat map separating bread rolls that were paid by cash, credit card and other payment methods. See first which payment methods are more common.\n\n\n\n \n\nWe noticed that there were some days and hours where a lot of bread rolls were sold. Do a quick analysis on how many buyers bought bread rolls at that time and day, and how many bread rolls each costumer bought.\n\n\n\n \n\nDo similar heat maps for three or four other items that are frequently sold. Choose any item that grabbed your attention! Analyze the heat maps to see if there are any patterns or oddities.",
    "crumbs": [
      "Projects",
      "Supermarket Basket Analysis"
    ]
  }
]