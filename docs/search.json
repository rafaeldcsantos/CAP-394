[
  {
    "objectID": "00-Lectures-Intro.html",
    "href": "00-Lectures-Intro.html",
    "title": "About the Course",
    "section": "",
    "text": "This is the material (slides and notes) of the introduction to the course.\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!",
    "crumbs": [
      "Lectures",
      "About the Course"
    ]
  },
  {
    "objectID": "00-Lectures-Intro.html#slides",
    "href": "00-Lectures-Intro.html#slides",
    "title": "About the Course",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!",
    "crumbs": [
      "Lectures",
      "About the Course"
    ]
  },
  {
    "objectID": "01-Lectures-WhatIs.html",
    "href": "01-Lectures-WhatIs.html",
    "title": "What is Data Science?",
    "section": "",
    "text": "This is the material (slides and notes) of the first lecture on the course.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.",
    "crumbs": [
      "Lectures",
      "What is Data Science?"
    ]
  },
  {
    "objectID": "01-Lectures-WhatIs.html#slides",
    "href": "01-Lectures-WhatIs.html#slides",
    "title": "What is Data Science?",
    "section": "",
    "text": "This is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.",
    "crumbs": [
      "Lectures",
      "What is Data Science?"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About this Course",
    "section": "",
    "text": "These pages contains the material used for the course Introduction to Data Science, part of the curriculum of the Graduate Program in Applied Computing, offeredy by the Brazilian National Institute for Space Research.\nThe course is usually offered in the second term of each year, exclusively asynchronously. Albeit the material is in English, classes are usually taught in Portuguese.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "About this Course",
    "section": "Syllabus",
    "text": "Syllabus\n    What is Data Science? Why Data Science exists as a discipline?     The role of a Data Scientist. Other roles and tasks in Data Science.     Data Collection and Discovery. Data Provenance and Annotation.      Data Representation: Flat Files, Databases, Tidy Data.     Introduction to Analytics, Exploratory Data Analysis and Machine Learning.     Reproducible Research. Data-based products.      Examples of applications, case studies and project development.\n\nBibliography\nFor a good all-around reference in Data Science see The Art of Data Science: A Guide for Anyone who works with Data (Peng and Matsui 2015) or Doing Data Science: Straight Talk from the Frontline (O’Neil and Schutt 2013).\nFor information on data scientists profiles see Analyzing the Analyzers: An Introspective Survey of Data Scientists and Their Work (Harris, Murphy, and Vaisman 2013) or Data scientists at work (Gutierrez 2014).\n\n\nUnder Construction\n\nThis material is continually being revised and improved.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#Schedule",
    "href": "index.html#Schedule",
    "title": "About this Course",
    "section": "Schedule for 2025",
    "text": "Schedule for 2025\nFor the second term of 2025 our schedule is Fridays, from 1:30PM to 5:30PM.\n\n\n\n\n\n\n\nDay\nTopics\n\n\n\n\nJune 20\nLectures suspended today\n\n\nJune 27\nIntroduction to the course. Reading of About the Course and What is Data Science? lectures, rules for evaluation.\n\n\nJuly 4\nReading of Skills for Data Science lecture. Q&A about the first homework (if needed).\n\n\nJuly 11\n\n\n\nJuly 18\n\n\n\nJuly 25\n\n\n\nAugust 1\n\n\n\nAugust 8\n\n\n\nAugust 15\n\n\n\nAugust 22\n\n\n\nAugust 27\n\n\n\nSeptember 5",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#homework",
    "href": "index.html#homework",
    "title": "About this Course",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\n\nAssignment\nDue date (before that day’s lecture!)\n\n\n\n\nHomework #1\nJuly 4\n\n\nTBD\nAugust 9\n\n\nCapstone Project\nSeptember 5",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "Resources/Slides/CAP394-2025-2-Skills.html",
    "href": "Resources/Slides/CAP394-2025-2-Skills.html",
    "title": "CAP-394",
    "section": "",
    "text": "This is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\n\n\n\n\n\n\nBasically, we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\n\n\n\n\n\n\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\n\n\n\n\n\n\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\n\n\n\n\n\n\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \n\n\n\n\n\n\nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\n\n\n\n\n\n\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\n\n\n\n\n\n\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\n\n\n\n\n\n\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\n\n\n\n\n\n\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\n\n\n\n\n\n\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\n\n\n\n\n\n\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\n\n\n\n\n\n\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\n\n\n\n\n\n\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\n\n\n\n\n\n\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\n\n\n\n\n\n\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\n\n\n\n\n\n\nOpinions on the use of LLMs (e.g., ChatGPT) for coding are divided. Some argue that relying on an LLM to generate code constitutes a form of cheating, as the author may not fully understand the solution. Others view LLMs as tools that can support and enhance the coding process. In the following slides, we will explore a historical perspective on writing code and offer some guidelines on the ethical use of LLMs in coding and data science.\n\n\n\n\n\n\nBefore the Internet, programmers learned primarily from coding books filled with examples. A common practice was to type out these examples, observe how they worked, and then adapt them to suit specific needs. Over time, programmers became capable of writing code from scratch, but they still frequently referred to books for details on commands and syntax.\n\n\n\n\n\n\nProgramming languages became more complex, and books covering their basics—along with catalogs of APIs and their functions—grew larger and more detailed. Eventually, much of this content became available online or downloadable, making it easier for programmers to consult as needed.\n\n\n\n\n\n\nOver time, a more structured approach emerged for finding coding references and examples—starting with general search engines and later shifting toward specialized websites.“Copying and Pasting from Stack Overflow” isn’t a real book, of course, but looking up code snippets there (or using ChatGPT) isn’t inherently wrong—as long as you verify the answers and make an effort to understand how and why they work.\n\n\n\n\n\n\nThere’s nothing inherently wrong with using LLMs to assist with coding. It can be seen as a natural evolution of earlier practices—such as copying code from books or the Internet and adapting it. However, in all cases, we should avoid blindly accepting what these sources provide, whether it’s a book, a website, or an AI model. Always test the code, run it, and adjust the details. This approach is far more effective when working with small code snippets than with large, complex applications.\n\n\n\n\n\n\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\n\n\n\n\n\n\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\n\n\n\n\n\n\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please write code in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\n\n\n\n\n\n\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically, we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\n\n\n\n\n\n\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\n\n\n\n\n\n\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\n\n\n\n\n\n\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\n\n\n\n\n\n\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\n\n\n\n\n\n\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\n\n\n\n\n\n\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\n\n\n\n\n\n\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\n\n\n\n\n\n\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\n\n\n\n\n\n\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\n\n\n\n\n\n\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\n\n\n\n\n\n\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\n\n\n\n\n\n\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\n\n\n\n\n\n\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\n\n\n\n\n\n\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\n\n\n\n\n\n\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\n\n\n\n\n\n\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\n\n\n\n\n\n\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\n\n\n\n\n\n\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\n\n\n\n\n\n\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\n\n\n\n\n\n\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\n\n\n\n\n\n\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.\n\n\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\nBasically, we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\nOpinions on the use of LLMs (e.g., ChatGPT) for coding are divided. Some argue that relying on an LLM to generate code constitutes a form of cheating, as the author may not fully understand the solution. Others view LLMs as tools that can support and enhance the coding process. In the following slides, we will explore a historical perspective on writing code and offer some guidelines on the ethical use of LLMs in coding and data science.\nBefore the Internet, programmers learned primarily from coding books filled with examples. A common practice was to type out these examples, observe how they worked, and then adapt them to suit specific needs. Over time, programmers became capable of writing code from scratch, but they still frequently referred to books for details on commands and syntax.\nProgramming languages became more complex, and books covering their basics—along with catalogs of APIs and their functions—grew larger and more detailed. Eventually, much of this content became available online or downloadable, making it easier for programmers to consult as needed.\nOver time, a more structured approach emerged for finding coding references and examples—starting with general search engines and later shifting toward specialized websites.“Copying and Pasting from Stack Overflow” isn’t a real book, of course, but looking up code snippets there (or using ChatGPT) isn’t inherently wrong—as long as you verify the answers and make an effort to understand how and why they work.\nThere’s nothing inherently wrong with using LLMs to assist with coding. It can be seen as a natural evolution of earlier practices—such as copying code from books or the Internet and adapting it. However, in all cases, we should avoid blindly accepting what these sources provide, whether it’s a book, a website, or an AI model. Always test the code, run it, and adjust the details. This approach is far more effective when working with small code snippets than with large, complex applications.\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please write code in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically, we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow."
  },
  {
    "objectID": "Resources/Slides/CAP394-2025-2-Skills.html#slides",
    "href": "Resources/Slides/CAP394-2025-2-Skills.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "This is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\n\n\n\n\n\n\nBasically, we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\n\n\n\n\n\n\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\n\n\n\n\n\n\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\n\n\n\n\n\n\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \n\n\n\n\n\n\nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\n\n\n\n\n\n\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\n\n\n\n\n\n\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\n\n\n\n\n\n\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\n\n\n\n\n\n\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\n\n\n\n\n\n\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\n\n\n\n\n\n\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\n\n\n\n\n\n\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\n\n\n\n\n\n\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\n\n\n\n\n\n\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\n\n\n\n\n\n\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\n\n\n\n\n\n\nOpinions on the use of LLMs (e.g., ChatGPT) for coding are divided. Some argue that relying on an LLM to generate code constitutes a form of cheating, as the author may not fully understand the solution. Others view LLMs as tools that can support and enhance the coding process. In the following slides, we will explore a historical perspective on writing code and offer some guidelines on the ethical use of LLMs in coding and data science.\n\n\n\n\n\n\nBefore the Internet, programmers learned primarily from coding books filled with examples. A common practice was to type out these examples, observe how they worked, and then adapt them to suit specific needs. Over time, programmers became capable of writing code from scratch, but they still frequently referred to books for details on commands and syntax.\n\n\n\n\n\n\nProgramming languages became more complex, and books covering their basics—along with catalogs of APIs and their functions—grew larger and more detailed. Eventually, much of this content became available online or downloadable, making it easier for programmers to consult as needed.\n\n\n\n\n\n\nOver time, a more structured approach emerged for finding coding references and examples—starting with general search engines and later shifting toward specialized websites.“Copying and Pasting from Stack Overflow” isn’t a real book, of course, but looking up code snippets there (or using ChatGPT) isn’t inherently wrong—as long as you verify the answers and make an effort to understand how and why they work.\n\n\n\n\n\n\nThere’s nothing inherently wrong with using LLMs to assist with coding. It can be seen as a natural evolution of earlier practices—such as copying code from books or the Internet and adapting it. However, in all cases, we should avoid blindly accepting what these sources provide, whether it’s a book, a website, or an AI model. Always test the code, run it, and adjust the details. This approach is far more effective when working with small code snippets than with large, complex applications.\n\n\n\n\n\n\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\n\n\n\n\n\n\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\n\n\n\n\n\n\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please write code in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\n\n\n\n\n\n\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically, we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\n\n\n\n\n\n\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\n\n\n\n\n\n\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\n\n\n\n\n\n\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\n\n\n\n\n\n\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\n\n\n\n\n\n\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\n\n\n\n\n\n\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\n\n\n\n\n\n\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\n\n\n\n\n\n\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\n\n\n\n\n\n\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\n\n\n\n\n\n\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\n\n\n\n\n\n\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\n\n\n\n\n\n\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\n\n\n\n\n\n\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\n\n\n\n\n\n\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\n\n\n\n\n\n\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\n\n\n\n\n\n\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\n\n\n\n\n\n\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\n\n\n\n\n\n\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\n\n\n\n\n\n\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\n\n\n\n\n\n\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\n\n\n\n\n\n\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.\n\n\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\nBasically, we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\nOpinions on the use of LLMs (e.g., ChatGPT) for coding are divided. Some argue that relying on an LLM to generate code constitutes a form of cheating, as the author may not fully understand the solution. Others view LLMs as tools that can support and enhance the coding process. In the following slides, we will explore a historical perspective on writing code and offer some guidelines on the ethical use of LLMs in coding and data science.\nBefore the Internet, programmers learned primarily from coding books filled with examples. A common practice was to type out these examples, observe how they worked, and then adapt them to suit specific needs. Over time, programmers became capable of writing code from scratch, but they still frequently referred to books for details on commands and syntax.\nProgramming languages became more complex, and books covering their basics—along with catalogs of APIs and their functions—grew larger and more detailed. Eventually, much of this content became available online or downloadable, making it easier for programmers to consult as needed.\nOver time, a more structured approach emerged for finding coding references and examples—starting with general search engines and later shifting toward specialized websites.“Copying and Pasting from Stack Overflow” isn’t a real book, of course, but looking up code snippets there (or using ChatGPT) isn’t inherently wrong—as long as you verify the answers and make an effort to understand how and why they work.\nThere’s nothing inherently wrong with using LLMs to assist with coding. It can be seen as a natural evolution of earlier practices—such as copying code from books or the Internet and adapting it. However, in all cases, we should avoid blindly accepting what these sources provide, whether it’s a book, a website, or an AI model. Always test the code, run it, and adjust the details. This approach is far more effective when working with small code snippets than with large, complex applications.\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please write code in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically, we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow."
  },
  {
    "objectID": "Resources/Slides/CAP394-2025-0-AboutCourse.html",
    "href": "Resources/Slides/CAP394-2025-0-AboutCourse.html",
    "title": "CAP-394",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!"
  },
  {
    "objectID": "Resources/Slides/CAP394-2025-0-AboutCourse.html#slides",
    "href": "Resources/Slides/CAP394-2025-0-AboutCourse.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "Today we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\n\n\n\n\n\n\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\n\n\n\n\n\n\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\n\n\n\n\n\n\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\n\n\n\n\n\n\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!\n\n\n\n\n\n\nToday we’ll talk about INPE, the Applied Computing Program, and the Introduction to Data Science (CAP-394) course.\nThe Introduction to Data Science is not part of any specialty track of the Graduate Program in Applied Computing but is related to several other courses in the program. It is not a prerequisite to any other course, and it does not have any prerequisites.\nHow we’ll divide the course time into lectures, plenty of homework and some small projects.\nBasically how we will manage the course. Lectures can be online depending on the schedule and on where I will be on the lecture’s day, but may also be in-person – do not consider this course as being fully online!Important: we will do some projects together to undestand the issues in a data science project but the students will also need to propose and execute a capstone project!\nI’m online (e-mail) basically every day, almost all the time, but don’t have too much free time for meetings. Plan ahead!"
  },
  {
    "objectID": "Resources/Slides/CAP394-2025-1-WhatIs.html",
    "href": "Resources/Slides/CAP394-2025-1-WhatIs.html",
    "title": "CAP-394",
    "section": "",
    "text": "This is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data."
  },
  {
    "objectID": "Resources/Slides/CAP394-2025-1-WhatIs.html#slides",
    "href": "Resources/Slides/CAP394-2025-1-WhatIs.html#slides",
    "title": "CAP-394",
    "section": "",
    "text": "This is our second lecture. We will loosely define what is data science and the job of the data scientists.\n\n\n\n\n\n\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\n\n\n\n\n\n\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\n\n\n\n\n\n\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\n\n\n\n\n\n\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\n\n\n\n\n\n\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\n\n\n\n\n\n\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\n\n\n\n\n\n\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\n\n\n\n\n\n\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\n\n\n\n\n\n\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\n\n\n\n\n\n\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\n\n\n\n\n\n\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\n\n\n\n\n\n\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\n\n\n\n\n\n\nSome topics for discussion.\n\n\n\n\n\n\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\n\n\n\n\n\n\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\n\n\n\n\n\n\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data.\n\n\n\n\n\n\nThis is our second lecture. We will loosely define what is data science and the job of the data scientists.\nBasically, we will get some interesting definitions, which are not completely clear – but we’ll see why!\nThere is a lot of hype around data science. We will see that while some concepts are new the fundamentals are based on science/technology that already existed before.\nThere is a lot of hype around data science, and we need to tone it down so we can really understand why it is useful. Here are the links to the sites:\nA quick look at the www.glassdoor.com.br site with job offers and industry averages. Take this with a grain of salt…\nSome random ads and calls for proposals/grants I’ve received in the last 5 years. Data Science academic jobs and opportunities are real!\nIt is quite easy to learn Data Science – there are many training offers from short online courses to degree-granting courses, from US$ 50 dollars a year to US$ 5000 dollars for three semestes, from blogs and social media posts to known universities and providers… https://www.datascienceprograms.org/ mentions that “We have researched over 1,000 universities with a data science degree at all levels. In our database, there ~990 different data science or analytics programs.”Which one should I choose? Depends on your constraints and self-learning abilities!\nThe original article is no longer available, as it happens with many news stories beyond paywalls, but the basic idea is the same: data science is built upon concepts, science and technology decades old, but the data deluge and new, cheaper technologies allowed mass usage and simplified the development of applications.\nThese four recent advances let us understand better the hype about data science. More (available) data and better/easier/cheaper tools allows more people to analyze the data. Advantages on learning from the data allows more people (particularly non-CS majors) to play with the data and discover new information.\nPlease see the source at http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram for a detailed description. I like to point at the interdisciplinary aspect of data science. You cannot just get the data and hack it with code or math without knowing what the data represents in the real world.\nSee https://www.crayondata.com/what-is-a-data-scientist-14-definitions-of-a-data-scientist/ for the complete list. It is interesting because most definitions cover some aspects of being a data scientist, with some pointing that we already are data scientists!\nBack to the definition: this is a very good definition for an academic data scientist, i.e. someone who many not have the economic constraints associated with companies (and in most cases not have the funding they have!)I’ll say this several times, but this book is highly recommended (considering this courses’ roots and objectives).\nOK, we’re getting closer to understand what is data science (right?). We can infer that is a data scientist. We know that are plenty of ways to study data science. In this course we will learn some data science by doing. We will play with some problems that may not be scientific but can be easily understood so we can ask questions about the data, formulate what-if scenarios and identify faults on our own approaches. This learn-by-doing can then be applied to your own project.\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.“Copying and Pasting from Stack Overflow” is not a real book, of course, but I feel that searching for answers for code snippets there (or using ChatGPT) is not wrong per se, as long as the reader verify the answers and try to understand what is going on!\nSome topics for discussion.\nSee the post at https://datasciencetut.com/is-data-science-a-dying-profession/ – altough ML and LLMs and other ML tools may help in some tasks,\nHere’s a result of uploading 2000 shopping-cart transactions to ChatGPT’s Data Analyst. After a few interactions and while providing Python code, Data Analyst was able to find something curious about this data (we’ll see it in detail during the course).\nData Analyst choked several times in a 64Mb CSV file. It was able to process the data and give some insights – including complaining about not having more data."
  },
  {
    "objectID": "02-Lectures-Skills.html",
    "href": "02-Lectures-Skills.html",
    "title": "Skills for Data Science",
    "section": "",
    "text": "This is the material (slides and notes) of the second lecture on the course.",
    "crumbs": [
      "Lectures",
      "Skills for Data Science"
    ]
  },
  {
    "objectID": "02-Lectures-Skills.html#slides",
    "href": "02-Lectures-Skills.html#slides",
    "title": "Skills for Data Science",
    "section": "Slides",
    "text": "Slides\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\n\n\n\n\n\n\nBasically, we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\n\n\n\n\n\n\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\n\n\n\n\n\n\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \n\n\n\n\n\n\nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\n\n\n\n\n\n\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\n\n\n\n\n\n\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \n\n\n\n\n\n\nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\n\n\n\n\n\n\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\n\n\n\n\n\n\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\n\n\n\n\n\n\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\n\n\n\n\n\n\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\n\n\n\n\n\n\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\n\n\n\n\n\n\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\n\n\n\n\n\n\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\n\n\n\n\n\n\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\n\n\n\n\n\n\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\n\n\n\n\n\n\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\n\n\n\n\n\n\nOpinions on the use of LLMs (e.g., ChatGPT) for coding are divided. Some argue that relying on an LLM to generate code constitutes a form of cheating, as the author may not fully understand the solution. Others view LLMs as tools that can support and enhance the coding process. In the following slides, we will explore a historical perspective on writing code and offer some guidelines on the ethical use of LLMs in coding and data science.\n\n\n\n\n\n\nBefore the Internet, programmers learned primarily from coding books filled with examples. A common practice was to type out these examples, observe how they worked, and then adapt them to suit specific needs. Over time, programmers became capable of writing code from scratch, but they still frequently referred to books for details on commands and syntax.\n\n\n\n\n\n\nProgramming languages became more complex, and books covering their basics—along with catalogs of APIs and their functions—grew larger and more detailed. Eventually, much of this content became available online or downloadable, making it easier for programmers to consult as needed.\n\n\n\n\n\n\nOver time, a more structured approach emerged for finding coding references and examples—starting with general search engines and later shifting toward specialized websites.“Copying and Pasting from Stack Overflow” isn’t a real book, of course, but looking up code snippets there (or using ChatGPT) isn’t inherently wrong—as long as you verify the answers and make an effort to understand how and why they work.\n\n\n\n\n\n\nThere’s nothing inherently wrong with using LLMs to assist with coding. It can be seen as a natural evolution of earlier practices—such as copying code from books or the Internet and adapting it. However, in all cases, we should avoid blindly accepting what these sources provide, whether it’s a book, a website, or an AI model. Always test the code, run it, and adjust the details. This approach is far more effective when working with small code snippets than with large, complex applications.\n\n\n\n\n\n\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\n\n\n\n\n\n\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\n\n\n\n\n\n\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please write code in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\n\n\n\n\n\n\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically, we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\n\n\n\n\n\n\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\n\n\n\n\n\n\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\n\n\n\n\n\n\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\n\n\n\n\n\n\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\n\n\n\n\n\n\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\n\n\n\n\n\n\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\n\n\n\n\n\n\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\n\n\n\n\n\n\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\n\n\n\n\n\n\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\n\n\n\n\n\n\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\n\n\n\n\n\n\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\n\n\n\n\n\n\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\n\n\n\n\n\n\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\n\n\n\n\n\n\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\n\n\n\n\n\n\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\n\n\n\n\n\n\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\n\n\n\n\n\n\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\n\n\n\n\n\n\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\n\n\n\n\n\n\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\n\n\n\n\n\n\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\n\n\n\n\n\n\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\n\n\n\n\n\n\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\n\n\n\n\n\n\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.",
    "crumbs": [
      "Lectures",
      "Skills for Data Science"
    ]
  },
  {
    "objectID": "02-Lectures-Skills.html#code-for-examples-used-in-this-lecture",
    "href": "02-Lectures-Skills.html#code-for-examples-used-in-this-lecture",
    "title": "Skills for Data Science",
    "section": "Code for examples used in this lecture",
    "text": "Code for examples used in this lecture\n\nDiagram for Time Spent/Enjoyable Activities\nThe pie charts in the slides used in this lecture were copied from a dead link (will be redirected to another site), but since we have the percentages it would be fairly simple to reproduce it.\nFirst we import the (Johnson et al. 2018) library:\n\nimport plotly.graph_objects as go\n\nLet’s create the data structures in Python:\n\ntasks = ['Collecting Data', 'Cleaning and Organizing', 'Building Training Sets',\n         'Mining Data', 'Refining Algorithms', 'Other']\nperTime = [19, 60, 3, 9, 4, 5]\nperUnenjoy = [21, 57, 10, 3, 4, 5]\n\nI’d like to use a pastel color scheme:\n\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0', '#ffb3e6']\n\nLet’s create the chart for “most spent part” and set some layour options:\n\nfig_time = go.Figure(data=[go.Pie(\n    labels=tasks,\n    values=perTime,\n    hole=0.4,\n    marker=dict(colors=colors),\n    textinfo='percent',\n    textposition='outside',\n    hoverinfo='label+percent',\n    sort=False, # Disable sorting to keep the order consistent\n    textfont=dict(family=\"Arial Black\",size=18,color=\"#808080\") \n)])\n\nfig_time.update_layout(\n    title='What data scientists spend the most time doing',\n    showlegend=True,\n    legend=dict(title=\"Tasks\", x=1.15, y=0.5, \n                traceorder=\"normal\", font=dict(size=12))\n)\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nDo more or less the same for the “lest enjoyable part” chart:\n\nfig_unenjoy = go.Figure(data=[go.Pie(\n    labels=tasks,\n    values=perUnenjoy,\n    hole=0.4,\n    marker=dict(colors=colors),\n    textinfo='percent',\n    textposition='outside',\n    hoverinfo='label+percent',\n    sort=False,  # Disable sorting to keep the order consistent\n    textfont=dict(family=\"Arial Black\",size=18,color=\"#808080\") \n)])\n\nfig_unenjoy.update_layout(\n    title=\"What's the least enjoyable part of data science?\",\n    showlegend=True,\n    legend=dict(title=\"Tasks\", x=1.15, y=0.5, \n                traceorder=\"normal\", font=dict(size=12))\n)\n\n                            \n                                            \n\n\n\n\nAverage Temperature Basic Example\nIn this notebook we will do a very simple data science project: plot the Earth’s average land temperature and see if it is getting higher. The original data came from the Berkeley Earth site, an independent U.S. non-profit organization focused on environmental data science. The original data file can be downloaded here or here (a local copy). It is a text file with annual and five-year average temperatures and respective uncertainties.\nThe first 30 lines of that file are shown below:\n% This file contains a brief summary of the land-surface average results \n% produced by the Berkeley Averaging method.  Temperatures are in \n% Celsius and reported as anomalies relative to the Jan 1951-Dec 1980\n% average. Uncertainties represent the 95% confidence interval for \n% statistical and spatial undersampling effects.\n% \n% The current dataset presented here is described as: \n% \n%   Estimated Global Land-Surface TAVG based on the Complete Berkeley Dataset\n% \n% \n% This analysis was run on 07-Feb-2022 15:32:40\n% \n% Results are based on 50590 time series \n%   with 20945177 data points\n% \n% Estimated Jan 1951-Dec 1980 absolute temperature (C): 8.60 +/- 0.06\n% \n% \n% \n% Year, Annual Anomaly, Annual Unc., Five-year Anomaly, Five-year Unc.\n \n  1750      -1.220          NaN              NaN             NaN\n  1751      -1.311          NaN              NaN             NaN\n  1753      -0.955        1.005              NaN             NaN\n  1754      -0.379        0.934              NaN             NaN\n  1755      -0.698        0.980           -0.553           0.608\n  1756      -0.421        1.596           -0.831           0.586\n  1757      -0.310        0.896           -1.024           0.612\n  1758      -2.345        1.366           -1.347           0.882\nLet’s use Python (Rossum and Jr. 2001), Pandas (McKinney 2012) and Matplotlib (Hunter 2007) for the analysis scripts, which will read the data and plot the year versus the corrected annual average temperature, and after that let’s see if we can see a trend using a basic linear model.\nFirst we import the libraries we’re going to use.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nNow we can read the file into a data frame – more on this later in the course. If you want to reproduce this make sure you have the file in the right path. Note that we have to explicitely name the columns for this data set.\n\ninputfile = \"Resources/Data/AverageTemperature/Complete_TAVG_summary.txt\"\ndata = pd.read_csv(inputfile,sep=\"\\\\s+\",skiprows=22,header=None)\ndata.columns = [\"Year\",\"Annual.Anomaly\",\"Annual.Uncertainty\",\n                \"FiveYear.Anomaly\",\"FiveYear.Uncertainty\"]\n\nLet’s see how the data looks (as a data frame):\n\n data \n\n\n\n\n\n\n\n\n\nYear\nAnnual.Anomaly\nAnnual.Uncertainty\nFiveYear.Anomaly\nFiveYear.Uncertainty\n\n\n\n\n0\n1750\n-1.220\nNaN\nNaN\nNaN\n\n\n1\n1751\n-1.311\nNaN\nNaN\nNaN\n\n\n2\n1753\n-0.955\n1.005\nNaN\nNaN\n\n\n3\n1754\n-0.379\n0.934\nNaN\nNaN\n\n\n4\n1755\n-0.698\n0.980\n-0.553\n0.608\n\n\n...\n...\n...\n...\n...\n...\n\n\n266\n2017\n1.306\n0.035\n1.290\n0.037\n\n\n267\n2018\n1.145\n0.052\n1.345\n0.036\n\n\n268\n2019\n1.345\n0.048\n1.307\n0.035\n\n\n269\n2020\n1.499\n0.036\nNaN\nNaN\n\n\n270\n2021\n1.240\n0.038\nNaN\nNaN\n\n\n\n\n271 rows × 5 columns\n\n\n\n\nWe want to plot the temperature, but we have the anomaly. All we need to do is to add 8.65ºC to the anomaly (see a detailed explanation at this NASA’s Earth Observatory site or NOAA’s National Centers for Environmental Information).\n\ndata[\"Annual\"] = data[\"Annual.Anomaly\"]+8.65\n\nNow let’s plot the annual temperature against the year:\n\nax = data.plot(kind=\"line\",x=\"Year\",y=\"Annual\")\nplt.show()\n\n\n\n\n\n\n\n\nLet’s fit a linear regression model that will basically describe temperature a function of the year using the data we have. Don’t worry about the math details for the time being.\n\nmodel = LinearRegression()\nX = pd.DataFrame(data[\"Year\"])\nY = pd.DataFrame(data[\"Annual\"])\nmodel.fit(X,Y)\nY_pred = model.predict(X)\n\nY_pred contains the predicted temperature values from the years. We can now create another plot that shows the original data and the linear model as a straigth line in red.\n\nplt.plot(data[\"Year\"],data[\"Annual\"])\nplt.plot(data[\"Year\"],Y_pred,color=\"red\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThis is our third lecture, and possibly the longest one. We will cover a very incomplete list of skills a data scientist should know.\nBasically, we will get a 90-minutes (or more) tour of the basic data science skills and see some examples.\nA bad joke here – there aren’t 820 steps, of course, but we will see that there are more than one person can handle, and this is not a bad thing!\nI use this figure a lot – it is based on the book Doing Data Science by Rachel Schutt and Cathy O’Neil, OReilly, 2014, which I highly recommend. The icons I use in this document are based on the figure.It shows the data science process, or a data science project cycle. Basically, we collect data about something we want to study, clean and preprocess it, do some basic (and more complex analysis), visualize/report/publish and eventually create a new data product that can be used by others. All this is used to make decisions, which purposefully will be outside of the scope of most of our projects (usually decisions come from managers and other stakeholders of the project). Not all steps must be followed, and some results may require retracing of the steps. But this basic compartmentalization helps us organize our ideas for the project, and we can comment on the skills required for each step. \nSince new tools, techniques, frameworks, algorithms, concepts, etc. are being actively developed, there is no way we can learn about all of those. Let’s take a tour through most of the better-known ones.\nThis is not always a technical skill – sometimes you need to be able to talk to and understand researchers, clients, managers, professors, etc. from other fields. You don’t need to be a full-fledged astronomer do help in an astronomy data science problem but need to understand the basic concepts and think about good questions, good data and good results.\nThis is a bit more complex. For many toy problems, the data would be already collected and organized, you just need to start the analysis. But for several other problems the data may be disorganized, or incomplete, or inaccessible, or even inexistent – collecting the data may be part of the data science project! \nAnother dead link, but you can get some references for the “80%” part at https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242 and https://www.researchgate.net/publication/335577003_Data_preparation_and_preprocessing_for_broadcast_systems_monitoring_in_PHM_framework Some people disagree, and https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/ is a good reading.Anyway we will see that for some clean, nice projects the “cleaning and organizing” part is way less than 60%, while for some more “rough” projects it may be more than 80%.\nSee the previous slide notes about this figure and its sources.Anyway, cleaning and collecting data (AKA data wrangling) seems to be not fun at all. But of course, it depends on the data and job, and may be a very important part on the whole data science process. We will see a very good example with the Supermarket data.\nI like the Merriam Webster definition because it covers an interesting definition for our purposes and an almost contrary view! We won’t have to learn a whole programming language, only learn (by doing) short bits of code to solve specific problems.\nCoding used to be an exclusive tool for computer scientists. These days are long gone – you can start coding (hacking) in a browser!Coding also provides the “glue” for data collection, preprocessing and analysis tools, allowing us to tell a story about the data. This is essential for reproduction of experiments.\nDon’t worry too much about the neural network bit. But imagine that you have to experiment with different parameters. Using the GUI-based tool you will have to open a dialog, set a value, click on a button and repeat for each and every change. Using something like a function call (the part in purple) you could use a script (small part of code) that changes the value programmatically and let the computer do the heavy work.\nTo create short pieces of code that process some data (basic cleaning, preprocessing, analysis) we don’t need a powerful computer with a compiler or integrated development environment – a basic browser would do. There are several companies that provide sites where you can edit and run basic code. Of course there are limitations, so for more processing-heavy work a full computer is recommended.\nAnother way to hack – simpler, perhaps more intuitive but more manual and less flexible: Orange.\nI like to show this as one of the proofs that data science is not a new science but something that was made possible by the evolution of technologies. These are punch cards, used until the 1980s. A deck of punch cards would represent the data and the code to process the data. Not as nearly as flexible or powerful as what we have today, but similar in the idea.\nThere is a large number of resources for learning Python (and for learning how to do a specific task in Python). It is a very good choice of general programming language.There are still two major versions of Python out there, I strongly suggest using version 3. If your computer have other versions installed and you want to use Python locally, use conda to create environments (a must in a Mac). If you will do data science only using hosted notebooks (e.g. Kaggle, Google Colab) this is not an issue.\nHere’s a simple code in Python that reads a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nTo show how simple is to create code for basic data science tasks here are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nR has a steep learning curve, specially if you have a strong OO or procedural programming, but there are several resources on how to solve specific problems in R. Use Rstudio (and the official release of R) to develop code in R in your machine, or a hosted notebook provider (Google Colab, Kaggle).\nThis is the R version of the Python code shown before. We will read a file with global average temperatures per year, do some basic operations in a variable and plot the results. All elements shown are screenshots from a notebook – we will see more about notebooks soon. The complete code is shown in the lecture notes for this course.\nHere are some more lines that create a linear fit to the data and plot it over the original data. The complete code is shown in the lecture notes for this course.\nIn other words: don’t worry about learning all of a programming language. Learn the basics and search for the specifics online.Don’t put all your programming eggs in a basket: R, Python are good for hacking, but a knowledge of other languages and tools (especially command line tools) may be handy. On the other hand almost everything can be done in almost any modern language!Don’t worry about perfect code – make code that works.\nOpinions on the use of LLMs (e.g., ChatGPT) for coding are divided. Some argue that relying on an LLM to generate code constitutes a form of cheating, as the author may not fully understand the solution. Others view LLMs as tools that can support and enhance the coding process. In the following slides, we will explore a historical perspective on writing code and offer some guidelines on the ethical use of LLMs in coding and data science.\nBefore the Internet, programmers learned primarily from coding books filled with examples. A common practice was to type out these examples, observe how they worked, and then adapt them to suit specific needs. Over time, programmers became capable of writing code from scratch, but they still frequently referred to books for details on commands and syntax.\nProgramming languages became more complex, and books covering their basics—along with catalogs of APIs and their functions—grew larger and more detailed. Eventually, much of this content became available online or downloadable, making it easier for programmers to consult as needed.\nOver time, a more structured approach emerged for finding coding references and examples—starting with general search engines and later shifting toward specialized websites.“Copying and Pasting from Stack Overflow” isn’t a real book, of course, but looking up code snippets there (or using ChatGPT) isn’t inherently wrong—as long as you verify the answers and make an effort to understand how and why they work.\nThere’s nothing inherently wrong with using LLMs to assist with coding. It can be seen as a natural evolution of earlier practices—such as copying code from books or the Internet and adapting it. However, in all cases, we should avoid blindly accepting what these sources provide, whether it’s a book, a website, or an AI model. Always test the code, run it, and adjust the details. This approach is far more effective when working with small code snippets than with large, complex applications.\nIs using ChatGPT cheating? It can be really helpful for writing small bits of code, but may fail for slightly more complex functions, As always, the user must know the right questions to ask.\nLLM tools (ChatGPT, CoPilot, CodeWhisperer, etc.) can help generating bits of code, with good results when given initial information. Use them wisely, check to code, learn with it, interact with the LLM giving suggestions and asking questions and enhancements. Use it as a code assistant, not as a coder.\nI’ve asked ChatGPT’s Data Analyst this: “Here’s a file with data on land-surface temperatures. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. Please write code in Python to plot it as a time series.“It had a hard time reading and preparing the data and gave up – all by itself!\nEDA has an interesting story, see https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_136Basically, we will take a look at the data – either with basic statistical analysis or with charts and plots. Doing this we will have a better idea on the nature of the data, its completeness, relations between the variables, etc.\nThere are many questions we can ask, meaning that there are many tools and techniques! We’ve already seen some patterns with the annual temperature example. Think about questions about your data and how to answer them with code/visualization/statistics.\nThe Iris flowers are a good dataset for basic exploratory data analysis. In this dataset there are measures (length and width of petals and sepals) of 150 samples of three classes of Iris flowers. They differ a bit in the dimensions. Let’s see what we can infer from this data. The complete code is shown in the lecture notes for this course.\nHere’s a heatmap showing the number of French rolls sold in a local supermarket chain per day and hour of day. We can detect some patterns that explain features on the data that were not clear – can you find some?The complete code is shown in the lecture notes for this course.\nNow that we used EDA we have a good (or even approximate) idea of our data’s structure, general behavior, completeness, etc. we can apply methods to better describe it. Machine Learning and Data Mining (basically the same thing, but broader in scope) can be used for this.In this course we won’t learn EDA, programming or machine learning in detail, just enough to do our exercises and credits. See courses in the Applied Computing program for more information on machine learning, neural networks, applied AI, etc.\nModels are combinations of algorithms and internal representations that can be used to describe data (and the physical phenomena behind it).For example, for the global warming data, we could use the parameters of the fitted curve to predict the temperature in any year in the future. For the Iris data we can infer some rules, e.g. that any Iris flower with a petal length shorter than 2.5cm is of class “setosa”.Models can be too simplistic or more complex than suggested by EDA: the temperature model does not present a good fit for years before 1820 or after 2000. Other more complex, better fitting curves may be used.Models can be incomplete: one simple rule classifies the “setosa” flowers but there is no simple rule that classifies “versicolor” and “virginica” without errors.\nModels depends a lot on what we want to do with the data. For example, for the supermarket basket data we could think of models that:Predict the total sales in cash depending on the day of the week and supermarket branch.Predict the number of French rolls that will be sold in an hour by fitting a curve that predicts rolls/hour for every day in the week.Estimate the probability that someone that buys more than 5 French rolls would also buy milk in the same transaction.Models ultimately depends on the data and algorithms used, and may yield unexpected or incorrect results, depending on several factors.Data may be incomplete or badly preprocessed.Metadata or ancillary data may be necessary (e.g. see the vertical gaps in this figure – what are they?)The existing data may not be enough for the analysis (e.g. we cannot identify individual clients on this data).That’s why Data Science is a process – eventually we need to trace back our steps.\nThere are plenty of choices to learn from data but we have to know a bit about the capabilities and limitations of the algorithms to use them efficiently. Even so, sometimes there is not a clear best choice for an algorithm.For example, neural networks can predict better the increase of temperature, but their prediction is hard to understand (compared to “temperature will increase X degrees in Y years”). Depending on the task we need to find a balance between accuracy and simplicity/interpretability.Some models are trained from data – this may become computationally expensive depending on the algorithm and the data.\nOne of the key concepts of data science is the emphasis on presenting results. There are several facets of this: creating nice, engaging visualizations of the data is useful for EDA and to present final results; telling a story about the data can be used for reproducibility and evaluation. One way to communicate results to other practitioners of data science (in contrast to reports for final users) is to use notebooks. We’ve already seen some glimpses of notebooks, which are basically a way to arrange text, images and code (including results of executing the code) in a single document that shows all steps and results of a particular analysis task. Notebooks can be shared as files, or publicly using tools like Kaggle, Google Colab, SciServer, etc.\nAfter doing preprocessing, EDA, analysis, documentation (use notebooks!) we may get a different dataset from the one we started with. This dataset was probably filtered, preprocessed, modified, annotated, etc. and may have additional value when compared to the raw original data – think about all the work you put on it! Consider publishing the data product of your processing, EDA and analysis with the analysis and documentation itself. Think about the value for others and how nice would it be if you had it to start with!\nWe’ve seen some problems that may arise when processing the data, doing EDA and creating/applying models. Let’s see what else can go wrong!\nHere’s a simple approach to answer the question “how are the total bills for transactions on the supermarket distributed?“ for the supermarket data. We see that the histogram is dominated by a bar on the 0-20000 reais range, which is weird – we wouldn’t even expect a 20000 bill for a local supermarket. Examining the data closely we see that there was a bill of R$ 505050.00 with a change of R$ 505044.22 – clearly a typo, but part of the data anyway. How can we automatically remove this outlier? Should we?\nSome data online may not be curated, formatted or freely available. I’ve found files that were supposed to be in CSV format but were badly formatted, reading was very complicated (e.g some data from CAPES uses a semicolon to separate the fields, but some fields contain semicolons…) Historic data, spread over several files, may have structural differences in the files themselves, so we cannot process all in the same way.If we need to “glue” different data sources together we may face issues that make the getting and processing steps of the data science process take much longer. And since new datasets may have different features or fields, we need to reevaluate processing code when doing new data.\nOne example of problems with data: spreadsheets should naturally be tidy, easily readable by humans and machines. But humans may prefer some aesthetics that could confuse machine-reading of the data.\nThese files were downloaded from CAPES Open Data Portal. Each spreadsheet covers an year of information on lecturers on graduate programs. We can see that the number of columns changed at least twice. Processing this whole dataset as one would require identification of a common set of columns.\nThis is a subset of our supermarket basked data, It is in Portuguese but I think we can understand the issue: everything has a UPC or similar code. Depending on what we want to analyze we would need to merge some itens in categories, e.g. “LEITE COND” (condensed milk), “FLV” (produce), “BOV.” (meat) and so on.\nBasically, check the code for every new data release, particularly when the data sources are out of the data science team control.\nEvaluation for this course will be based on attendance, homework and projects. Here’s the guidelines/suggestions for your project.\nStep 1 is the one that may take more time. You must find a project that 1) is interesting enough for you; 2) has public or accessible data; 3) can be explored with at least the EDA concepts; 4) can be done in 3-4 months.Step 2 is also very important, but if we get an interesting project we may already have interesting questions about it. But beware of projects for which the data is not readily accessible!\nThis is the fun part! Create code (notebooks!) that explore the data and help you tell a story about it. Check for issues on the data, see how complete it is, visualize it to see patterns.It is possible that the code (especially notebooks) get too large or unwieldly, consider creating a notebook for data cleaning and saving it in an intermediate format, and other notebooks to explore it using EDA.\nIf you can do EDA on your data you can possibly see some patterns. Think again about the questions you may have about your data and see if any of those patterns can be used to answer the questions (or at least to raise more questions!)\nCreate well-documented notebooks, with lots of text, figures and explanation about the steps you’re taking. Avoid code-only notebooks!Even if the results are bad, create notebooks to tell stories about the data. These may be useful for future attempts!\nSometimes we have the ideas and questions but not the data or the ability to create code to explore it. It is very tempting to ask a data scientist (or programmer) to solve the task for you, but remember that being a data scientist means working in an interdisciplinary way! Don’t just throw the problem and run away, talk with your collaborator, work together as a team and you will get the results.\nFinally!There are some additional slides that will not be part of this lecture (but eventually published).\nI strongly recommend the Schutt & O’Neil book, which present case studies, code, basic concepts in an easy-to-follow way.“Introducing Data Science” is a Python-centric good, practical introduction. “Python for Data Analysis” is also a good introduction, with examples formatted as notebooks, making it easy to follow.",
    "crumbs": [
      "Lectures",
      "Skills for Data Science"
    ]
  },
  {
    "objectID": "00-Homework-01.html",
    "href": "00-Homework-01.html",
    "title": "Homework #1",
    "section": "",
    "text": "How to deliver\n\nPlease deliver the answer to this homework in a single PDF file. Use figures, but try to keep it under three pages.\n\n\n\nFind a source of open data on the web (see some suggestions below). Try to get a source of data that you can relate to. Write one or two paragraphs about what it contains, who publishes it, what it can used for.\nGet at least one of the files from that source. How easy/hard was it? How well-documented is it? Does it looks tidy?\nAlternatively get a data source related to your own research and describe it.\n\n\n\nA non-categorized list.\n\nCatálogo IPEVSC – Bases de Dados Online.pdf\nDownload PubMed Data\nSciELO Data\nAgência de Bibliotecas e Coleções Digitais da Universidade de São Paulo\nPortal Brasileiro de Dados Abertos\nSete conjuntos de dados públicos que você pode analisar gratuitamente agora mesmo\nArquivo Nacional - Outras bases de dados\nBase dos Dados (ONG)\nPortal Brasileiro de Dados Abertos do Banco Central\nDados Abertos - Instituto Brasileiro de Geografia e Estatística\nTabela Brasileira de Composição de Alimentos\nPortal da Transparência - Controladoria Geral da União\nGoverno Aberto SP\nPortal de Dados Abertos do Ministério da Educação\nipeadata\nDados Abertos da Agência Nacional de Águas e Saneamento Básico\nRemuneração dos Magistrados\nCatálogo de Dados Abertos do Senado Federal\nHarvard Dataverse\nEuropean data - The official portal for European data\nWorld Bank Open Data\nUNdata\nU.S. Government’s Open Data\nNASA Earth Observation Data\nKaggle Datasets\nOur World in Data\nCDC logoCenters for Disease Control and Prevention Data\nWorld Health Organization Data\nGlobal Biodiversity Information Facility\nNOAA Climate Data Online\nCopernicus Climate Data Store\nOpenML\nUC Irvine Machine Learning Repository\nHarvard Dataverse"
  },
  {
    "objectID": "00-Homework-01.html#find-data",
    "href": "00-Homework-01.html#find-data",
    "title": "Homework #1",
    "section": "",
    "text": "Find a source of open data on the web (see some suggestions below). Try to get a source of data that you can relate to. Write one or two paragraphs about what it contains, who publishes it, what it can used for.\nGet at least one of the files from that source. How easy/hard was it? How well-documented is it? Does it looks tidy?\nAlternatively get a data source related to your own research and describe it."
  },
  {
    "objectID": "00-Homework-01.html#data-sources",
    "href": "00-Homework-01.html#data-sources",
    "title": "Homework #1",
    "section": "",
    "text": "A non-categorized list.\n\nCatálogo IPEVSC – Bases de Dados Online.pdf\nDownload PubMed Data\nSciELO Data\nAgência de Bibliotecas e Coleções Digitais da Universidade de São Paulo\nPortal Brasileiro de Dados Abertos\nSete conjuntos de dados públicos que você pode analisar gratuitamente agora mesmo\nArquivo Nacional - Outras bases de dados\nBase dos Dados (ONG)\nPortal Brasileiro de Dados Abertos do Banco Central\nDados Abertos - Instituto Brasileiro de Geografia e Estatística\nTabela Brasileira de Composição de Alimentos\nPortal da Transparência - Controladoria Geral da União\nGoverno Aberto SP\nPortal de Dados Abertos do Ministério da Educação\nipeadata\nDados Abertos da Agência Nacional de Águas e Saneamento Básico\nRemuneração dos Magistrados\nCatálogo de Dados Abertos do Senado Federal\nHarvard Dataverse\nEuropean data - The official portal for European data\nWorld Bank Open Data\nUNdata\nU.S. Government’s Open Data\nNASA Earth Observation Data\nKaggle Datasets\nOur World in Data\nCDC logoCenters for Disease Control and Prevention Data\nWorld Health Organization Data\nGlobal Biodiversity Information Facility\nNOAA Climate Data Online\nCopernicus Climate Data Store\nOpenML\nUC Irvine Machine Learning Repository\nHarvard Dataverse"
  }
]